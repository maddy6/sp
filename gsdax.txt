import pandas as pd
import numpy as np
from sklearn.metrics import mean_absolute_percentage_error

# ---------- 0. Helper & params (tune these) ----------
ALPHA_BASE = 0.25     # EWMA alpha for baseline (0.25 = medium memory). Tune 0.05-0.5
ALPHA_SHORT = 0.6     # EWMA alpha for short-term (trend numerator)
ALPHA_LONG = 0.07     # EWMA alpha for long-term (trend denominator)
SHRINK_K = 5.0        # shrinkage constant (higher -> more shrink to 1)
WINSOR_P = 0.01       # winsorize tails on rates
SEASON_SMOOTH_KERNEL = np.array([0.25, 0.5, 0.25])  # neighbor smoothing
EPS = 1e-9

# ---------- 1. Preprocess ----------
df = df.copy()
df['MONTH'] = df['MONTH'].astype(int)
df['YEAR'] = df['YEAR'].astype(int)
# If MONTH stored as YYYYMM:
df['month_num'] = df['MONTH'] % 100
# time index for ordering
df['time_idx'] = df['YEAR'] * 100 + df['month_num']

# compute rate (openings per headcount)
df['rate'] = df['NEW_OPENINGS_COUNT'] / df['HEADCOUNT'].replace(0, np.nan)
df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=['rate','HEADCOUNT'])

# sort
df = df.sort_values(['BUSINESS_GROUPS_TA','time_idx']).reset_index(drop=True)

# ---------- 2. Winsorize rate per group to remove spikes (robust)
def winsorize_group(g):
    low = g['rate'].quantile(WINSOR_P)
    high = g['rate'].quantile(1-WINSOR_P)
    g['rate_w'] = g['rate'].clip(low, high)
    return g
df = df.groupby('BUSINESS_GROUPS_TA').apply(winsorize_group).reset_index(drop=True)

# ---------- 3. Seasonal index (robust) per group-month ----------
# median rate per (group, month)
season_med = df.groupby(['BUSINESS_GROUPS_TA','month_num'])['rate_w'].median().unstack(fill_value=np.nan)

# baseline group median (robust)
group_med = df.groupby('BUSINESS_GROUPS_TA')['rate_w'].median()

# seasonal multiplier = month_median / group_median (multiplicative)
seasonal = season_med.div(group_med, axis=0).fillna(1.0)  # fill missing as neutral

# smoothing across months (wrap-around)
def smooth_row(arr):
    # arr is length 12 (month 1..12). handle missing by treating NaN as 1
    a = np.where(np.isnan(arr), 1.0, arr)
    # wrap
    a_roll_plus = np.roll(a, -1)
    a_roll_minus = np.roll(a, 1)
    sm = (SEASON_SMOOTH_KERNEL[0]*a_roll_minus +
          SEASON_SMOOTH_KERNEL[1]*a +
          SEASON_SMOOTH_KERNEL[2]*a_roll_plus)
    return sm

season_smoothed = seasonal.apply(lambda r: smooth_row(r.values), axis=1, result_type='expand')
season_smoothed.columns = seasonal.columns

# ---------- 4. Shrink seasonal index toward 1 based on observation counts ----------
counts = df.groupby(['BUSINESS_GROUPS_TA','month_num'])['rate_w'].count().unstack(fill_value=0)
# shrink weight lambda = n / (n + K) so when n small -> go toward 1
lambda_w = counts / (counts + SHRINK_K)
lambda_w = lambda_w.reindex_like(season_smoothed).fillna(0)
season_shrunk = (lambda_w * season_smoothed) + ((1 - lambda_w) * 1.0)

# ---------- 5. Baseline (EWMA last available) & trend (EWMA short/long ratio) ----------
def compute_group_baseline_trend(g):
    g = g.sort_values('time_idx').copy()
    # EWMA baseline (alpha = ALPHA_BASE) on winsorized rate
    g['ewm_base'] = g['rate_w'].ewm(alpha=ALPHA_BASE, adjust=False).mean()
    g['ewm_short'] = g['rate_w'].ewm(alpha=ALPHA_SHORT, adjust=False).mean()
    g['ewm_long'] = g['rate_w'].ewm(alpha=ALPHA_LONG, adjust=False).mean()
    return g

df = df.groupby('BUSINESS_GROUPS_TA').apply(compute_group_baseline_trend).reset_index(drop=True)

# Get last baseline per group (to use for next-month forecast)
last_baseline = df.groupby('BUSINESS_GROUPS_TA').apply(lambda g: g.iloc[-1]['ewm_base'] if len(g)>0 else np.nan)
last_short = df.groupby('BUSINESS_GROUPS_TA').apply(lambda g: g.iloc[-1]['ewm_short'] if len(g)>0 else np.nan)
last_long = df.groupby('BUSINESS_GROUPS_TA').apply(lambda g: g.iloc[-1]['ewm_long'] if len(g)>0 else np.nan)
trend_ratio = (last_short / (last_long + EPS)).fillna(1.0).clip(lower=0.5, upper=1.5)  # cap extremes

# ---------- 6. Forecast function for a target month (leave-last-month-out backtest uses this) ----------
def forecast_for_target_month(df_train, df_target):
    # df_train: train data (all months < target)
    # df_target: rows for the target month (contains HEADCOUNT for month we want to predict)
    # Build seasonal, baseline, trend from df_train
    # 1) seasonal medians
    season_med = df_train.groupby(['BUSINESS_GROUPS_TA','month_num'])['rate_w'].median().unstack(fill_value=np.nan)
    group_med = df_train.groupby('BUSINESS_GROUPS_TA')['rate_w'].median()
    seasonal = season_med.div(group_med, axis=0).fillna(1.0)
    # smooth
    season_smoothed = seasonal.apply(lambda r: smooth_row(r.values), axis=1, result_type='expand')
    season_smoothed.columns = seasonal.columns
    # counts & shrink
    counts = df_train.groupby(['BUSINESS_GROUPS_TA','month_num'])['rate_w'].count().unstack(fill_value=0)
    lambda_w = counts / (counts + SHRINK_K)
    lambda_w = lambda_w.reindex_like(season_smoothed).fillna(0)
    season_shrunk = (lambda_w * season_smoothed) + ((1 - lambda_w) * 1.0)
    # baseline & trend from EWM
    g_ewm = df_train.groupby('BUSINESS_GROUPS_TA').apply(compute_group_baseline_trend).reset_index(drop=True)
    last_base = g_ewm.groupby('BUSINESS_GROUPS_TA').apply(lambda g: g.iloc[-1]['ewm_base'] if len(g)>0 else np.nan)
    last_short = g_ewm.groupby('BUSINESS_GROUPS_TA').apply(lambda g: g.iloc[-1]['ewm_short'] if len(g)>0 else np.nan)
    last_long = g_ewm.groupby('BUSINESS_GROUPS_TA').apply(lambda g: g.iloc[-1]['ewm_long'] if len(g)>0 else np.nan)
    trend_ratio = (last_short / (last_long + EPS)).fillna(1.0).clip(lower=0.5, upper=1.5)

    # fallback values (for unseen groups)
    global_base = g_ewm['ewm_base'].median() if 'ewm_base' in g_ewm else df_train['rate_w'].median()
    # build predictions
    preds = []
    for idx, row in df_target.iterrows():
        g = row['BUSINESS_GROUPS_TA']
        m = int(row['month_num'])
        head = row['HEADCOUNT']
        s = 1.0
        if (g in season_shrunk.index) and (m in season_shrunk.columns):
            s = season_shrunk.loc[g, m]
            # if NaN, fallback to 1
            if pd.isna(s):
                s = 1.0
        # baseline
        base = last_base.get(g, np.nan)
        if pd.isna(base):
            base = global_base
        trend = trend_ratio.get(g, 1.0)
        # final predicted rate and count
        pred_rate = base * s * trend
        pred_count = pred_rate * head
        preds.append(pred_count)
    df_target = df_target.copy()
    df_target['pred_count_simple'] = preds
    return df_target

# ---------- 7. Simple leave-last-month-out backtest ----------
# We'll predict the latest MONTH value in your dataset using all earlier months.
last_month = df['MONTH'].max()
train = df[df['MONTH'] < last_month].copy()
test = df[df['MONTH'] == last_month].copy()

pred_df = forecast_for_target_month(train, test)
# Evaluate MAPE (ignore rows with 0 actuals)
mask = pred_df['NEW_OPENINGS_COUNT'] > 0
mape = mean_absolute_percentage_error(pred_df.loc[mask, 'NEW_OPENINGS_COUNT'],
                                      pred_df.loc[mask, 'pred_count_simple'])
print("Simple seasonal+EWMA MAPE on last month:", mape)

# See per-group errors
pred_df['pct_err'] = (pred_df['pred_count_simple'] - pred_df['NEW_OPENINGS_COUNT']) / (pred_df['NEW_OPENINGS_COUNT'] + EPS) * 100
display = pred_df[['BUSINESS_GROUPS_TA','NEW_OPENINGS_COUNT','pred_count_simple','pct_err']].sort_values('pct_err')
print(display.head(20))

# ---------- 8. If you want hyperparameter tuning (alpha, SHRINK_K) do a small grid search ----------
def evaluate_params(df, alphas=[0.1,0.25,0.5], shrink_ks=[2,5,10]):
    best = {'mape': 1e9, 'params':None}
    # use simple holdout: last month as test
    last_month = df['MONTH'].max()
    train = df[df['MONTH'] < last_month].copy()
    test = df[df['MONTH'] == last_month].copy()
    for a in alphas:
        for ks in shrink_ks:
            global ALPHA_BASE, ALPHA_SHORT, ALPHA_LONG, SHRINK_K
            ALPHA_BASE, ALPHA_SHORT, ALPHA_LONG, SHRINK_K = a, 0.6, 0.07, ks
            pred = forecast_for_target_month(train, test)
            mask = pred['NEW_OPENINGS_COUNT'] > 0
            m = mean_absolute_percentage_error(pred.loc[mask, 'NEW_OPENINGS_COUNT'], pred.loc[mask, 'pred_count_simple'])
            if m < best['mape']:
                best = {'mape': m, 'params': (a, ks)}
    return best

# Uncomment to run tuning (may take a little while)
#best = evaluate_params(df, alphas=[0.05,0.1,0.25,0.5], shrink_ks=[1,3,5,10])
#print("Best:", best)


























# Forecast pipeline (run in Python 3.8+)
# Required packages: pandas, numpy, statsmodels, scikit-learn, joblib
# pip install pandas numpy statsmodels scikit-learn joblib

import pandas as pd
import numpy as np
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import GroupKFold
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error
from joblib import dump, load

# ---------- 1. Load data ----------
# Replace with the path to your CSV or read whatever DataFrame you have:
# df = pd.read_csv("your_file.csv")
# For interactive use, if you already have a DataFrame 'df' loaded, skip the read.

# Ensure columns exist:
required_cols = ['BUSINESS_GROUPS_TA', 'NEW_OPENINGS_COUNT', 'HEADCOUNT', 'MONTH', 'YEAR']
assert all(c in df.columns for c in required_cols), "Make sure dataset has exact column names."

# ---------- 2. Clean & features ----------
df = df.copy()
# MONTH in your image looks like YYYYMM (e.g., 202307). If it's an int, extract month:
df['MONTH'] = df['MONTH'].astype(int)
df['YEAR'] = df['YEAR'].astype(int)
df['month_num'] = df['MONTH'] % 100  # if month is YYYYMM
# Add rate target (openings per headcount)
df['rate'] = df['NEW_OPENINGS_COUNT'] / df['HEADCOUNT']
# log headcount for offset
df['log_headcount'] = np.log(df['HEADCOUNT'].replace(0, np.nan))
df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=['log_headcount'])

# Optional: rolling historical features per group — helps model
df = df.sort_values(['BUSINESS_GROUPS_TA', 'YEAR', 'month_num'])
df['rate_lag1'] = df.groupby('BUSINESS_GROUPS_TA')['rate'].shift(1)
df['rate_lag3_mean'] = df.groupby('BUSINESS_GROUPS_TA')['rate'].shift(1).rolling(3).mean().reset_index(level=0, drop=True)

# Month and business group as categorical
df['month_cat'] = df['month_num'].astype(str)
df['bg'] = df['BUSINESS_GROUPS_TA'].astype(str)

# ---------- 3. Exploratory diagnostics ----------
print("Overall mean rate:", df['rate'].mean())
month_index = df.groupby('month_num')['rate'].mean()
print("Monthly seasonal index (avg rate by month):")
print(month_index)

group_index = df.groupby('BUSINESS_GROUPS_TA')['rate'].mean().sort_values(ascending=False)
print("Top group rates:\n", group_index.head())

# ---------- 4. Overdispersion test for Poisson ----------
# Fit simple Poisson to test deviance / df:
formula = 'NEW_OPENINGS_COUNT ~ C(BUSINESS_GROUPS_TA) + C(month_cat) + YEAR'
poisson_model = smf.glm(formula=formula, data=df, family=sm.families.Poisson(), offset=np.log(df['HEADCOUNT'])).fit()
deviance = poisson_model.deviance
df_resid = poisson_model.df_resid
print("Poisson deviance/df_resid:", deviance / df_resid)
# If >>1, overdispersion likely — use Negative Binomial

use_nb = (deviance / df_resid) > 1.5
print("Use Negative Binomial?", use_nb)

# ---------- 5. Fit main statistical model (GLM with offset) ----------
if use_nb:
    # Negative Binomial via statsmodels GLM with NB family
    glm_nb = smf.glm(formula=formula, data=df, family=sm.families.NegativeBinomial(alpha=1.0),
                     offset=np.log(df['HEADCOUNT'])).fit()
    glm_model = glm_nb
else:
    glm_model = poisson_model

print(glm_model.summary())

# ---------- 6. Fit tree model on rate (complementary) ----------
# Prepare features for tree: month_cat, bg, log_headcount, lag features
feats = ['log_headcount', 'rate_lag1', 'rate_lag3_mean', 'month_num']
X = df[feats].fillna(0).values
y = df['rate'].values

# Use GroupKFold by BUSINESS_GROUPS_TA for robust CV
gkf = GroupKFold(n_splits=5)
gb = GradientBoostingRegressor(n_estimators=300, max_depth=4, learning_rate=0.05)
# Fit on full data (we rely on backtest below)
gb.fit(X, y)

# ---------- 7. Ensemble predictions ----------
# Predict expected count from GLM (which predicts mean count):
df['glm_pred_mean_count'] = glm_model.predict(df)  # this returns expected counts (since offset used)
# But depending on formula/predict, ensure these are counts; if predict gave rate, multiply by HEADCOUNT
# Statsmodels with offset and count family gives expected count directly.

# Tree predicts rate; convert to expected counts:
tree_rate_pred = gb.predict(X)
df['tree_pred_mean_count'] = tree_rate_pred * df['HEADCOUNT']

# Ensemble: weighted average (weights tuned on validation)
w_glm = 0.6
w_tree = 0.4
df['ensemble_pred_count'] = w_glm * df['glm_pred_mean_count'] + w_tree * df['tree_pred_mean_count']
df['ensemble_pred_rate'] = df['ensemble_pred_count'] / df['HEADCOUNT']

# ---------- 8. Backtesting (leave-last-month-out per group) ----------
# We'll perform a simple time-split: train on all months < max_month and test on last month.
last_month = df['MONTH'].max()
train = df[df['MONTH'] < last_month]
test = df[df['MONTH'] == last_month]

# Refit models on train (GLM & Tree), predict for test
# GLM refit
glm_train = smf.glm(formula=formula, data=train, family=(sm.families.NegativeBinomial() if use_nb else sm.families.Poisson()), offset=np.log(train['HEADCOUNT'])).fit()
test['glm_pred'] = glm_train.predict(test)
# tree refit
X_train = train[feats].fillna(0).values
y_train = train['rate'].values
gb2 = GradientBoostingRegressor(n_estimators=300, max_depth=4, learning_rate=0.05)
gb2.fit(X_train, y_train)
test_X = test[feats].fillna(0).values
test['tree_pred_rate'] = gb2.predict(test_X)
test['tree_pred_count'] = test['tree_pred_rate'] * test['HEADCOUNT']
test['ensemble'] = 0.6 * test['glm_pred'] + 0.4 * test['tree_pred_count']

# Evaluate percent deviation
test['pct_error'] = (test['ensemble'] - test['NEW_OPENINGS_COUNT']) / test['NEW_OPENINGS_COUNT'] * 100
mape = mean_absolute_percentage_error(test['NEW_OPENINGS_COUNT'], test['ensemble'])
print("Backtest MAPE on last month:", mape)
print("Backtest mean % error:", test['pct_error'].mean())
print(test[['BUSINESS_GROUPS_TA','NEW_OPENINGS_COUNT','ensemble','pct_error']].sort_values('pct_error'))

# ---------- 9. Calibration if systematic bias ----------
# Compute multiplicative adjustment per group if needed:
group_bias = (test.groupby('BUSINESS_GROUPS_TA')['NEW_OPENINGS_COUNT'].sum() /
              test.groupby('BUSINESS_GROUPS_TA')['ensemble'].sum()).replace([np.inf, -np.inf], 1).fillna(1)
# Apply the group_bias when forecasting next month
test = test.join(group_bias.rename('group_bias'), on='BUSINESS_GROUPS_TA')
test['ensemble_calibrated'] = test['ensemble'] * test['group_bias']

# Re-evaluate
mape_cal = mean_absolute_percentage_error(test['NEW_OPENINGS_COUNT'], test['ensemble_calibrated'])
print("Calibrated MAPE:", mape_cal)

# ---------- 10. Uncertainty (parametric bootstrap using NB as example) ----------
def nb_bootstrap_predict(glm_res, df_subset, n_sims=500):
    # glm_res is fitted NB/Poisson model
    mu = glm_res.predict(df_subset)  # expected counts
    sims = np.random.negative_binomial(n=1/ glm_res.scale if hasattr(glm_res,'scale') else 10,
                                       p=1/(1+mu* (glm_res.scale if hasattr(glm_res,'scale') else 1e-6)),
                                       size=(n_sims, len(mu)))
    # Above is heuristic; if exact NB params available, use those. This is illustrative.
    return sims

# Note: use a proper NB param draw in your environment. Statsmodels NB parameterization differs.

# ---------- Save models for production ----------
dump(glm_model, 'glm_model.joblib')
dump(gb, 'gb_model.joblib')

# ---------- Final forecast: produce next-month predictions ----------
# Prepare df_next (rows you want prediction for); here as example we predict for 'test' set
df_next = test.copy()
# Predict with saved models
df_next['glm_pred_next'] = glm_train.predict(df_next)
df_next['tree_rate_next'] = gb2.predict(df_next[feats].fillna(0).values)
df_next['tree_count_next'] = df_next['tree_rate_next'] * df_next['HEADCOUNT']
df_next['final_pred'] = 0.6 * df_next['glm_pred_next'] + 0.4 * df_next['tree_count_next']
# Apply calibration factor if exists:
df_next['group_bias'] = df_next['BUSINESS_GROUPS_TA'].map(group_bias).fillna(1)
df_next['final_pred_calibrated'] = df_next['final_pred'] * df_next['group_bias']

print(df_next[['BUSINESS_GROUPS_TA','NEW_OPENINGS_COUNT','final_pred_calibrated']])

# Save results
df_next.to_csv("forecast_next_month.csv", index=False)

























# ==============================
# 1. Prepare historical data with SMA/EMA
# ==============================
hist_df = df[['MONTH', 'BUSINESS_GROUPS_TA', 'MONTHLY_ATTRITION_RATE']].rename(
    columns={'MONTH': 'Future Month',
             'BUSINESS_GROUPS_TA': 'Business Group',
             'MONTHLY_ATTRITION_RATE': 'Attrition Rate'}
)
hist_df['Future Month'] = pd.to_datetime(hist_df['Future Month'])
hist_df['Type'] = 'Actual'

# compute pre-forecast SMA/EMA on historical attrition
df_sorted = df.sort_values(['BUSINESS_GROUPS_TA', 'MONTH']).copy()
df_sorted['SMA'] = df_sorted.groupby('BUSINESS_GROUPS_TA')['MONTHLY_ATTRITION_RATE'].transform(
    lambda x: x.rolling(window=3, min_periods=1).mean()
)
df_sorted['EMA'] = df_sorted.groupby('BUSINESS_GROUPS_TA')['MONTHLY_ATTRITION_RATE'].transform(
    lambda x: x.ewm(span=3, adjust=False).mean()
)

sma_hist = df_sorted[['MONTH', 'BUSINESS_GROUPS_TA', 'SMA']].rename(
    columns={'MONTH': 'Future Month', 'BUSINESS_GROUPS_TA': 'Business Group', 'SMA': 'Attrition Rate'}
)
sma_hist['Future Month'] = pd.to_datetime(sma_hist['Future Month'])
sma_hist['Type'] = 'Forecast_SMA'

ema_hist = df_sorted[['MONTH', 'BUSINESS_GROUPS_TA', 'EMA']].rename(
    columns={'MONTH': 'Future Month', 'BUSINESS_GROUPS_TA': 'Business Group', 'EMA': 'Attrition Rate'}
)
ema_hist['Future Month'] = pd.to_datetime(ema_hist['Future Month'])
ema_hist['Type'] = 'Forecast_EMA'

# ==============================
# 2. Prepare forecast part (already in forecast_df)
# ==============================
def make_forecast_subset(col, label):
    tmp = forecast_df[['Future Month', 'Business Group', col]].rename(
        columns={col: 'Attrition Rate'}
    )
    tmp['Future Month'] = pd.to_datetime(tmp['Future Month'])
    tmp['Type'] = label
    return tmp

forecast_sma = make_forecast_subset('Fut Vol Attr. Month %_sma', 'Forecast_SMA')
forecast_ema = make_forecast_subset('Fut Vol Attr. Month %_ema', 'Forecast_EMA')

forecast_actual = forecast_df[['Future Month', 'Business Group', 'Act Vol Attr-Month %']].rename(
    columns={'Act Vol Attr-Month %': 'Attrition Rate'}
)
forecast_actual['Future Month'] = pd.to_datetime(forecast_actual['Future Month'])
forecast_actual['Type'] = 'Actual'

# ==============================
# 3. Combine all
# ==============================
plot_df = pd.concat(
    [hist_df, sma_hist, ema_hist, forecast_sma, forecast_ema, forecast_actual],
    ignore_index=True
)

# ==============================
# 4. Plot with styles
# ==============================
forecast_start_dt = pd.to_datetime(FORECAST_START)

plt.figure(figsize=(16, 8))
colors = {'Actual': 'black', 'Forecast_SMA': 'blue', 'Forecast_EMA': 'green'}
groups = ['Banking']

for g in groups:
    sub = plot_df[plot_df['Business Group'] == g].sort_values('Future Month')

    for typ in ['Actual', 'Forecast_SMA', 'Forecast_EMA']:
        data = sub[sub['Type'] == typ]

        if data.empty:
            continue

        pre = data[data['Future Month'] < forecast_start_dt]
        post = data[data['Future Month'] >= forecast_start_dt]

        # plot pre (solid)
        if not pre.empty:
            plt.plot(pre['Future Month'], pre['Attrition Rate'],
                     color=colors[typ],
                     linestyle='-',
                     label=f"{typ} Pre" if typ != 'Actual' else "Actual")

        # plot post (dotted for forecasts, solid for actual)
        if not post.empty:
            linestyle = '-' if typ == 'Actual' else '--'
            plt.plot(post['Future Month'], post['Attrition Rate'],
                     color=colors[typ],
                     linestyle=linestyle,
                     label=f"{typ} Forecast")

plt.axvline(forecast_start_dt, color='gray', linestyle=':', linewidth=1)
plt.title("Attrition Rate Actual vs SMA/EMA (Solid=History, Dotted=Forecast)")
plt.xlabel("Month")
plt.ylabel("Attrition Rate (%)")
plt.legend(fontsize='small')
plt.grid(True)
plt.tight_layout()
plt.show()






















# -----------------------
# 1. Prepare combined data
# -----------------------
actual_df = df[['MONTH', 'BUSINESS_GROUPS_TA', 'MONTHLY_ATTRITION_RATE']].rename(
    columns={'MONTH': 'Future Month',
             'BUSINESS_GROUPS_TA': 'Business Group',
             'MONTHLY_ATTRITION_RATE': 'Attrition Rate'}
)
actual_df['Type'] = 'Actual'

def make_forecast_subset(col, label):
    tmp = forecast_df[['Future Month', 'Business Group', col]].rename(
        columns={col: 'Attrition Rate'}
    )
    tmp['Type'] = label
    return tmp

forecast_sma = make_forecast_subset('Fut Vol Attr. Month %_sma', 'Forecast_SMA')
forecast_ema = make_forecast_subset('Fut Vol Attr. Month %_ema', 'Forecast_EMA')

# combine
plot_df = pd.concat([actual_df, forecast_sma, forecast_ema], ignore_index=True)
plot_df['Future Month'] = pd.to_datetime(plot_df['Future Month'])

# define forecast start date
forecast_start_dt = pd.to_datetime(FORECAST_START)

# -----------------------
# 2. Plot with pre vs post styles
# -----------------------
plt.figure(figsize=(16, 8))

colors = {
    'Actual': 'black',
    'Forecast_SMA': 'blue',
    'Forecast_EMA': 'green'
}

groups = ['Banking']

for g in groups:
    sub = plot_df[plot_df['Business Group'] == g].sort_values('Future Month')

    for typ in ['Actual', 'Forecast_SMA', 'Forecast_EMA']:
        data = sub[sub['Type'] == typ]

        if data.empty:
            continue

        # split pre and post forecast
        pre = data[data['Future Month'] < forecast_start_dt]
        post = data[data['Future Month'] >= forecast_start_dt]

        # plot pre (solid)
        if not pre.empty:
            plt.plot(pre['Future Month'], pre['Attrition Rate'],
                     color=colors[typ],
                     linestyle='-',
                     label=f"{typ} Pre" if typ != 'Actual' else "Actual")

        # plot post (dotted for forecasts, solid for actual)
        if not post.empty:
            post_linestyle = '-' if typ == 'Actual' else '--'
            plt.plot(post['Future Month'], post['Attrition Rate'],
                     color=colors[typ],
                     linestyle=post_linestyle,
                     label=f"{typ} Forecast")

plt.axvline(forecast_start_dt, color='gray', linestyle=':', linewidth=1)
plt.title("Attrition Rate Actual vs SMA/EMA (Solid=History, Dotted=Forecast)")
plt.xlabel("Month")
plt.ylabel("Attrition Rate (%)")
plt.legend(fontsize='small')
plt.grid(True)
plt.tight_layout()
plt.show()























import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# -----------------------
# PARAMETERS
# -----------------------
BASE_MONTH_HC = 229568
FORECAST_START = "2025-04-01"   # forecast start month
FORECAST_END   = "2026-12-01"   # forecast end month
DIVIDE_BY_100  = True           # True if rates are in %
ROLL_WINDOW = 3                 # rolling window for smoothing
# -----------------------

# Prepare df
df['MONTH'] = pd.to_datetime(df['MONTH']).dt.to_period('M').dt.to_timestamp()
df['YEAR']  = df['MONTH'].dt.year

forecast_index = pd.date_range(start=FORECAST_START, end=FORECAST_END, freq="MS")
groups = df['BUSINESS_GROUPS_TA'].unique()

all_rows = []

# ==========================
# 1. PER-GROUP FORECASTING
# ==========================
for g in groups:
    grp = df[df['BUSINESS_GROUPS_TA'] == g].copy().set_index('MONTH').sort_index()

    # --- smoothing ---
    grp['SMA3'] = grp['MONTHLY_ATTRITION_RATE'].rolling(window=ROLL_WINDOW).mean()
    grp['EMA3'] = grp['MONTHLY_ATTRITION_RATE'].ewm(span=ROLL_WINDOW, adjust=False).mean()
    grp['MED3'] = grp['MONTHLY_ATTRITION_RATE'].rolling(window=ROLL_WINDOW).median()
    alpha_wilder = 1.0 / ROLL_WINDOW
    grp['WILDER3'] = grp['MONTHLY_ATTRITION_RATE'].ewm(alpha=alpha_wilder, adjust=False).mean()

    # fallback util
    def last_valid(col):
        if (col in grp.columns) and (not grp[col].empty):
            v = grp[col].iloc[-1]
            if not pd.isna(v):
                return v
        if (not grp['MONTHLY_ATTRITION_RATE'].empty):
            return grp['MONTHLY_ATTRITION_RATE'].iloc[-1]
        return np.nan

    last_sma = last_valid('SMA3')
    last_ema = last_valid('EMA3')
    last_med = last_valid('MED3')
    last_wil = last_valid('WILDER3')

    # last known headcount BEFORE forecast start
    pre_forecast_hc_series = grp[grp.index < pd.to_datetime(FORECAST_START)]['HEADCOUNT']
    if not pre_forecast_hc_series.empty:
        last_known_hc = pre_forecast_hc_series.iloc[-1]
    else:
        last_known_hc = grp['HEADCOUNT'].iloc[-1] if (not grp['HEADCOUNT'].empty) else BASE_MONTH_HC

    # build rows
    for fut_ts in forecast_index:
        if fut_ts in grp.index:
            act_rate  = grp.at[fut_ts, 'MONTHLY_ATTRITION_RATE'] if 'MONTHLY_ATTRITION_RATE' in grp.columns else np.nan
            act_exits = grp.at[fut_ts, 'VOLUNTARY_EXITS'] if 'VOLUNTARY_EXITS' in grp.columns else np.nan
            act_hc    = grp.at[fut_ts, 'HEADCOUNT'] if 'HEADCOUNT' in grp.columns else np.nan
        else:
            act_rate  = np.nan
            act_exits = np.nan
            act_hc    = np.nan

        # smoothed fallback
        def get_smoothed(col, last_val):
            if (fut_ts in grp.index) and (col in grp.columns) and (not pd.isna(grp.at[fut_ts, col])):
                return grp.at[fut_ts, col]
            return last_val

        fut_rate_sma = get_smoothed('SMA3', last_sma)
        fut_rate_ema = get_smoothed('EMA3', last_ema)
        fut_rate_med = get_smoothed('MED3', last_med)
        fut_rate_wil = get_smoothed('WILDER3', last_wil)

        # counts
        def to_count(rate):
            if pd.isna(rate):
                return np.nan
            return (rate / 100.0 * last_known_hc) if DIVIDE_BY_100 else (rate * last_known_hc)

        fut_count_sma = to_count(fut_rate_sma)
        fut_count_ema = to_count(fut_rate_ema)
        fut_count_med = to_count(fut_rate_med)
        fut_count_wil = to_count(fut_rate_wil)

        row = {
            "Future Month": fut_ts,
            "Business Group": g,
            "Fut Vol Attr. Month %_sma": round(fut_rate_sma, 4) if not pd.isna(fut_rate_sma) else np.nan,
            "Fut Vol Attr. Month %_ema": round(fut_rate_ema, 4) if not pd.isna(fut_rate_ema) else np.nan,
            "Fut Vol Attr. Month %_med": round(fut_rate_med, 4) if not pd.isna(fut_rate_med) else np.nan,
            "Fut Vol Attr. Month %_wilder": round(fut_rate_wil, 4) if not pd.isna(fut_rate_wil) else np.nan,
            "Base Month HC": int(last_known_hc) if not pd.isna(last_known_hc) else np.nan,
            "Fut Vol. Attr Count_sma": int(round(fut_count_sma)) if (not pd.isna(fut_count_sma)) else np.nan,
            "Fut Vol. Attr Count_ema": int(round(fut_count_ema)) if (not pd.isna(fut_count_ema)) else np.nan,
            "Fut Vol. Attr Count_med": int(round(fut_count_med)) if (not pd.isna(fut_count_med)) else np.nan,
            "Fut Vol. Attr Count_wilder": int(round(fut_count_wil)) if (not pd.isna(fut_count_wil)) else np.nan,
            "Act Vol Attr. Month %": round(act_rate,4) if not pd.isna(act_rate) else np.nan,
            "Act Vol Attr Count": int(act_exits) if not pd.isna(act_exits) else np.nan,
            "Actual Headcount": int(act_hc) if not pd.isna(act_hc) else np.nan,
        }
        all_rows.append(row)

forecast_df = pd.DataFrame(all_rows)

# ==========================
# 2. PER-GROUP PLOTS
# ==========================
actual_df = df[['MONTH','BUSINESS_GROUPS_TA','MONTHLY_ATTRITION_RATE']].rename(
    columns={'MONTH':'Future Month','BUSINESS_GROUPS_TA':'Business Group','MONTHLY_ATTRITION_RATE':'Attrition Rate'})
actual_df['Type'] = 'Actual'

def make_forecast_subset(col, label):
    tmp = forecast_df[['Future Month','Business Group',col]].rename(columns={col:'Attrition Rate'})
    tmp['Type'] = label
    return tmp

forecast_sma = make_forecast_subset('Fut Vol Attr. Month %_sma','Forecast_SMA')
forecast_ema = make_forecast_subset('Fut Vol Attr. Month %_ema','Forecast_EMA')
forecast_med = make_forecast_subset('Fut Vol Attr. Month %_med','Forecast_MED')
forecast_wilder = make_forecast_subset('Fut Vol Attr. Month %_wilder','Forecast_WILDER')

plot_df = pd.concat([actual_df, forecast_sma, forecast_ema, forecast_med, forecast_wilder], ignore_index=True)
plot_df['Future Month'] = pd.to_datetime(plot_df['Future Month'])

plt.figure(figsize=(16,8))
forecast_start_dt = pd.to_datetime(FORECAST_START)

colors = {
    'Actual': 'black',
    'Forecast_SMA': 'blue',
    'Forecast_EMA': 'green',
    'Forecast_MED': 'orange',
    'Forecast_WILDER': 'purple'
}

for g in groups:
    sub = plot_df[plot_df['Business Group'] == g].sort_values('Future Month')
    for typ in ['Actual','Forecast_SMA','Forecast_EMA','Forecast_MED','Forecast_WILDER']:
        data = sub[sub['Type'] == typ]
        if data.empty:
            continue
        hist = data[data['Future Month'] < forecast_start_dt]
        if not hist.empty:
            plt.plot(hist['Future Month'], hist['Attrition Rate'], color=colors[typ], label=f"{g} {typ} (Hist)")
        fore = data[data['Future Month'] >= forecast_start_dt]
        if not fore.empty:
            plt.plot(fore['Future Month'], fore['Attrition Rate'], linestyle=':', color=colors[typ], label=f"{g} {typ} (Forecast)")

plt.axvline(forecast_start_dt, color='gray', linestyle=':', linewidth=1)
plt.title("Attrition Rate — Actual vs SMA/EMA/Median/Wilder (solid=hist, dotted=forecast)")
plt.xlabel("Month")
plt.ylabel("Attrition Rate (%)")
plt.legend(ncol=2, fontsize='small')
plt.grid(True)
plt.tight_layout()
plt.show()

# ==========================
# 3. OVERALL WEIGHTED TOTALS
# ==========================
# Actual overall (weighted)
actual_tot = df.groupby('MONTH').apply(
    lambda d: pd.Series({
        'total_exits': (d['MONTHLY_ATTRITION_RATE'] / 100.0 * d['HEADCOUNT']).sum(),
        'total_hc'   : d['HEADCOUNT'].sum()
    })
).reset_index().rename(columns={'MONTH':'Future Month'})
actual_tot['overall_rate'] = actual_tot['total_exits'] / actual_tot['total_hc'] * 100.0
actual_tot['Type'] = 'Actual_Total'
actual_tot = actual_tot[['Future Month','overall_rate','Type']].rename(columns={'overall_rate':'Rate'})

# Forecast totals
def make_total(forecast_df, count_col, type_name):
    tmp = forecast_df.groupby('Future Month').agg({
        count_col: 'sum',
        'Base Month HC': 'sum'
    }).reset_index()
    tmp['Rate'] = (tmp[count_col] / tmp['Base Month HC']) * 100.0
    tmp['Type'] = type_name
    return tmp[['Future Month','Rate','Type']]

forecast_tot_sma = make_total(forecast_df,'Fut Vol. Attr Count_sma','Forecast_Total_SMA')
forecast_tot_ema = make_total(forecast_df,'Fut Vol. Attr Count_ema','Forecast_Total_EMA')
forecast_tot_med = make_total(forecast_df,'Fut Vol. Attr Count_med','Forecast_Total_MED')
forecast_tot_wilder = make_total(forecast_df,'Fut Vol. Attr Count_wilder','Forecast_Total_WILDER')

tot_plot = pd.concat([actual_tot, forecast_tot_sma, forecast_tot_ema, forecast_tot_med, forecast_tot_wilder], ignore_index=True)
tot_plot['Future Month'] = pd.to_datetime(tot_plot['Future Month'])
tot_plot['ForecastFlag'] = tot_plot['Future Month'] >= forecast_start_dt

plt.figure(figsize=(12,6))
def draw_total_line(type_name, color, label):
    hist = tot_plot[(tot_plot['Type']==type_name) & (~tot_plot['ForecastFlag'])].sort_values('Future Month')
    fore = tot_plot[(tot_plot['Type']==type_name) & (tot_plot['ForecastFlag'])].sort_values('Future Month')
    if not hist.empty:
        plt.plot(hist['Future Month'], hist['Rate'], color=color, label=f"{label} (Hist)", linewidth=2)
    if not fore.empty:
        plt.plot(fore['Future Month'], fore['Rate'], color=color, linestyle=':', label=f"{label} (Forecast)", linewidth=2)

draw_total_line('Actual_Total','black','Overall Actual')
draw_total_line('Forecast_Total_SMA','blue','Overall SMA')
draw_total_line('Forecast_Total_EMA','green','Overall EMA')
draw_total_line('Forecast_Total_MED','orange','Overall MEDIAN')
draw_total_line('Forecast_Total_WILDER','purple','Overall WILDER')

plt.axvline(forecast_start_dt, color='gray', linestyle=':', linewidth=1)
plt.title("Overall Attrition Rate (weighted) — Actual vs SMA/EMA/Median/Wilder")
plt.xlabel("Month")
plt.ylabel("Attrition Rate (%)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Done: you have forecast_df, per-group plots, and overall weighted plot











def draw_line(sub, type_name, color, label, force_solid=False):
    hist = sub[(sub['TYPE'] == type_name) & (~sub['ForecastFlag'])]
    fore = sub[(sub['TYPE'] == type_name) & (sub['ForecastFlag'])]
    
    if not hist.empty:
        plt.plot(hist['Future Month'], hist['Attrition Rate'], 
                 color=color, label=f'{label} Hist')
    
    if not fore.empty:
        plt.plot(fore['Future Month'], fore['Attrition Rate'], 
                 color=color, 
                 linestyle='-' if force_solid else '--',   # Actual solid, forecast dotted
                 label=f'{label} Forecast')

# Draw lines
draw_line(sub, 'Actual', colors['Actual'], 'Actual', force_solid=True)   # <-- Always solid
draw_line(sub, 'Forecast_SMA', colors['Forecast_SMA'], 'SMA')
draw_line(sub, 'Forecast_EMA', colors['Forecast_EMA'], 'EMA')
draw_line(sub, 'Forecast_MED', colors['Forecast_MED'], 'MED')
draw_line(sub, 'Forecast_ZLEMA', colors['Forecast_ZLEMA'], 'ZLEMA')
draw_line(sub, 'Forecast_DEMA', colors['Forecast_DEMA'], 'DEMA')












import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# -----------------------
# PARAMETERS
# -----------------------
BASE_MONTH_HC = 229568
FORECAST_START = "202504"
FORECAST_END   = "202512"
DIVIDE_BY_100  = True
ROLL_WINDOW = 3
# -----------------------

# --- Safe universal conversion of MONTH ---
def safe_month_parser(x):
    x = str(x)
    for fmt in ("%Y%m", "%Y-%m", "%Y-%m-%d"):
        try:
            return pd.to_datetime(x, format=fmt)
        except ValueError:
            continue
    return pd.NaT  # fallback if unrecognized

df['MONTH'] = df['MONTH'].apply(safe_month_parser)
df['YEAR']  = df['MONTH'].dt.year

# Forecast index (month start timestamps)
forecast_index = pd.date_range(start=pd.to_datetime(FORECAST_START, format="%Y%m"),
                               end=pd.to_datetime(FORECAST_END, format="%Y%m"),
                               freq="MS")

groups = df['BUSINESS_GROUPS_TA'].unique()
all_rows = []

for g in groups:
    grp = df[df['BUSINESS_GROUPS_TA'] == g].copy().set_index('MONTH').sort_index()

    # smoothing: SMA, EMA, Rolling Median, Wilder (alpha = 1/N)
    if 'MONTHLY_ATTRITION_RATE' not in grp.columns:
        print(f"⚠️ Warning: MONTHLY_ATTRITION_RATE not in dataset for group {g}")
        continue

    grp['SMA3'] = grp['MONTHLY_ATTRITION_RATE'].rolling(window=ROLL_WINDOW).mean()
    grp['EMA3'] = grp['MONTHLY_ATTRITION_RATE'].ewm(span=ROLL_WINDOW, adjust=False).mean()
    grp['MED3'] = grp['MONTHLY_ATTRITION_RATE'].rolling(window=ROLL_WINDOW).median()
    alpha_wilder = 1.0 / ROLL_WINDOW
    grp['WILDER3'] = grp['MONTHLY_ATTRITION_RATE'].ewm(alpha=alpha_wilder, adjust=False).mean()

    # last-available fallback utility
    def last_valid(col):
        if (col in grp.columns) and (not grp[col].empty):
            v = grp[col].iloc[-1]
            if not pd.isna(v):
                return v
        if (not grp['MONTHLY_ATTRITION_RATE'].empty):
            return grp['MONTHLY_ATTRITION_RATE'].iloc[-1]
        return np.nan

    last_sma = last_valid('SMA3')
    last_ema = last_valid('EMA3')
    last_med = last_valid('MED3')
    last_wil = last_valid('WILDER3')

    # last known headcount BEFORE forecast start
    pre_forecast_hc_series = grp[grp.index < pd.to_datetime(FORECAST_START, format="%Y%m")]['HEADCOUNT']
    if not pre_forecast_hc_series.empty:
        last_known_hc = pre_forecast_hc_series.iloc[-1]
    else:
        last_known_hc = grp['HEADCOUNT'].iloc[-1] if (not grp['HEADCOUNT'].empty) else BASE_MONTH_HC

    # build rows for every forecast_index month
    for fut_ts in forecast_index:
        # actuals if present
        if fut_ts in grp.index:
            act_rate  = grp.at[fut_ts, 'MONTHLY_ATTRITION_RATE'] if 'MONTHLY_ATTRITION_RATE' in grp.columns else np.nan
            act_exits = grp.at[fut_ts, 'VOLUNTARY_EXITS'] if 'VOLUNTARY_EXITS' in grp.columns else np.nan
            act_hc    = grp.at[fut_ts, 'HEADCOUNT'] if 'HEADCOUNT' in grp.columns else np.nan
        else:
            act_rate  = np.nan
            act_exits = np.nan
            act_hc    = np.nan

        # smoothed values
        def get_smoothed(col, last_val):
            if (fut_ts in grp.index) and (col in grp.columns) and (not pd.isna(grp.at[fut_ts, col])):
                return grp.at[fut_ts, col]
            return last_val

        fut_rate_sma = get_smoothed('SMA3', last_sma)
        fut_rate_ema = get_smoothed('EMA3', last_ema)
        fut_rate_med = get_smoothed('MED3', last_med)
        fut_rate_wil = get_smoothed('WILDER3', last_wil)

        # convert % → counts
        def to_count(rate):
            if pd.isna(rate):
                return np.nan
            return (rate / 100.0 * last_known_hc) if DIVIDE_BY_100 else (rate * last_known_hc)

        fut_count_sma = to_count(fut_rate_sma)
        fut_count_ema = to_count(fut_rate_ema)
        fut_count_med = to_count(fut_rate_med)
        fut_count_wil = to_count(fut_rate_wil)

        # final row
        row = {
            "Future Month": fut_ts,
            "Business Group": g,
            "Fut Vol Attr. Month %_sma": round(fut_rate_sma, 4) if not pd.isna(fut_rate_sma) else np.nan,
            "Fut Vol Attr. Month %_ema": round(fut_rate_ema, 4) if not pd.isna(fut_rate_ema) else np.nan,
            "Fut Vol Attr. Month %_med": round(fut_rate_med, 4) if not pd.isna(fut_rate_med) else np.nan,
            "Fut Vol Attr. Month %_wilder": round(fut_rate_wil, 4) if not pd.isna(fut_rate_wil) else np.nan,
            "Base Month HC": int(last_known_hc) if not pd.isna(last_known_hc) else np.nan,
            "Fut Vol. Attr Count_sma": int(round(fut_count_sma)) if (not pd.isna(fut_count_sma)) else np.nan,
            "Fut Vol. Attr Count_ema": int(round(fut_count_ema)) if (not pd.isna(fut_count_ema)) else np.nan,
            "Fut Vol. Attr Count_med": int(round(fut_count_med)) if (not pd.isna(fut_count_med)) else np.nan,
            "Fut Vol. Attr Count_wilder": int(round(fut_count_wil)) if (not pd.isna(fut_count_wil)) else np.nan,
            "Act Vol Attr. Month %": round(act_rate,4) if not pd.isna(act_rate) else np.nan,
            "Act Vol Attr Count": int(act_exits) if not pd.isna(act_exits) else np.nan,
            "Actual Headcount": int(act_hc) if not pd.isna(act_hc) else np.nan,
        }
        all_rows.append(row)

# Final forecast DataFrame
forecast_df = pd.DataFrame(all_rows)











df['MONTH'] = pd.to_datetime(df['MONTH'].astype(str), format='%Y%m')
df['MONTH'] = df['MONTH'].dt.to_period('M').astype(str)


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# -----------------------
# PARAMETERS
# -----------------------
BASE_MONTH_HC = 229568
FORECAST_START = "202504"
FORECAST_END   = "202512"
DIVIDE_BY_100  = True
ROLL_WINDOW = 3
# -----------------------

# Convert MONTH (yyyymm) → datetime (month start)
df['MONTH'] = pd.to_datetime(df['MONTH'].astype(str), format='%Y%m')
df['YEAR']  = df['MONTH'].dt.year

# Forecast index (month start timestamps)
forecast_index = pd.date_range(start=pd.to_datetime(FORECAST_START, format="%Y%m"),
                               end=pd.to_datetime(FORECAST_END, format="%Y%m"),
                               freq="MS")

groups = df['BUSINESS_GROUPS_TA'].unique()
all_rows = []

for g in groups:
    grp = df[df['BUSINESS_GROUPS_TA'] == g].copy().set_index('MONTH').sort_index()

    # smoothing: SMA, EMA, Rolling Median, Wilder (alpha = 1/N)
    if 'MONTHLY_ATTRITION_RATE' not in grp.columns:
        print(f"⚠️ Warning: MONTHLY_ATTRITION_RATE not in dataset for group {g}")
        continue

    grp['SMA3'] = grp['MONTHLY_ATTRITION_RATE'].rolling(window=ROLL_WINDOW).mean()
    grp['EMA3'] = grp['MONTHLY_ATTRITION_RATE'].ewm(span=ROLL_WINDOW, adjust=False).mean()
    grp['MED3'] = grp['MONTHLY_ATTRITION_RATE'].rolling(window=ROLL_WINDOW).median()
    alpha_wilder = 1.0 / ROLL_WINDOW
    grp['WILDER3'] = grp['MONTHLY_ATTRITION_RATE'].ewm(alpha=alpha_wilder, adjust=False).mean()

    # last-available fallback utility
    def last_valid(col):
        if (col in grp.columns) and (not grp[col].empty):
            v = grp[col].iloc[-1]
            if not pd.isna(v):
                return v
        if (not grp['MONTHLY_ATTRITION_RATE'].empty):
            return grp['MONTHLY_ATTRITION_RATE'].iloc[-1]
        return np.nan

    last_sma = last_valid('SMA3')
    last_ema = last_valid('EMA3')
    last_med = last_valid('MED3')
    last_wil = last_valid('WILDER3')

    # last known headcount BEFORE forecast start
    pre_forecast_hc_series = grp[grp.index < pd.to_datetime(FORECAST_START, format="%Y%m")]['HEADCOUNT']
    if not pre_forecast_hc_series.empty:
        last_known_hc = pre_forecast_hc_series.iloc[-1]
    else:
        last_known_hc = grp['HEADCOUNT'].iloc[-1] if (not grp['HEADCOUNT'].empty) else BASE_MONTH_HC

    # build rows for every forecast_index month
    for fut_ts in forecast_index:
        # actuals if present
        if fut_ts in grp.index:
            act_rate  = grp.at[fut_ts, 'MONTHLY_ATTRITION_RATE'] if 'MONTHLY_ATTRITION_RATE' in grp.columns else np.nan
            act_exits = grp.at[fut_ts, 'VOLUNTARY_EXITS'] if 'VOLUNTARY_EXITS' in grp.columns else np.nan
            act_hc    = grp.at[fut_ts, 'HEADCOUNT'] if 'HEADCOUNT' in grp.columns else np.nan
        else:
            act_rate  = np.nan
            act_exits = np.nan
            act_hc    = np.nan

        # smoothed values
        def get_smoothed(col, last_val):
            if (fut_ts in grp.index) and (col in grp.columns) and (not pd.isna(grp.at[fut_ts, col])):
                return grp.at[fut_ts, col]
            return last_val

        fut_rate_sma = get_smoothed('SMA3', last_sma)
        fut_rate_ema = get_smoothed('EMA3', last_ema)
        fut_rate_med = get_smoothed('MED3', last_med)
        fut_rate_wil = get_smoothed('WILDER3', last_wil)

        # convert % → counts
        def to_count(rate):
            if pd.isna(rate):
                return np.nan
            return (rate / 100.0 * last_known_hc) if DIVIDE_BY_100 else (rate * last_known_hc)

        fut_count_sma = to_count(fut_rate_sma)
        fut_count_ema = to_count(fut_rate_ema)
        fut_count_med = to_count(fut_rate_med)
        fut_count_wil = to_count(fut_rate_wil)

        # final row
        row = {
            "Future Month": fut_ts,
            "Business Group": g,
            "Fut Vol Attr. Month %_sma": round(fut_rate_sma, 4) if not pd.isna(fut_rate_sma) else np.nan,
            "Fut Vol Attr. Month %_ema": round(fut_rate_ema, 4) if not pd.isna(fut_rate_ema) else np.nan,
            "Fut Vol Attr. Month %_med": round(fut_rate_med, 4) if not pd.isna(fut_rate_med) else np.nan,
            "Fut Vol Attr. Month %_wilder": round(fut_rate_wil, 4) if not pd.isna(fut_rate_wil) else np.nan,
            "Base Month HC": int(last_known_hc) if not pd.isna(last_known_hc) else np.nan,
            "Fut Vol. Attr Count_sma": int(round(fut_count_sma)) if (not pd.isna(fut_count_sma)) else np.nan,
            "Fut Vol. Attr Count_ema": int(round(fut_count_ema)) if (not pd.isna(fut_count_ema)) else np.nan,
            "Fut Vol. Attr Count_med": int(round(fut_count_med)) if (not pd.isna(fut_count_med)) else np.nan,
            "Fut Vol. Attr Count_wilder": int(round(fut_count_wil)) if (not pd.isna(fut_count_wil)) else np.nan,
            "Act Vol Attr. Month %": round(act_rate,4) if not pd.isna(act_rate) else np.nan,
            "Act Vol Attr Count": int(act_exits) if not pd.isna(act_exits) else np.nan,
            "Actual Headcount": int(act_hc) if not pd.isna(act_hc) else np.nan,
        }
        all_rows.append(row)

# Final forecast DataFrame
forecast_df = pd.DataFrame(all_rows)







# -----------------------
# PARAMETERS
# -----------------------
BASE_MONTH_HC = 229568
FORECAST_START = "202504"  # YYYYMM
FORECAST_END   = "202512"  # YYYYMM
DIVIDE_BY_100  = True
ROLL_WINDOW = 3
# -----------------------

# Prepare df — internally datetime, plus string for joining/display
df['MONTH'] = pd.to_datetime(df['MONTH'].astype(str), format='%Y-%m', errors='coerce') \
                .dt.to_period('M').dt.to_timestamp()
df['MONTH_YYYYMM'] = df['MONTH'].dt.strftime('%Y%m')   # <-- new
df['YEAR']  = df['MONTH'].dt.year

# forecast index also as datetime
forecast_index = pd.date_range(
    start=pd.to_datetime(FORECAST_START, format='%Y%m'),
    end=pd.to_datetime(FORECAST_END, format='%Y%m'),
    freq="MS"
)

groups = df['BUSINESS_GROUPS_TA'].unique()










import pandas as pd
import numpy as np

# --- Convert PERIOD (YYYYMM) to proper datetime ---
df['PERIOD'] = pd.to_datetime(df['PERIOD'].astype(str), format='%Y%m')
df['YEAR'] = df['PERIOD'].dt.year

# --- Forecast index (if needed) ---
forecast_index = pd.date_range(start=FORECAST_START, end=FORECAST_END, freq="MS")
groups = df['BUSINESS_GROUPS_TA'].unique()

all_rows = []

# --- Parameters ---
ROLL_WINDOW = 3    # example rolling window
alpha_wilder = 1 / ROLL_WINDOW

# --- Define Zero Lag EMA ---
def zlema(series, period):
    ema1 = series.ewm(span=period, adjust=False).mean()
    lag = int(np.floor((period - 1) / 2))
    lag_series = series.shift(lag)
    zlema_series = (2 * series - lag_series).ewm(span=period, adjust=False).mean()
    return zlema_series

# --- Define DEMA ---
def dema(series, period):
    ema1 = series.ewm(span=period, adjust=False).mean()
    ema2 = ema1.ewm(span=period, adjust=False).mean()
    return 2 * ema1 - ema2

# --- Define T3 ---
def t3(series, period, vfactor=0.7):
    e1 = series.ewm(span=period, adjust=False).mean()
    e2 = e1.ewm(span=period, adjust=False).mean()
    e3 = e2.ewm(span=period, adjust=False).mean()
    e4 = e3.ewm(span=period, adjust=False).mean()
    e5 = e4.ewm(span=period, adjust=False).mean()
    e6 = e5.ewm(span=period, adjust=False).mean()
    c1 = -vfactor ** 3
    c2 = 3 * (vfactor ** 2 + vfactor ** 3)
    c3 = -3 * (vfactor + vfactor ** 2 + vfactor ** 3)
    c4 = 1 + 3 * vfactor + vfactor ** 3 + 3 * vfactor ** 2
    return c1 * e6 + c2 * e5 + c3 * e4 + c4 * e3

# --- Example Kalman filter (simple) ---
def kalman_filter(series):
    n = len(series)
    Q = 1e-5  # process variance
    xhat = np.zeros(n)
    P = np.zeros(n)
    xhatminus = np.zeros(n)
    Pminus = np.zeros(n)
    K = np.zeros(n)
    xhat[0] = series.iloc[0]
    P[0] = 1.0
    for k in range(1, n):
        xhatminus[k] = xhat[k-1]
        Pminus[k] = P[k-1] + Q
        K[k] = Pminus[k] / (Pminus[k] + 1)
        xhat[k] = xhatminus[k] + K[k] * (series.iloc[k] - xhatminus[k])
        P[k] = (1 - K[k]) * Pminus[k]
    return pd.Series(xhat, index=series.index)

# --- Loop by Business Group ---
for g in groups:
    grp = df[df['BUSINESS_GROUPS_TA'] == g].copy().set_index('PERIOD').sort_index()

    # Smoothing calculations
    grp['SMA'] = grp['NEW_OPENINGS_COUNT'].rolling(window=ROLL_WINDOW).mean()
    grp['EMA'] = grp['NEW_OPENINGS_C]()






import pandas as pd

# --- Example: df1 already loaded ---
# df1 = pd.read_excel('monthwise_updated_attrition_dataset.xlsx')

# --- Step 1: Convert MONTH from YYYY-MM to YYYYMM format ---
df1['MONTH_YYYYMM'] = pd.to_datetime(df1['MONTH']).dt.strftime('%Y%m').astype(int)

# --- Step 2: Filter only the columns we need (MONTH + HEADCOUNT) ---
df_headcount = df1[['MONTH_YYYYMM', 'HEADCOUNT']]

# --- Step 3: Assume grouped_df already loaded ---
# grouped_df has a column 'MONTH' in YYYYMM format
# Make sure its MONTH column is also int
grouped_df['MONTH'] = grouped_df['MONTH'].astype(int)

# --- Step 4: Left join on MONTH ---
merged_df = pd.merge(
    grouped_df,
    df_headcount,
    how='left',
    left_on='MONTH',
    right_on='MONTH_YYYYMM'
)

# --- Step 5: Drop helper column if needed ---
merged_df.drop(columns=['MONTH_YYYYMM'], inplace=True)

# --- Check ---
print(merged_df.head())





import pandas as pd

# assuming your dataframe is named dft as in the screenshot
# Group by PERIOD and BUSINESS_GROUPS_TA, summing the OPENINGS_AVAILABLE_EX_CAMP
grouped_df = (
    dft
    .groupby(['PERIOD', 'BUSINESS_GROUPS_TA'], as_index=False)['OPENINGS_AVAILABLE_EX_CAMP']
    .sum()
    .rename(columns={'OPENINGS_AVAILABLE_EX_CAMP': 'NEW_OPENINGS_COUNT'})
)

# This will give you total new openings per period per business group
print(grouped_df.head())






1️⃣ What the plot is showing

Each node (circle) = a business group.

Node size = CURRENT_HEADCOUNT — bigger circle = more employees.

Node color (red shade) = attrition_frac — darker red = higher voluntary attrition rate.

Lines between nodes (edges) = similarity of risk profiles between business groups (based on attrition + error %).

Edge thickness = how strongly similar they are.



import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

COL_BUS = 'BUSINESS_GROUPS'
COL_HC = 'CURRENT_HEADCOUNT'
COL_PROJ_EXIST = 'PROJECTED_VOL'
COL_PROJ_NEW = 'New Projected Vol'
COL_ACTUAL_VOL = 'Actual Vol Terms'
COL_PROJ_HC = 'PROJECTED_HEADCOUNT'
COL_PROJ_HC_NEW = 'New Projected Headcount'
COL_ACTUAL_HC_JUNE = 'Actual Headcount June'
COL_PCT_NEW_VS_ACT = '% (new) Vs Actuals'

df = pd.read_csv("your_input.csv")

# -----------------------------
# 1. Attrition Funnel
# -----------------------------
totals = {
    "Current HC": df[COL_HC].sum(),
    "Proj Exits (Exist)": df[COL_PROJ_EXIST].sum(),
    "Proj Exits (New)": df[COL_PROJ_NEW].sum(),
    "Actual Exits": df[COL_ACTUAL_VOL].sum(),
}
plt.figure(figsize=(8,6))
plt.plot(list(totals.keys()), list(totals.values()), marker="o", linewidth=3, color="teal")
plt.fill_between(list(totals.keys()), list(totals.values()), alpha=0.2, color="teal")
plt.title("Attrition Funnel: Headcount → Projections → Actuals")
plt.ylabel("Headcount")
plt.tight_layout()
plt.savefig("funnel_attrition.png")
plt.close()

# -----------------------------
# 2. Radar Chart (per Business Group)
# -----------------------------
import math

groups = df[COL_BUS].unique()[:6]  # top 6 groups for clarity
labels = ["Proj Exist", "Proj New", "Actual"]

angles = np.linspace(0, 2*np.pi, len(labels), endpoint=False).tolist()
angles += angles[:1]

plt.figure(figsize=(6,6))
ax = plt.subplot(111, polar=True)
for g in groups:
    vals = df.loc[df[COL_BUS]==g, [COL_PROJ_EXIST, COL_PROJ_NEW, COL_ACTUAL_VOL]].values.flatten().tolist()
    vals += vals[:1]
    ax.plot(angles, vals, label=g, linewidth=2)
    ax.fill(angles, vals, alpha=0.15)

ax.set_xticks(angles[:-1])
ax.set_xticklabels(labels)
plt.title("Radar Chart: Projections vs Actual (Sample Groups)")
plt.legend(bbox_to_anchor=(1.1,1.05))
plt.tight_layout()
plt.savefig("radar_projection.png")
plt.close()

# -----------------------------
# 3. Accuracy Gauge (Speedometer)
# -----------------------------
ema_err = abs(df[COL_PROJ_NEW].sum() - df[COL_ACTUAL_VOL].sum())
exist_err = abs(df[COL_PROJ_EXIST].sum() - df[COL_ACTUAL_VOL].sum())

ema_acc = max(0, 100 - (ema_err/df[COL_ACTUAL_VOL].sum()*100))
exist_acc = max(0, 100 - (exist_err/df[COL_ACTUAL_VOL].sum()*100))

fig, ax = plt.subplots(figsize=(6,3), subplot_kw={'projection':'polar'})
theta = np.linspace(-np.pi/2, np.pi/2, 100)
r = np.ones_like(theta)
ax.plot(theta, r, linewidth=10, color="lightgray")

# EMA pointer
ax.plot([(-np.pi/2)+(ema_acc/100*np.pi)], [1], marker="o", markersize=15, color="green")
# Existing pointer
ax.plot([(-np.pi/2)+(exist_acc/100*np.pi)], [1], marker="o", markersize=15, color="red")

ax.set_rticks([]); ax.set_xticks([])
ax.set_yticklabels([]); ax.set_xticklabels([])
plt.title(f"Prediction Accuracy: EMA {ema_acc:.1f}% | Existing {exist_acc:.1f}%")
plt.tight_layout()
plt.savefig("accuracy_gauge.png")
plt.close()

# -----------------------------
# 4. Treemap of Attrition
# -----------------------------
import squarify

sizes = df[COL_HC]
colors = df[COL_PCT_NEW_VS_ACT]
labels = [f"{b}\nHC:{hc}\nErr:{err:.1f}%" for b,hc,err in zip(df[COL_BUS], sizes, colors)]

plt.figure(figsize=(12,8))
squarify.plot(sizes=sizes, label=labels, color=sns.color_palette("coolwarm", len(sizes)), alpha=0.8)
plt.axis("off")
plt.title("Treemap: Business Groups by Headcount (colored by % error New vs Actual)")
plt.tight_layout()
plt.savefig("treemap_attrition.png")
plt.close()

# -----------------------------
# 5. Scenario Simulation Curve
# -----------------------------
base = df[COL_PROJ_NEW].sum()
actual = df[COL_ACTUAL_VOL].sum()
x = np.linspace(-20, 20, 41)  # +/- 20% deviation
y = df[COL_HC].sum() - (base * (1 + x/100))

plt.figure(figsize=(8,6))
plt.plot(x, y, color="blue", linewidth=2)
plt.axvline(0, color="gray", linestyle="--")
plt.axhline(df[COL_ACTUAL_HC_JUNE].sum(), color="red", linestyle="--", label="Actual June HC")
plt.title("Scenario Simulation: Headcount Impact if Attrition deviates from EMA")
plt.xlabel("Deviation from EMA Projection (%)")
plt.ylabel("Remaining Headcount")
plt.legend()
plt.tight_layout()
plt.savefig("scenario_simulation.png")
plt.close()


























import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os

# -----------------------------
# Config
# -----------------------------
COL_BUS = 'BUSINESS_GROUPS'
COL_HC = 'CURRENT_HEADCOUNT'
COL_ATTR = 'VOLUNTARY_ATTRITION_RATE'
COL_PROJ_EXIST = 'PROJECTED_VOL'
COL_PROJ_NEW = 'New Projected Vol'
COL_ACTUAL_VOL = 'Actual Vol Terms'
COL_PCT_EXIST_VS_ACT = '% (Existing) Vs Actuals'
COL_PCT_NEW_VS_ACT = '% (new) Vs Actuals'
COL_PROJ_HC = 'PROJECTED_HEADCOUNT'
COL_PROJ_HC_NEW = 'New Projected Headcount'
COL_ACTUAL_HC_JUNE = 'Actual Headcount June'
COL_PROJ_EXIST_VS_JUNE = 'Projected (Existing Vs June)'
COL_PROJ_NEW_VS_JUNE = 'Projected (New Vs June)'

OUTDIR = "hr_viz_static"
os.makedirs(OUTDIR, exist_ok=True)

# -----------------------------
# Load data
# -----------------------------
df = pd.read_csv("your_input.csv")

# -----------------------------
# KPI summary (big picture bar chart)
# -----------------------------
totals = {
    "Existing Projection": df[COL_PROJ_EXIST].sum(),
    "New (EMA) Projection": df[COL_PROJ_NEW].sum(),
    "Actual Terms": df[COL_ACTUAL_VOL].sum()
}
plt.figure(figsize=(8,5))
sns.barplot(x=list(totals.keys()), y=list(totals.values()), palette="Set2")
plt.title("Overall Attrition Terms: Existing vs EMA vs Actual")
plt.ylabel("Headcount")
plt.tight_layout()
plt.savefig(f"{OUTDIR}/kpi_summary.png")
plt.close()

# -----------------------------
# Business group comparison (clustered bars)
# -----------------------------
df_group = df[[COL_BUS, COL_PROJ_EXIST, COL_PROJ_NEW, COL_ACTUAL_VOL]].set_index(COL_BUS)
df_group.plot(kind="bar", figsize=(12,6))
plt.title("Business Group: Projected vs Actual Attrition")
plt.ylabel("Headcount")
plt.xticks(rotation=45, ha="right")
plt.tight_layout()
plt.savefig(f"{OUTDIR}/group_comparison.png")
plt.close()

# -----------------------------
# Prediction error heatmap
# -----------------------------
df["Error_New"] = df[COL_PROJ_NEW] - df[COL_ACTUAL_VOL]
df["Error_Exist"] = df[COL_PROJ_EXIST] - df[COL_ACTUAL_VOL]

err_mat = df[[COL_BUS, "Error_New", "Error_Exist"]].set_index(COL_BUS)
plt.figure(figsize=(10,6))
sns.heatmap(err_mat, annot=True, fmt=".0f", cmap="coolwarm", center=0)
plt.title("Prediction Error Heatmap (Projection - Actual)")
plt.tight_layout()
plt.savefig(f"{OUTDIR}/prediction_error_heatmap.png")
plt.close()

# -----------------------------
# Pareto chart for EMA projections
# -----------------------------
df_pareto = df.groupby(COL_BUS)[COL_PROJ_NEW].sum().sort_values(ascending=False)
cum_pct = df_pareto.cumsum() / df_pareto.sum() * 100

fig, ax1 = plt.subplots(figsize=(12,6))
df_pareto.plot(kind="bar", ax=ax1, color="skyblue")
ax1.set_ylabel("New Projected Vol")
ax2 = ax1.twinx()
cum_pct.plot(ax=ax2, color="red", marker="o")
ax2.set_ylabel("Cumulative %")
plt.title("Pareto Analysis: New Projected Vol (EMA)")
plt.tight_layout()
plt.savefig(f"{OUTDIR}/pareto_new_proj.png")
plt.close()

# -----------------------------
# Risk scatter (quadrant style)
# -----------------------------
plt.figure(figsize=(8,6))
x = df[COL_ATTR]*100
y = df[COL_PCT_NEW_VS_ACT]
sizes = df[COL_HC]/df[COL_HC].max()*1000
plt.scatter(x, y, s=sizes, alpha=0.6, c="teal", edgecolor="k")

# quadrant lines
plt.axhline(y.median(), color="gray", linestyle="--")
plt.axvline(x.median(), color="gray", linestyle="--")

for i, row in df.iterrows():
    plt.text(row[COL_ATTR]*100, row[COL_PCT_NEW_VS_ACT], row[COL_BUS], fontsize=8)

plt.xlabel("Voluntary Attrition Rate (%)")
plt.ylabel("% (New) Vs Actuals")
plt.title("Risk Quadrant: Attrition vs Prediction Gap")
plt.tight_layout()
plt.savefig(f"{OUTDIR}/risk_quadrant.png")
plt.close()

# -----------------------------
# Distribution of % error
# -----------------------------
plt.figure(figsize=(8,6))
sns.violinplot(y=df[COL_PCT_NEW_VS_ACT], inner="box", color="lightblue")
sns.swarmplot(y=df[COL_PCT_NEW_VS_ACT], color="k", size=4)
plt.title("Distribution of % (New) Vs Actuals")
plt.ylabel("Prediction Error %")
plt.tight_layout()
plt.savefig(f"{OUTDIR}/error_distribution.png")
plt.close()

print(f"All static figures saved in: {OUTDIR}")




























# Copy-paste this cell into Jupyter and run (make sure df is loaded with the exact columns you listed)
import pandas as pd
import numpy as np
import networkx as nx
from sklearn.metrics.pairwise import cosine_similarity

# ---------- Helpers to parse input columns robustly ----------
def to_fraction(col):
    ser = pd.Series(col).astype(str).str.replace('%','').str.replace(',','').str.strip()
    num = pd.to_numeric(ser, errors='coerce').fillna(0.0)
    if (num.max() > 1.5):
        num = num / 100.0
    return num

def to_percent_number(col):
    ser = pd.Series(col).astype(str).str.replace('%','').str.replace(',','').str.strip()
    num = pd.to_numeric(ser, errors='coerce').fillna(0.0)
    if (num.max() <= 1.5):
        num = num * 100.0
    return num

# ---------- Core function ----------
def interpret_attrition_network(df,
                                sim_threshold=0.9,
                                headcount_pctile=75,
                                attrition_pctile=75,
                                error_pctile=75,
                                verbose=True):
    """
    Build similarity graph from (attrition_frac, new_error_pct), compute metrics,
    and produce CEO-level interpretation per group and per-cluster tables.

    Inputs:
      - df: dataframe with the exact columns you provided.
      - sim_threshold: cosine similarity threshold to create an edge between groups.
      - headcount_pctile / attrition_pctile / error_pctile: percentiles used to detect 'high' flags.
    Returns:
      - group_table (DataFrame): one row per BUSINESS_GROUPS with metrics, flags, suggested_action, exec_summary
      - cluster_table (DataFrame): per-cluster aggregated summary and recommended action
    """
    # Required columns (exact names)
    req = [
        'BUSINESS_GROUPS','CURRENT_HEADCOUNT','VOLUNTARY_ATTRITION_RATE',
        'New Projected Vol','Actual Vol Terms','PROJECTED_VOL',
        '% (Existing) Vs Actuals','% (new) Vs Actuals',
        'PROJECTED_HEADCOUNT','New Projected Headcount','Actual Headcount June',
        'Projected (Existing Vs June)','Projected (New Vs June)'
    ]
    miss = [c for c in req if c not in df.columns]
    if miss:
        raise ValueError("Missing required columns (exact names): " + ", ".join(miss))

    d = df.copy()

    # Parse numeric fields we will use
    d['CURRENT_HEADCOUNT'] = pd.to_numeric(d['CURRENT_HEADCOUNT'], errors='coerce').fillna(0.0)
    d['attrition_frac'] = to_fraction(d['VOLUNTARY_ATTRITION_RATE'])
    d['new_error_pct'] = to_percent_number(d['% (new) Vs Actuals']).fillna(0.0)    # percent (e.g. 5.0)
    d['existing_error_pct'] = to_percent_number(d['% (Existing) Vs Actuals']).fillna(0.0)
    d['attrition_count_expected'] = (d['CURRENT_HEADCOUNT'] * d['attrition_frac']).round(1)
    d['net_change_new_vs_june'] = d['New Projected Headcount'].astype(float) - d['Actual Headcount June'].astype(float)
    d['abs_new_error_pct'] = d['new_error_pct'].abs()

    # Build similarity graph on (attrition_frac, new_error_pct)
    features = np.vstack([d['attrition_frac'].astype(float), d['new_error_pct'].astype(float)]).T
    sim = cosine_similarity(features)
    np.fill_diagonal(sim, 0.0)

    G = nx.Graph()
    for i, gname in enumerate(d['BUSINESS_GROUPS']):
        G.add_node(gname,
                   headcount=float(d.loc[i, 'CURRENT_HEADCOUNT']),
                   attrition=float(d.loc[i, 'attrition_frac']),
                   new_error=float(d.loc[i, 'new_error_pct']))

    # add edges where similarity > threshold
    n = len(d)
    for i in range(n):
        for j in range(i+1, n):
            if sim[i, j] >= sim_threshold:
                G.add_edge(d.loc[i, 'BUSINESS_GROUPS'], d.loc[j, 'BUSINESS_GROUPS'], weight=float(sim[i,j]))

    # Graph metrics
    degree = dict(G.degree())
    # weighted degree = sum of weights of edges incident on node (use 0 if no edges)
    weighted_degree = {}
    for node in G.nodes():
        s = 0.0
        for nbr in G.neighbors(node):
            s += G.edges[node, nbr].get('weight', 1.0)
        weighted_degree[node] = s
    deg_cent = nx.degree_centrality(G) if G.number_of_nodes()>0 else {n:0 for n in G.nodes()}
    btw = nx.betweenness_centrality(G) if G.number_of_nodes()>0 else {n:0 for n in G.nodes()}
    cls = nx.closeness_centrality(G) if G.number_of_nodes()>0 else {n:0 for n in G.nodes()}

    # Cluster / connected component analysis
    components = list(nx.connected_components(G))
    # Map node -> comp_id
    node2comp = {}
    comp_summaries = []
    for cid, comp in enumerate(components, start=1):
        comp_nodes = list(comp)
        comp_headcount = sum([G.nodes[n]['headcount'] for n in comp_nodes])
        comp_attr = np.mean([G.nodes[n]['attrition'] for n in comp_nodes])
        comp_error = np.mean([G.nodes[n]['new_error'] for n in comp_nodes])
        comp_summaries.append({
            'cluster_id': cid,
            'n_groups': len(comp_nodes),
            'groups': comp_nodes,
            'total_headcount': comp_headcount,
            'avg_attrition_frac': comp_attr,
            'avg_new_error_pct': comp_error
        })
        for n in comp_nodes:
            node2comp[n] = cid
    # Nodes not in any component (isolated nodes) become single-node clusters
    isolated = [n for n in G.nodes() if n not in node2comp]
    for iso in isolated:
        cid = len(comp_summaries) + 1
        node2comp[iso] = cid
        comp_summaries.append({
            'cluster_id': cid,
            'n_groups': 1,
            'groups': [iso],
            'total_headcount': G.nodes[iso]['headcount'],
            'avg_attrition_frac': G.nodes[iso]['attrition'],
            'avg_new_error_pct': G.nodes[iso]['new_error']
        })

    # Build per-group table
    rows = []
    # thresholds using percentiles (plus sensible minimums)
    headcount_high_thresh = np.percentile(d['CURRENT_HEADCOUNT'], headcount_pctile)
    attrition_high_thresh = max(np.percentile(d['attrition_frac'], attrition_pctile), 0.07)  # at least 7%
    error_high_thresh = np.percentile(d['abs_new_error_pct'], error_pctile)

    # For composite risk score: normalize three signals
    def minmax(s):
        if s.max()==s.min():
            return (s*0.0)
        return (s - s.min()) / (s.max() - s.min())

    norm_attr = minmax(d['attrition_frac'])
    norm_head = minmax(d['CURRENT_HEADCOUNT'])
    norm_err  = minmax(d['abs_new_error_pct'])
    composite = 0.55*norm_attr + 0.35*norm_head + 0.10*norm_err

    for i, row in d.iterrows():
        name = row['BUSINESS_GROUPS']
        hc = float(row['CURRENT_HEADCOUNT'])
        attr = float(row['attrition_frac'])
        attr_pct = attr * 100.0
        err = float(row['new_error_pct'])
        deg = degree.get(name, 0)
        wdeg = weighted_degree.get(name, 0.0)
        dc = deg_cent.get(name, 0.0)
        bw = btw.get(name, 0.0)
        cl = cls.get(name, 0.0)
        comp_id = node2comp.get(name, None)
        comp_info = next((c for c in comp_summaries if c['cluster_id']==comp_id), None)

        # flags
        is_large = hc >= headcount_high_thresh
        is_high_attr = attr >= attrition_high_thresh
        is_high_error = abs(err) >= error_high_thresh
        is_high_degree = deg >= np.percentile(list(degree.values()) if degree else [0], 75) if degree else False

        # suggested action rules (simple template)
        actions = []
        if is_large and is_high_attr:
            actions.append("Immediate retention program & leadership review")
            actions.append("Comp/benefits audit for this group")
        elif is_large and is_high_error:
            actions.append("Model review & local forecast adjustment (large group)")
        elif is_high_attr:
            actions.append("Targeted retention actions (stay interviews, incentives)")
        elif is_high_error:
            actions.append("Model tuning: EMA under/over-performs; investigate drivers")
        else:
            actions.append("Monitor; maintain current programs")

        # cluster-level augmentation
        if comp_info and comp_info['avg_attrition_frac'] >= attrition_high_thresh and comp_info['n_groups']>1:
            actions.append("Cluster-level intervention recommended (systemic driver likely)")

        # centrality advice
        if dc >= np.percentile(list(dc for dc in deg_cent.values()) if deg_cent else [0], 75):
            actions.append("High centrality: stabilizing this group can reduce system risk")

        suggested_action = " / ".join(actions)

        # auto-generated CEO-level sentence (concise)
        neighbors = list(G.neighbors(name)) if name in G.nodes() else []
        neighbor_text = ""
        if neighbors:
            neighbor_text = f" Similar groups: {', '.join(neighbors[:4]) + ('...' if len(neighbors)>4 else '')}."
        cluster_text = ""
        if comp_info:
            cluster_text = f" Part of cluster #{comp_id} (size={comp_info['n_groups']}, total HC={int(comp_info['total_headcount'])})."
        exec_summary = (f"{name}: {('LARGE' if is_large else 'SMALLER')} team (HC={int(hc)}), "
                        f"attrition ~ {attr_pct:.1f}%. Model error ~ {err:.1f}%.{neighbor_text}{cluster_text} "
                        f"Recommended: {suggested_action}.")

        rows.append({
            'BUSINESS_GROUPS': name,
            'CURRENT_HEADCOUNT': int(hc),
            'Attrition_pct': attr_pct,
            'New_error_pct': err,
            'Degree': deg,
            'Weighted_degree': round(wdeg, 3),
            'Degree_centrality': round(dc, 3),
            'Betweenness': round(bw, 4),
            'Cluster_id': comp_id,
            'Cluster_size': comp_info['n_groups'] if comp_info else 1,
            'Cluster_total_headcount': int(comp_info['total_headcount']) if comp_info else int(hc),
            'Is_large_headcount': bool(is_large),
            'Is_high_attrition': bool(is_high_attr),
            'Is_high_error': bool(is_high_error),
            'Composite_risk': round(float(composite.iloc[i]), 3),
            'Suggested_action': suggested_action,
            'Exec_summary': exec_summary
        })

    group_table = pd.DataFrame(rows).sort_values('Composite_risk', ascending=False).reset_index(drop=True)

    # Build cluster summary table
    crows = []
    for c in comp_summaries:
        cid = c['cluster_id']
        avg_attr_pct = c['avg_attrition_frac']*100.0
        avg_err = c['avg_new_error_pct']
        rec = "Cluster-level review" if (c['avg_attrition_frac'] >= attrition_high_thresh and c['n_groups']>1) else "No cluster alarm"
        # escalate if very large cluster headcount
        if c['total_headcount'] >= np.percentile(d['CURRENT_HEADCOUNT'], 75):
            rec += " / High-impact cluster"
        crows.append({
            'cluster_id': cid,
            'n_groups': c['n_groups'],
            'groups': ", ".join(c['groups']),
            'total_headcount': int(c['total_headcount']),
            'avg_attrition_pct': round(avg_attr_pct, 2),
            'avg_new_error_pct': round(avg_err, 2),
            'recommended_action': rec
        })
    cluster_table = pd.DataFrame(crows).sort_values('total_headcount', ascending=False).reset_index(drop=True)

    # Top-level textual executive summary
    top_by_risk = group_table.head(5)
    top_list = "; ".join([f"{r['BUSINESS_GROUPS']} (HC={r['CURRENT_HEADCOUNT']}, Attr={r['Attrition_pct']:.1f}%)" for _, r in top_by_risk.iterrows()])

    overall_summary = (
        f"Executive Summary:\n"
        f"- Top risk groups (by composite score): {top_list}.\n"
        f"- Number of detected clusters (connected groups): {len(comp_summaries)}. "
        f"{sum(1 for c in comp_summaries if c['avg_attrition_frac']>=attrition_high_thresh)} clusters have above-threshold attrition.\n"
        f"- Thresholds used: headcount >= {int(headcount_high_thresh)} (>= {headcount_pctile}th pct), "
        f"attrition >= {attrition_high_thresh:.2%}, error >= {error_high_thresh:.2f}%.\n"
        f"- Immediate recommended actions: prioritize the big & high-attrition groups for retention programs and leadership reviews; "
        f"investigate groups where the new model error is high (model tuning/local adjustments)."
    )

    if verbose:
        print(overall_summary)
        print("\nTop 10 groups with suggested action (first 10 rows):\n")
        display(group_table.head(10))
        print("\nCluster summary:\n")
        display(cluster_table)

    return group_table, cluster_table

# ---------- Example usage ----------
# group_table, cluster_table = interpret_attrition_network(df, sim_threshold=0.9)
# group_table.to_csv('attrition_group_interpretation.csv', index=False)
# cluster_table.to_csv('attrition_cluster_summary.csv', index=False)























# Robust Network Graph with reliable colorbar (use in Jupyter)
import numpy as np
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
import matplotlib as mpl
from sklearn.metrics.pairwise import cosine_similarity

# ---------- (OPTIONAL) sample df for quick test ----------
# Comment out if you already have df loaded
# df = pd.DataFrame({
#     'BUSINESS_GROUPS': ['A','B','C','D'],
#     'CURRENT_HEADCOUNT': [120, 45, 300, 80],
#     'attrition_frac': [0.07, 0.12, 0.03, 0.20],
#     'new_error_pct': [5.0, 12.0, 3.0, 8.0]
# })

# ---------- Validate df columns ----------
req = ['BUSINESS_GROUPS','CURRENT_HEADCOUNT','attrition_frac','new_error_pct']
missing = [c for c in req if c not in df.columns]
if missing:
    raise ValueError("Missing required columns in df: " + ", ".join(missing))

# ---------- Compute pairwise similarity and build graph ----------
features = np.c_[df['attrition_frac'].astype(float), df['new_error_pct'].astype(float)]
sim = cosine_similarity(features)
np.fill_diagonal(sim, 0.0)
threshold = 0.9  # tweak as needed

G = nx.Graph()
for i, gname in enumerate(df['BUSINESS_GROUPS']):
    G.add_node(gname,
               size=float(df['CURRENT_HEADCOUNT'].iloc[i]),
               color=float(df['attrition_frac'].iloc[i]) )

for i in range(len(df)):
    for j in range(i+1, len(df)):
        if sim[i, j] > threshold:
            G.add_edge(df['BUSINESS_GROUPS'].iloc[i],
                       df['BUSINESS_GROUPS'].iloc[j],
                       weight=float(sim[i, j]))

if G.number_of_nodes() == 0:
    raise RuntimeError("Graph has no nodes (check df).")

# ---------- Layout and drawing ----------
fig, ax = plt.subplots(figsize=(10, 8))
pos = nx.spring_layout(G, k=0.5, seed=42)  # reproducible layout

# node sizes and colors
nodes_list = list(G.nodes())
sizes = [G.nodes[n]['size'] * 5 for n in nodes_list]   # scale factor
colors = [G.nodes[n]['color'] for n in nodes_list]

# fix degenerate vmin/vmax
vmin = min(colors)
vmax = max(colors)
if np.isclose(vmin, vmax):
    vmin = vmin - 1e-3
    vmax = vmax + 1e-3

# draw nodes (use draw_networkx_nodes which returns a PathCollection)
nodes_pc = nx.draw_networkx_nodes(G, pos,
                                  nodelist=nodes_list,
                                  node_size=sizes,
                                  node_color=colors,
                                  cmap='Reds',
                                  vmin=vmin,
                                  vmax=vmax,
                                  ax=ax)
nx.draw_networkx_labels(G, pos, ax=ax, font_size=10)
# draw edges with width proportional to weight (if present)
if G.number_of_edges() > 0:
    widths = [G[u][v].get('weight', 0.2) * 3.0 for u, v in G.edges()]
    nx.draw_networkx_edges(G, pos, ax=ax, edge_color='gray', width=widths, alpha=0.7)

ax.set_title("Business Group Risk Map (size=headcount, color=attrition%)")
ax.axis('off')

# ---------- Add colorbar robustly ----------
try:
    # preferred: create ScalarMappable with same cmap and norm then attach to the figure
    norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)
    sm = mpl.cm.ScalarMappable(cmap=mpl.cm.get_cmap('Reds'), norm=norm)
    # set_array must be non-empty to avoid ambiguous mappable behavior in some backends
    sm.set_array(np.asarray(colors))
    # attach colorbar to the figure, specifying the axis to steal space from
    cbar = fig.colorbar(sm, ax=ax, fraction=0.046, pad=0.04)
    cbar.set_label('Attrition Fraction')
except Exception as e:
    # fallback: use scatter + colorbar (more explicit mappable)
    print("Primary colorbar method failed, using fallback scatter-based colorbar. Error:", e)
    xs = [pos[n][0] for n in nodes_list]
    ys = [pos[n][1] for n in nodes_list]
    sc = ax.scatter(xs, ys, s=sizes, c=colors, cmap='Reds', vmin=vmin, vmax=vmax)
    cbar = fig.colorbar(sc, ax=ax, fraction=0.046, pad=0.04)
    cbar.set_label('Attrition Fraction')

plt.show()























import numpy as np
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib as mpl

# Example dummy df (replace with your real df)
# df = pd.DataFrame({
#     'BUSINESS_GROUPS':['A','B','C'],
#     'CURRENT_HEADCOUNT':[100,200,150],
#     'attrition_frac':[0.1,0.25,0.4],
#     'new_error_pct':[0.05,0.2,0.1]
# })

# 1️⃣ Compute similarity matrix
features = np.c_[df['attrition_frac'], df['new_error_pct']]
sim = cosine_similarity(features)
np.fill_diagonal(sim, 0)
threshold = 0.9

# 2️⃣ Build Graph
G = nx.Graph()
for i, g in enumerate(df['BUSINESS_GROUPS']):
    G.add_node(
        g,
        size=df['CURRENT_HEADCOUNT'].iloc[i],
        color=df['attrition_frac'].iloc[i]
    )

for i in range(len(df)):
    for j in range(i + 1, len(df)):
        if sim[i, j] > threshold:
            G.add_edge(df['BUSINESS_GROUPS'].iloc[i],
                       df['BUSINESS_GROUPS'].iloc[j],
                       weight=sim[i, j])

# 3️⃣ Prepare node attributes
pos = nx.spring_layout(G, k=0.5)
sizes = [G.nodes[n]['size'] * 5 for n in G.nodes()]
colors = [G.nodes[n]['color'] for n in G.nodes()]

# 4️⃣ Draw graph
plt.figure(figsize=(10, 8))
nodes = nx.draw_networkx_nodes(G, pos, node_size=sizes,
                               node_color=colors, cmap=plt.cm.Reds)
nx.draw_networkx_labels(G, pos)
nx.draw_networkx_edges(G, pos, edge_color='gray')

plt.title("Business Group Risk Map (size=headcount, color=attrition%)")

# 5️⃣ Add colorbar properly
norm = mpl.colors.Normalize(vmin=min(colors), vmax=max(colors))
sm = mpl.cm.ScalarMappable(cmap=plt.cm.Reds, norm=norm)
sm.set_array([])  # needed for colorbar
cbar = plt.colorbar(sm)
cbar.set_label('Attrition Fraction')

plt.show()
