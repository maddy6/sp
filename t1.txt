1. Logarithmic Transformations
When to use: Apply to features with a right-skewed distribution or exponential trends.
Implementation:
python
Copy
Edit
data["log_high_density_swipe"] = (data["high_density_swipe_score"] + 1e-6).log()
data["log_cnt_cnp_60days"] = (data["cnt_cnp_60days"] + 1).log()
2. Square Root Transformations
When to use: For moderately skewed data or to reduce feature magnitudes while retaining relative differences.
Implementation:
python
Copy
Edit
data["sqrt_pct_cnp_amt_60days"] = data["pct_cnp_amt_60days"].sqrt()
data["sqrt_auth_decline_ratio"] = data["auth_decline_ratio"].sqrt()
3. Polynomial Transformations
When to use: To model non-linear relationships by introducing squared or cubic terms.
Implementation:
python
Copy
Edit
data["high_density_swipe_sq"] = data["high_density_swipe_score"] ** 2
data["auth_decline_ratio_cubed"] = data["auth_decline_ratio"] ** 3
4. Reciprocal Transformations
When to use: For features where smaller values have higher significance (e.g., inverse distances).
Implementation:
python
Copy
Edit
data["reciprocal_dist_miles_btw_swp"] = 1 / (data["dist_miles_btw_swp"] + 1e-6)
5. Exponential Transformations
When to use: To emphasize differences in small feature values.
Implementation:
python
Copy
Edit
data["exp_high_density_swipe"] = data["high_density_swipe_score"].exp()
6. Sigmoid Transformation
When to use: For normalizing features to a [0, 1] range while emphasizing extremes.
Formula: 
sigmoid
(
ùë•
)
=
1
1
+
ùëí
‚àí
ùë•
sigmoid(x)= 
1+e 
‚àíx
 
1
‚Äã
 
Implementation:
python
Copy
Edit
data["sigmoid_auth_decline_ratio"] = 1 / (1 + (-data["auth_decline_ratio"]).exp())
7. Interactions Between Features
Combine features to uncover complex relationships:
Multiplicative Interaction:
python
Copy
Edit
data["interaction_1"] = data["high_density_swipe_score"] * data["cnt_cnp_60days"]
data["interaction_2"] = data["auth_decline_ratio"] * data["pct_cnp_amt_60days"]
Division Interaction:
python
Copy
Edit
data["interaction_3"] = data["pct_cnp_amt_60days"] / (data["dist_miles_btw_swp"] + 1e-6)
8. Quantile Transformation
When to use: To map feature values into quantile ranges (e.g., deciles or quartiles).
Implementation:
python
Copy
Edit
data["high_density_swipe_quantile"] = data["high_density_swipe_score"].quantile([0.25, 0.5, 0.75])
9. Weighted Decay
When to use: For features where recent values are more important than older ones.
Implementation:
python
Copy
Edit
alpha = 0.9  # Decay factor
data["weighted_cnt_cnp_60days"] = data["cnt_cnp_60days"].ewm(alpha=alpha).mean()
10. Radial Basis Function (RBF)
When to use: To emphasize proximity to a certain value.
Formula: 
RBF
(
ùë•
,
ùëê
)
=
ùëí
‚àí
ùõæ
‚ãÖ
(
ùë•
‚àí
ùëê
)
2
RBF(x,c)=e 
‚àíŒ≥‚ãÖ(x‚àíc) 
2
 
 
Implementation:
python
Copy
Edit
gamma = 0.1
center = 10  # Example center value
data["rbf_high_density_swipe"] = (-gamma * (data["high_density_swipe_score"] - center) ** 2).exp()
11. Feature Ranking (Rank Transformation)
When to use: To capture relative importance rather than raw values.
Implementation:
python
Copy
Edit
data["rank_pct_cnp_amt_60days"] = data["pct_cnp_amt_60days"].rank()
12. Clipping Outliers
Cap extreme values to reduce noise.
Implementation:
python
Copy
Edit
data["clipped_high_density_swipe"] = data["high_density_swipe_score"].clip(upper=100)
13. Square and Cube Root
For smoothing extreme high values without drastic reductions.
Implementation:
python
Copy
Edit
data["sqroot_dist_miles"] = data["dist_miles_btw_swp"] ** (1 / 2)
data["cubroot_dist_miles"] = data["dist_miles_btw_swp"] ** (1 / 3)
14. Density Normalization
Normalize cnt_cnp_60days and pct_cnp_amt_60days by their respective averages for region or time:
python
Copy
Edit
data["norm_cnp_60days"] = data["cnt_cnp_60days"] / data["cnt_cnp_60days"].mean()
Which Transformations to Focus On
For High Variance/Skewed Features: Use log, square root, or sigmoid transformations.
For Distance Features: Use reciprocal or radial basis transformations.
For Complex Relationships: Use interaction terms, RBF, or polynomial terms.























# Initialize PCA with 20 components (adjust based on explained variance)
pca_model = H2OPrincipalComponentAnalysisEstimator(k=20, transform="STANDARDIZE", pca_method="GramSVD")
pca_model.train(x=feature_cols, training_frame=train)

# Transform datasets using PCA
train_pca = pca_model.transform(train)
test_pca = pca_model.transform(test)
train_rs_pca = pca_model.transform(train_rs)
oot_pca = pca_model.transform(oot)

# Convert PCA-transformed frames to H2OFrame
train_pca[target_col] = train[target_col]
test_pca[target_col] = test[target_col]
train_rs_pca[target_col] = train_rs[target_col]
oot_pca[target_col] = oot[target_col]



gbm_model = H2OGradientBoostingEstimator(
    ntrees=500, 
    learn_rate=0.05, 
    max_depth=5, 
    seed=42
)

# Train the GBM model
gbm_model.train(x=train_pca.columns[:-1], y=target_col, training_frame=train_pca)

# Predict on test and OOT sets
y_pred_test = gbm_model.predict(test_pca)
y_pred_oot = gbm_model.predict(oot_pca)

# Compute AUC
auc_test = H2OBinomialModelMetrics(y_pred_test, test[target_col]).auc()
auc_oot = H2OBinomialModelMetrics(y_pred_oot, oot[target_col]).auc()

print(f"Test AUC: {auc_test:.4f}")
print(f"OOT AUC: {auc_oot:.4f}")





# Get PCA importance
pca_importance = pca_model.varimp(use_pandas=True)
print(pca_importance)

# Plot cumulative variance
import matplotlib.pyplot as plt

explained_variance = pca_importance['percentage'][:20].cumsum()
plt.plot(range(1, 21), explained_variance, marker='o', linestyle='--')
plt.xlabel("Number of Components")
plt.ylabel("Cumulative Explained Variance")
plt.title("PCA Explained Variance in H2O")
plt.show()








import h2o
import shap
import numpy as np
import pandas as pd
from h2o.estimators import H2OGradientBoostingEstimator
import matplotlib.pyplot as plt

# üöÄ Initialize H2O
h2o.init()

# üìÇ Load Model & Data
model = h2o.load_model("my_fraud_model")
data = h2o.import_file("fraud_features_v5.csv")

# üìä Get Predictions
predictions = model.predict(data)
data["score"] = predictions["predict"]  # Add score column

# üéØ Filter Low-Scoring Fraud Transactions (<500)
low_score_fraud = data[(data["score"] < 500) & (data["fraud_block_ind"] == 1)]

# üèÜ Convert H2OFrame to Pandas for SHAP Analysis
low_score_fraud_pd = low_score_fraud.as_data_frame()
X_low = low_score_fraud_pd.drop(columns=["fraud_block_ind", "score"])  # Exclude target & score

# üß† SHAP Explainer
explainer = shap.Explainer(model.predict, X_low)
shap_values = explainer(X_low)

# üîç SHAP Summary Plot (Which Features Are Lowering Scores?)
shap.summary_plot(shap_values, X_low)

# üîé SHAP Dependence Plot for Swipe-Related Features
shap.dependence_plot("avg_swipe_amt_24hr", shap_values, X_low)

# üîé SHAP Dependence Plot for Another Swipe Feature
shap.dependence_plot("pct_swipe_amt_24hr_by_60days", shap_values, X_low)
