# Initialize PCA with 20 components (adjust based on explained variance)
pca_model = H2OPrincipalComponentAnalysisEstimator(k=20, transform="STANDARDIZE", pca_method="GramSVD")
pca_model.train(x=feature_cols, training_frame=train)

# Transform datasets using PCA
train_pca = pca_model.transform(train)
test_pca = pca_model.transform(test)
train_rs_pca = pca_model.transform(train_rs)
oot_pca = pca_model.transform(oot)

# Convert PCA-transformed frames to H2OFrame
train_pca[target_col] = train[target_col]
test_pca[target_col] = test[target_col]
train_rs_pca[target_col] = train_rs[target_col]
oot_pca[target_col] = oot[target_col]



gbm_model = H2OGradientBoostingEstimator(
    ntrees=500, 
    learn_rate=0.05, 
    max_depth=5, 
    seed=42
)

# Train the GBM model
gbm_model.train(x=train_pca.columns[:-1], y=target_col, training_frame=train_pca)

# Predict on test and OOT sets
y_pred_test = gbm_model.predict(test_pca)
y_pred_oot = gbm_model.predict(oot_pca)

# Compute AUC
auc_test = H2OBinomialModelMetrics(y_pred_test, test[target_col]).auc()
auc_oot = H2OBinomialModelMetrics(y_pred_oot, oot[target_col]).auc()

print(f"Test AUC: {auc_test:.4f}")
print(f"OOT AUC: {auc_oot:.4f}")





# Get PCA importance
pca_importance = pca_model.varimp(use_pandas=True)
print(pca_importance)

# Plot cumulative variance
import matplotlib.pyplot as plt

explained_variance = pca_importance['percentage'][:20].cumsum()
plt.plot(range(1, 21), explained_variance, marker='o', linestyle='--')
plt.xlabel("Number of Components")
plt.ylabel("Cumulative Explained Variance")
plt.title("PCA Explained Variance in H2O")
plt.show()








import h2o
import shap
import numpy as np
import pandas as pd
from h2o.estimators import H2OGradientBoostingEstimator
import matplotlib.pyplot as plt

# 🚀 Initialize H2O
h2o.init()

# 📂 Load Model & Data
model = h2o.load_model("my_fraud_model")
data = h2o.import_file("fraud_features_v5.csv")

# 📊 Get Predictions
predictions = model.predict(data)
data["score"] = predictions["predict"]  # Add score column

# 🎯 Filter Low-Scoring Fraud Transactions (<500)
low_score_fraud = data[(data["score"] < 500) & (data["fraud_block_ind"] == 1)]

# 🏆 Convert H2OFrame to Pandas for SHAP Analysis
low_score_fraud_pd = low_score_fraud.as_data_frame()
X_low = low_score_fraud_pd.drop(columns=["fraud_block_ind", "score"])  # Exclude target & score

# 🧠 SHAP Explainer
explainer = shap.Explainer(model.predict, X_low)
shap_values = explainer(X_low)

# 🔍 SHAP Summary Plot (Which Features Are Lowering Scores?)
shap.summary_plot(shap_values, X_low)

# 🔎 SHAP Dependence Plot for Swipe-Related Features
shap.dependence_plot("avg_swipe_amt_24hr", shap_values, X_low)

# 🔎 SHAP Dependence Plot for Another Swipe Feature
shap.dependence_plot("pct_swipe_amt_24hr_by_60days", shap_values, X_low)
