import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# -----------------------------
# CONFIG
# -----------------------------
DATA_PATH = 'monthwise_updated_attrition_dataset.xlsx'

# choose training and validation years
train_years = [2020, 2021, 2022]  # you can change this
validation_years = [2025]         # you can change this

N_SIMS = 2000
RANDOM_SEED = 42
SHOCK_STD_MULT = 1.5
sns.set(style='whitegrid', context='talk')

# -----------------------------
np.random.seed(RANDOM_SEED)

# -----------------------------
# LOAD DATA
# -----------------------------
df = pd.read_excel(DATA_PATH)

# make sure columns exist
cols_needed = ['BUSINESS_GROUPS_TA','MONTH','HEADCOUNT','VOLUNTARY_EXITS','MONTHLY_ATTRITION_RATE']
assert all(c in df.columns for c in cols_needed), f"Missing columns: {set(cols_needed)-set(df.columns)}"

# ensure attrition as fraction
if df['MONTHLY_ATTRITION_RATE'].max() > 1:
    df['attr_frac'] = df['MONTHLY_ATTRITION_RATE'] / 100.0
else:
    df['attr_frac'] = df['MONTHLY_ATTRITION_RATE']

# convert MONTH to datetime
try:
    df['MONTH'] = pd.to_datetime(df['MONTH'], format='%Y-%m')
except Exception:
    df['MONTH'] = pd.to_datetime(df['MONTH'], errors='coerce')

df['YEAR'] = df['MONTH'].dt.year

# -----------------------------
# HELPER FUNCTIONS
# -----------------------------
def fit_beta_moments(p_series, eps=1e-6):
    m = p_series.mean()
    v = p_series.var(ddof=0)
    maxv = m*(1-m) - eps
    if v >= maxv or v <= 0:
        phi = max(10.0, (m*(1-m) / (v+eps)) if v>0 else 50.0)
        alpha = m * phi
        beta = (1-m) * phi
    else:
        common = (m*(1-m)/v) - 1.0
        alpha = max(eps, m * common)
        beta = max(eps, (1-m) * common)
    return float(alpha), float(beta)

def detect_shock_params(p_series, k=SHOCK_STD_MULT):
    mu = p_series.mean()
    sigma = p_series.std(ddof=0)
    threshold = mu + k * sigma
    shock_mask = p_series > threshold
    q = shock_mask.sum() / len(p_series)
    if shock_mask.sum() > 0:
        mag = p_series[shock_mask].mean() / max(mu, 1e-9)
    else:
        mag = 1.0
    return {'base_mean': mu, 'base_std': sigma, 'threshold': threshold,
            'shock_prob': float(q), 'shock_mag': float(mag)}

def simulate_future(alpha, beta, shock_prob, shock_mag,
                    hc_arr, n_sims=2000, shock_memory_decay=0.6):
    n_months = len(hc_arr)
    exits_sims = np.zeros((n_sims, n_months), dtype=int)
    rate_sims = np.zeros((n_sims, n_months), dtype=float)
    for sim in range(n_sims):
        q_curr = shock_prob
        for t in range(n_months):
            p = np.random.beta(alpha, beta)
            is_shock = np.random.rand() < q_curr
            if is_shock:
                p = min(0.9999, p * shock_mag)
                q_curr = shock_prob + (1 - shock_prob) * shock_memory_decay
            else:
                q_curr = shock_prob + (q_curr - shock_prob) * shock_memory_decay
            exits = np.random.binomial(hc_arr[t], p)
            exits_sims[sim, t] = exits
            rate_sims[sim, t] = p
    return exits_sims, rate_sims

# -----------------------------
# MAIN: train/validate per group
# -----------------------------
group_names = df['BUSINESS_GROUPS_TA'].unique()

for group_name in group_names:
    g = df[df['BUSINESS_GROUPS_TA']==group_name].copy().sort_values('MONTH')

    # split train/validation by years
    train = g[g['YEAR'].isin(train_years)]
    val = g[g['YEAR'].isin(validation_years)]

    if len(train)==0 or len(val)==0:
        print(f"Skipping {group_name} (no train or no val data)")
        continue

    # fit on train only
    alpha, beta = fit_beta_moments(train['attr_frac'])
    shock = detect_shock_params(train['attr_frac'])

    shock_prob = shock['shock_prob']
    shock_mag = max(shock['shock_mag'],1.0)

    hc_future = val['HEADCOUNT'].values

    exits_sims, rate_sims = simulate_future(alpha, beta, shock_prob, shock_mag,
                                            hc_future, n_sims=N_SIMS, shock_memory_decay=0.6)

    median_pred = np.median(exits_sims, axis=0)
    p10_pred = np.percentile(exits_sims, 10, axis=0)
    p90_pred = np.percentile(exits_sims, 90, axis=0)

    # assemble plot df
    plot_df = pd.DataFrame({
        'MONTH': val['MONTH'],
        'ACTUAL_EXITS': val['VOLUNTARY_EXITS'].values,
        'PRED_MEDIAN': median_pred,
        'PRED_P10': p10_pred,
        'PRED_P90': p90_pred
    })

    # plot actual vs predicted
    plt.figure(figsize=(12,6))
    plt.fill_between(plot_df['MONTH'], plot_df['PRED_P10'], plot_df['PRED_P90'],
                     color='skyblue', alpha=0.3, label='Prediction Interval (10–90%)')
    plt.plot(plot_df['MONTH'], plot_df['PRED_MEDIAN'],
             color='blue', linewidth=2, marker='o', label='Predicted Median')
    plt.plot(plot_df['MONTH'], plot_df['ACTUAL_EXITS'],
             color='red', linewidth=2, marker='s', label='Actual VOLUNTARY_EXITS')

    plt.title(f"Actual vs Predicted Voluntary Exits — {group_name}")
    plt.xlabel("Month")
    plt.ylabel("Voluntary Exits")
    plt.legend()
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

    # also optional: print basic MAE
    mae = np.mean(np.abs(plot_df['ACTUAL_EXITS'] - plot_df['PRED_MEDIAN']))
    print(f"{group_name} MAE on validation period: {mae:.2f}")


























import matplotlib.pyplot as plt

# Loop through each group and plot Actual vs Base
for g in val_preds['BUSINESS_GROUPS_TA'].unique():
    sub = val_preds[val_preds['BUSINESS_GROUPS_TA'] == g]
    if sub.empty:
        continue
    
    plt.figure(figsize=(8,4))
    # Plot base prediction
    plt.plot(sub['MONTH'], sub['pred_mean_base'], 'b*-', label='Base Prediction')  # blue stars
    # Plot actual
    plt.plot(sub['MONTH'], sub['VOLUNTARY_EXITS'], 'ro-', label='Actual')          # red circles
    
    plt.title(f'Actual vs Base Prediction: {g}')
    plt.xlabel('Month')
    plt.ylabel('Exits')
    plt.xticks(rotation=45)
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.tight_layout()
    plt.show()


















val_preds = val_pred_base_df.copy()
adj_preds = []

for _, row in val_preds.iterrows():
    g = row['BUSINESS_GROUPS_TA']
    train_g = train_df[train_df['BUSINESS_GROUPS_TA'] == g].sort_values('MONTH')
    train_g = compute_hires_rate(train_g)

    # Get hires_rate_pred
    if train_g.shape[0] >= 3:
        hires_rate_pred = train_g['hires_rate'].dropna().iloc[-3:].mean()
    elif train_g.shape[0] >= 1:
        hires_rate_pred = train_g['hires_rate'].dropna().iloc[-1]
    else:
        hires_rate_pred = 0.0

    gamma = gamma_map.get(g, 0.0)

    headcount = row['HEADCOUNT']
    if headcount > 0:
        base_rate = row['pred_mean_base'] / headcount
    else:
        base_rate = 0.0

    adj_rate = base_rate + gamma * hires_rate_pred
    pred_mean_adj = max(0, headcount * adj_rate)
    adj_preds.append(pred_mean_adj)

val_preds['pred_mean_hires_adj'] = adj_preds










import matplotlib.pyplot as plt

plt.figure(figsize=(10,6))
plt.plot(val_preds['MONTH'], val_preds['ACTUAL_EXITS'], label='Actual Exits', marker='o')
plt.plot(val_preds['MONTH'], val_preds['pred_mean_hires_adj'], label='Predicted (Adj)', marker='x')
plt.title('Attrition Forecast: Actual vs Predicted')
plt.xlabel('Month')
plt.ylabel('Exits')
plt.legend()
plt.grid(True)
plt.show()













# --------------------------------------------------
# Robust fast-reacting attrition pipeline (no ML/time-series)
# --------------------------------------------------
import pandas as pd
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns

# -------------------------
# 1. Load & basic prep
# -------------------------
df = pd.read_excel('monthwise_updated_attrition_dataset.xlsx')
df['MONTH'] = pd.to_datetime(df['MONTH'])
df = df.sort_values(['BUSINESS_GROUPS_TA','MONTH']).reset_index(drop=True)

# train: 2021,2022,2023 ; val: 2025
train_mask = df['MONTH'].dt.year.isin([2021,2022,2023])
val_mask   = df['MONTH'].dt.year == 2025
train_df = df[train_mask].copy().reset_index(drop=True)
val_df   = df[val_mask].copy().reset_index(drop=True)

groups = df['BUSINESS_GROUPS_TA'].unique()

# -------------------------
# 2. Prior from train (method-of-moments)
# -------------------------
train_lambda = train_df['VOLUNTARY_EXITS'] / train_df['HEADCOUNT']
m = train_lambda.mean()
v = train_lambda.var(ddof=1)
if v <= 0 or np.isnan(v):
    alpha0, beta0 = 1.0, 1.0 / max(m,1e-6)
else:
    alpha0 = (m**2)/v
    beta0  = m / v

# -------------------------
# helpers
# -------------------------
def months_between(later, earlier):
    return (later.year - earlier.year)*12 + (later.month - earlier.month)

def weighted_posterior_for_group(train_g, pred_month, delta, alpha0, beta0):
    # train_g: dataframe of past months for this group
    if train_g.shape[0] == 0:
        return alpha0, beta0  # fallback
    diffs = np.array([months_between(pred_month, t) for t in train_g['MONTH']])
    # only use months where diffs>0 (past)
    diffs = np.maximum(diffs, 0)
    weights = (delta ** diffs).astype(float)
    alpha_post = alpha0 + np.sum(weights * train_g['VOLUNTARY_EXITS'].values)
    beta_post  = beta0  + np.sum(weights * train_g['HEADCOUNT'].values)
    return alpha_post, beta_post

def predict_with_delta_delta_grid(train_df, val_df, groups, delta_grid, alpha0, beta0):
    # finds best delta per group by validation MAE (if val months present), else picks robust default
    best_delta = {}
    val_preds = []
    for g in groups:
        train_g = train_df[train_df['BUSINESS_GROUPS_TA']==g]
        val_g = val_df[val_df['BUSINESS_GROUPS_TA']==g]
        if val_g.shape[0] == 0:
            best_delta[g] = 0.9  # default if no val
            continue
        best_mae = np.inf
        best_d = None
        for d in delta_grid:
            preds = []
            for _, row in val_g.iterrows():
                a_post, b_post = weighted_posterior_for_group(train_g, row['MONTH'], d, alpha0, beta0)
                pred_mean = row['HEADCOUNT'] * (a_post / b_post)
                preds.append(pred_mean)
            mae = np.mean(np.abs(np.array(preds) - val_g['VOLUNTARY_EXITS'].values))
            if mae < best_mae:
                best_mae = mae; best_d = d
        best_delta[g] = best_d
    # final predictions using best deltas
    all_val_preds = []
    for _, row in val_df.iterrows():
        g = row['BUSINESS_GROUPS_TA']
        a_post, b_post = weighted_posterior_for_group(train_df[train_df['BUSINESS_GROUPS_TA']==g],
                                                      row['MONTH'], best_delta[g], alpha0, beta0)
        r = a_post
        p = b_post / (b_post + row['HEADCOUNT'])
        # mean predictive using NB mean = HEADCOUNT * (a_post/b_post)
        pred_mean = row['HEADCOUNT'] * (a_post / b_post)
        # CI (2.5,97.5)
        lower = stats.nbinom.ppf(0.025, r, p)
        upper = stats.nbinom.ppf(0.975, r, p)
        all_val_preds.append({
            'BUSINESS_GROUPS_TA': g, 'MONTH': row['MONTH'],
            'pred_mean_base': pred_mean, 'pred_lower_base': lower, 'pred_upper_base': upper,
            'VOLUNTARY_EXITS': row['VOLUNTARY_EXITS'], 'HEADCOUNT': row['HEADCOUNT']
        })
    return pd.DataFrame(all_val_preds), best_delta

# -------------------------
# 3. Grid-search delta per group (validation-driven)
# -------------------------
delta_grid = np.concatenate([np.linspace(0.3,0.95,14), np.array([0.98])])  # include near-1
val_pred_base_df, best_delta_map = predict_with_delta_delta_grid(train_df, val_df, groups, delta_grid, alpha0, beta0)

# -------------------------
# 4. Mechanistic hires correction: estimate gamma per group (residual_rate ≈ gamma * hires_rate)
# -------------------------
def compute_hires_rate(df_group):
    # compute hires_t = head_t - head_{t-1} + exits_t ; normalize by head_{t-1}
    df_group = df_group.sort_values('MONTH').reset_index(drop=True)
    hires = []
    hires_rate = []
    for i in range(len(df_group)):
        if i==0:
            hires.append(np.nan)
            hires_rate.append(np.nan)
        else:
            h = df_group.loc[i,'HEADCOUNT'] - df_group.loc[i-1,'HEADCOUNT'] + df_group.loc[i,'VOLUNTARY_EXITS']
            hires.append(h)
            denom = max(df_group.loc[i-1,'HEADCOUNT'],1)
            hires_rate.append(h/denom)
    df_group['hires'] = hires
    df_group['hires_rate'] = hires_rate
    return df_group

# prepare base in-sample predictions on train for gamma estimation
# for train, predict each train row using weighted posterior but with prediction month = that train row (using earlier months only)
train_preds = []
for g in groups:
    train_g = train_df[train_df['BUSINESS_GROUPS_TA']==g].copy()
    train_g = compute_hires_rate(train_g)
    if train_g.shape[0] == 0:
        continue
    preds = []
    for idx,row in train_g.iterrows():
        # only use earlier months for prediction
        prev_train = train_g[train_g['MONTH'] < row['MONTH']]
        if prev_train.shape[0] == 0:
            a_post, b_post = alpha0, beta0
        else:
            # use group's best_delta if available else 0.9
            d = best_delta_map.get(g, 0.9)
            a_post, b_post = weighted_posterior_for_group(prev_train, row['MONTH'], d, alpha0, beta0)
        pred_mean = row['HEADCOUNT'] * (a_post / b_post)
        preds.append(pred_mean)
    train_g = train_g.assign(pred_base = preds)
    train_preds.append(train_g)
train_preds_df = pd.concat(train_preds, ignore_index=True) if len(train_preds) else pd.DataFrame()

# compute residual_rate and fit gamma per group via OLS closed form
gamma_map = {}
for g in groups:
    sub = train_preds_df[train_preds_df['BUSINESS_GROUPS_TA']==g].dropna(subset=['hires_rate','pred_base'])
    if sub.shape[0] < 6:
        gamma_map[g] = 0.0
        continue
    # residual rate
    sub['resid_rate'] = (sub['VOLUNTARY_EXITS'] - sub['pred_base']) / sub['HEADCOUNT']
    X = sub['hires_rate'].values
    Y = sub['resid_rate'].values
    # regularized OLS gamma = sum(XY)/(sum(X^2)+lambda)
    lam = 1e-6
    gamma = (X.dot(Y)) / (X.dot(X) + lam)
    gamma_map[g] = gamma

# -------------------------
# 5. Build hires-adjusted predictions for validation
# -------------------------
val_preds = val_pred_base_df.copy()
adj_preds = []
for _, row in val_preds.iterrows():
    g = row['BUSINESS_GROUPS_TA']
    # estimate hires_rate for this group's last observed interval using train latest months
    train_g = train_df[train_df['BUSINESS_GROUPS_TA']==g].sort_values('MONTH')
    train_g = compute_hires_rate(train_g)
    if train_g.shape[0] >= 3:
        # use 3-month average hires_rate (last available)
        hires_rate_pred = train_g['hires_rate'].dropna().iloc[-3:].mean()
    elif train_g.shape[0] >= 1:
        hires_rate_pred = train_g['hires_rate'].dropna().iloc[-1] if not train_g['hires_rate'].dropna().empty else 0.0
    else:
        hires_rate_pred = 0.0
    gamma = gamma_map.get(g, 0.0)
    base_rate = (row['pred_mean_base'] / row['HEADCOUNT'])
    adj_rate = base_rate + gamma * hires_rate_pred
    pred_mean_adj = max(0.0, row['HEADCOUNT'] * adj_rate)
    adj_preds.append(pred_mean_adj)
val_preds['pred_mean_hires_adj'] = adj_preds

# -------------------------
# 6. Ensemble (inverse-MAE weighting using train performance)
# -------------------------
# compute train MAE of base and hires-adjusted
# base train predictions already in train_preds_df
# compute train preds_hires_adj similarly to above (use gamma_map)
train_preds_df['pred_hires_adj'] = train_preds_df.apply(
    lambda r: max(0.0, r['pred_base'] + (gamma_map.get(r['BUSINESS_GROUPS_TA'],0.0) * (r['hires_rate']*r['HEADCOUNT'] if not np.isnan(r['hires_rate']) else 0.0))),
    axis=1
)

mae_base_train = train_preds_df.dropna().assign(abs_err = lambda d: np.abs(d['VOLUNTARY_EXITS'] - d['pred_base']))['abs_err'].mean()
mae_hires_train = train_preds_df.dropna().assign(abs_err = lambda d: np.abs(d['VOLUNTARY_EXITS'] - d['pred_hires_adj']))['abs_err'].mean()
# weights - inverse MAE
w_base = (1/mae_base_train) / ((1/mae_base_train) + (1/mae_hires_train)) if mae_base_train>0 and mae_hires_train>0 else 0.5
w_hires = 1 - w_base

# final ensemble preds on validation
val_preds['pred_ensemble'] = w_base * val_preds['pred_mean_base'] + w_hires * val_preds['pred_mean_hires_adj']

# -------------------------
# 7. Metrics & plots
# -------------------------
val_preds['abs_err_base'] = np.abs(val_preds['VOLUNTARY_EXITS'] - val_preds['pred_mean_base'])
val_preds['abs_err_adj']  = np.abs(val_preds['VOLUNTARY_EXITS'] - val_preds['pred_mean_hires_adj'])
val_preds['abs_err_ens']  = np.abs(val_preds['VOLUNTARY_EXITS'] - val_preds['pred_ensemble'])

def summary_metrics(dfp, col_prefix):
    return {
        f'{col_prefix}_MAE': dfp[f'abs_err_{col_prefix}'].mean(),
        f'{col_prefix}_MAE_pct_of_head': (dfp[f'abs_err_{col_prefix}']/dfp['HEADCOUNT']).mean()
    }

print("Validation MAE base, adj, ensemble (absolute):", val_preds['abs_err_base'].mean(), val_preds['abs_err_adj'].mean(), val_preds['abs_err_ens'].mean())

# Plot Actual vs Predicted for validation grouped
for g in groups:
    sub = val_preds[val_preds['BUSINESS_GROUPS_TA']==g]
    if sub.empty: continue
    plt.figure(figsize=(8,4))
    plt.errorbar(sub['MONTH'], sub['pred_mean_base'], yerr=[sub['pred_mean_base']-sub['pred_lower_base'], sub['pred_upper_base']-sub['pred_mean_base']], fmt='x-', label='Base ±95%CI')
    plt.plot(sub['MONTH'], sub['pred_mean_hires_adj'], 's--', label='Hires adj')
    plt.plot(sub['MONTH'], sub['pred_ensemble'], 'd--', label=f'Ensemble w={w_base:.2f}')
    plt.plot(sub['MONTH'], sub['VOLUNTARY_EXITS'], 'o-', label='Actual')
    plt.title(f'Validation: {g}')
    plt.legend()
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

# Residuals & EWMA (for one group example)
example_group = groups[0]
res_df = train_preds_df[train_preds_df['BUSINESS_GROUPS_TA']==example_group].copy()
res_df['resid'] = res_df['VOLUNTARY_EXITS'] - res_df['pred_base']
res_df['std_resid'] = res_df['resid'] / np.sqrt(res_df['pred_base'] + 1e-6)
# EWMA
lambda_ewma = 0.3
s = []
s_t = 0.0
for v in res_df['std_resid'].fillna(0).values:
    s_t = lambda_ewma * v + (1-lambda_ewma) * s_t
    s.append(s_t)
res_df['ewma_std_resid'] = s
plt.figure(figsize=(8,3))
plt.plot(res_df['MONTH'], res_df['ewma_std_resid'], '-o', label='EWMA std residual')
plt.axhline(2.0, color='red', linestyle='--')
plt.axhline(-2.0, color='red', linestyle='--')
plt.title(f'EWMA standardized residuals (example group: {example_group})')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()






















# ---------------------------------------------------------
# Install if needed
# pip install pymc arviz numpyro jax jaxlib matplotlib
# ---------------------------------------------------------

import pandas as pd
import numpy as np
import pymc as pm
import arviz as az
import matplotlib.pyplot as plt

# ---------------------------------------------------------
# 1. Load and split data
# ---------------------------------------------------------
df = pd.read_excel('monthwise_updated_attrition_dataset.xlsx')

# ensure MONTH is datetime
df['MONTH'] = pd.to_datetime(df['MONTH'])

# define train and validation masks
train_mask = df['MONTH'].dt.year.isin([2021, 2022, 2023])
val_mask   = df['MONTH'].dt.year.isin([2025])

train_df = df[train_mask].copy().sort_values(['BUSINESS_GROUPS_TA','MONTH'])
val_df   = df[val_mask].copy().sort_values(['BUSINESS_GROUPS_TA','MONTH'])

# categories for groups (must match across train and val)
groups = df['BUSINESS_GROUPS_TA'].unique()
n_groups = len(groups)
group_map = {g:i for i,g in enumerate(groups)}

train_idx = train_df['BUSINESS_GROUPS_TA'].map(group_map).values
val_idx   = val_df['BUSINESS_GROUPS_TA'].map(group_map).values

# data arrays
exits_train = train_df['VOLUNTARY_EXITS'].values
head_train  = train_df['HEADCOUNT'].values

head_val    = val_df['HEADCOUNT'].values
exits_val   = val_df['VOLUNTARY_EXITS'].values  # for later comparison

# ---------------------------------------------------------
# 2. Build model on train data only
# ---------------------------------------------------------
with pm.Model() as hier_model:
    # Hyperpriors
    alpha_hyper = pm.HalfNormal('alpha_hyper', sigma=2.0)
    beta_hyper  = pm.HalfNormal('beta_hyper', sigma=2.0)

    # Group-level attrition rates
    theta_group = pm.Gamma('theta_group', alpha=alpha_hyper, beta=beta_hyper, shape=n_groups)

    # Train likelihood
    mu_train = head_train * theta_group[train_idx]
    y_obs = pm.Poisson('y_obs', mu=mu_train, observed=exits_train)

    # Fast sampling
    trace = pm.sampling_jax.sample_numpyro_nuts(
        draws=1000,
        tune=1000,
        target_accept=0.9,
        random_seed=42
    )

# ---------------------------------------------------------
# 3. Predictions for train and validation
# ---------------------------------------------------------
theta_draws = trace.posterior['theta_group'].stack(sample=("chain","draw")).values  # shape (n_groups, draws)

# predict function
def predict_exits(headcounts, indices):
    pred_mean, pred_lower, pred_upper = [], [], []
    for i,gidx in enumerate(indices):
        lam_draws = headcounts[i] * theta_draws[gidx,:]
        pred_mean.append(lam_draws.mean())
        pred_lower.append(np.percentile(lam_draws, 2.5))
        pred_upper.append(np.percentile(lam_draws, 97.5))
    return pd.DataFrame({
        'pred_mean':pred_mean,
        'pred_lower':pred_lower,
        'pred_upper':pred_upper
    })

pred_train = predict_exits(head_train, train_idx)
pred_val   = predict_exits(head_val, val_idx)

train_results = pd.concat([train_df.reset_index(drop=True), pred_train], axis=1)
val_results   = pd.concat([val_df.reset_index(drop=True), pred_val], axis=1)

# ---------------------------------------------------------
# 4. Visualisations
# ---------------------------------------------------------
# (A) Actual vs Predicted (Train)
plt.figure(figsize=(12,6))
for g in groups:
    dfg = train_results[train_results['BUSINESS_GROUPS_TA']==g]
    plt.plot(dfg['MONTH'], dfg['VOLUNTARY_EXITS'], 'o-', label=f'{g} Actual')
    plt.plot(dfg['MONTH'], dfg['pred_mean'], 'x--', label=f'{g} Predicted')
plt.xlabel('Month')
plt.ylabel('Exits')
plt.title('TRAIN DATA: Actual vs Predicted Exits by Group')
plt.legend(bbox_to_anchor=(1.05,1))
plt.tight_layout()
plt.show()

# (B) Actual vs Predicted (Validation)
plt.figure(figsize=(12,6))
for g in groups:
    dfg = val_results[val_results['BUSINESS_GROUPS_TA']==g]
    if len(dfg)==0:  # skip groups with no val data
        continue
    plt.errorbar(dfg['MONTH'], dfg['pred_mean'],
                 yerr=[dfg['pred_mean']-dfg['pred_lower'], dfg['pred_upper']-dfg['pred_mean']],
                 fmt='x--', capsize=3, label=f'{g} Predicted ±95% CI')
    plt.plot(dfg['MONTH'], dfg['VOLUNTARY_EXITS'], 'o-', label=f'{g} Actual')
plt.xlabel('Month')
plt.ylabel('Exits')
plt.title('VALIDATION DATA: Actual vs Predicted Exits by Group (with 95% CI)')
plt.legend(bbox_to_anchor=(1.05,1))
plt.tight_layout()
plt.show()

# (C) Scatter Plot Predicted vs Actual (Validation)
plt.figure(figsize=(6,6))
plt.scatter(val_results['VOLUNTARY_EXITS'], val_results['pred_mean'])
plt.plot([val_results['VOLUNTARY_EXITS'].min(), val_results['VOLUNTARY_EXITS'].max()],
         [val_results['VOLUNTARY_EXITS'].min(), val_results['VOLUNTARY_EXITS'].max()],
         'r--', label='Perfect Prediction')
plt.xlabel('Actual Exits (Validation)')
plt.ylabel('Predicted Exits (Validation)')
plt.title('Predicted vs Actual (Validation Data)')
plt.legend()
plt.tight_layout()
plt.show()

















# install if needed
# pip install pymc arviz matplotlib

import pandas as pd
import numpy as np
import pymc as pm
import arviz as az
import matplotlib.pyplot as plt

# -----------------------------------------
# 1. Load your data
# -----------------------------------------
df = pd.read_excel('monthwise_updated_attrition_dataset.xlsx')

# Sort by group and month
df = df.sort_values(['BUSINESS_GROUPS_TA','MONTH']).reset_index(drop=True)

# Basic features
groups = df['BUSINESS_GROUPS_TA'].unique()
n_groups = len(groups)
group_idx = df['BUSINESS_GROUPS_TA'].astype('category').cat.codes.values

# Observed counts and exposure
exits = df['VOLUNTARY_EXITS'].values
headcount = df['HEADCOUNT'].values

# -----------------------------------------
# 2. Build hierarchical Bayesian model
# -----------------------------------------
with pm.Model() as hier_model:
    # Hyperpriors for group-level attrition rate (Gamma hyperprior)
    alpha_hyper = pm.HalfNormal('alpha_hyper', sigma=2.0)
    beta_hyper  = pm.HalfNormal('beta_hyper', sigma=2.0)
    
    # Group-level attrition rates (latent)
    theta_group = pm.Gamma('theta_group', alpha=alpha_hyper, beta=beta_hyper, shape=n_groups)
    
    # Expected exits per row = headcount * theta of that group
    mu = headcount * theta_group[group_idx]
    
    # Likelihood: Poisson observed exits
    y_obs = pm.Poisson('y_obs', mu=mu, observed=exits)
    
    # Predictive posterior
    trace = pm.sample(2000, tune=2000, target_accept=0.95, random_seed=42)

# -----------------------------------------
# 3. Summarise posterior
# -----------------------------------------
az.summary(trace, var_names=['alpha_hyper','beta_hyper','theta_group'])

# Posterior mean attrition rates per group:
theta_means = trace.posterior['theta_group'].mean(dim=("chain","draw")).values

group_results = pd.DataFrame({
    'BUSINESS_GROUPS_TA': groups,
    'posterior_attrition_rate': theta_means
})

print(group_results)

# -----------------------------------------
# 4. Make predictions for each row
# -----------------------------------------
# For each draw, predicted exits per row
theta_draws = trace.posterior['theta_group'].stack(sample=("chain","draw")).values  # shape (n_groups, draws)
predicted_exits = []
for i, gidx in enumerate(group_idx):
    lam_draws = headcount[i] * theta_draws[gidx,:]
    pred_mean = lam_draws.mean()
    pred_lower = np.percentile(lam_draws, 2.5)
    pred_upper = np.percentile(lam_draws, 97.5)
    predicted_exits.append((pred_mean, pred_lower, pred_upper))

predicted_exits = pd.DataFrame(predicted_exits, columns=['pred_mean','pred_lower','pred_upper'])
df_pred = pd.concat([df, predicted_exits], axis=1)

# -----------------------------------------
# 5. Visualise predicted vs actual
# -----------------------------------------
plt.figure(figsize=(10,6))
for g in groups:
    dfg = df_pred[df_pred['BUSINESS_GROUPS_TA']==g]
    plt.plot(dfg['MONTH'], dfg['VOLUNTARY_EXITS'], 'o-', label=f'{g} Actual')
    plt.plot(dfg['MONTH'], dfg['pred_mean'], 'x--', label=f'{g} Predicted')
plt.xlabel('Month')
plt.ylabel('Exits')
plt.title('Actual vs Predicted Exits by Group')
plt.legend(bbox_to_anchor=(1.05,1))
plt.tight_layout()
plt.show()



















# Requires: pandas, numpy, scipy
import pandas as pd
import numpy as np
from scipy import stats

# 1. load (adjust path as needed)
df = pd.read_excel('monthwise_updated_attrition_dataset.xlsx')  # your file
# ensure MONTH sorts chronologically
df = df.sort_values(['BUSINESS_GROUPS_TA', 'MONTH']).reset_index(drop=True)

# 2. quick per-row per-person exit rate estimate (no pooling)
df['lambda_hat'] = df['VOLUNTARY_EXITS'] / df['HEADCOUNT']  # observed per-person rate

# 3. Empirical Bayes (method-of-moments) to get global Gamma prior for theta
m = df['lambda_hat'].mean()
v = df['lambda_hat'].var(ddof=1)
# safety if v is 0 or extremely small:
if v <= 0 or np.isnan(v):
    # fallback weak prior around mean
    alpha0, beta0 = 1.0, 1.0 / max(m, 1e-6)
else:
    alpha0 = (m**2) / v
    beta0  = m / v
# Note: in the Gamma(rate) parameterization, mean = alpha/beta

# 4. create cumulative-up-to-previous-month statistics per group (so forecasting uses only past)
g = df.groupby('BUSINESS_GROUPS_TA')
df['cum_exits_prev'] = g['VOLUNTARY_EXITS'].cumsum() - df['VOLUNTARY_EXITS']
df['cum_head_prev']  = g['HEADCOUNT'].cumsum() - df['HEADCOUNT']

# For the first row of a group, cum_*_prev will be 0 -> posterior is just prior

# 5. posterior parameters using past data (alpha_post = alpha0 + sum_exits_past ; beta_post = beta0 + sum_headcount_past)
df['alpha_post_prev'] = alpha0 + df['cum_exits_prev']
df['beta_post_prev']  = beta0  + df['cum_head_prev']

# 6. choose exposure (HEADCOUNT) for next-month prediction. If you have a next-month HEADCOUNT use it;
#    otherwise use current HEADCOUNT as a simple estimator for next month exposure.
df['HEADCOUNT_next_est'] = df['HEADCOUNT']  # simple, can be improved by modeling hires separately

# 7. predictive mean for next-month exits:
#    E[exits_next] = HEADCOUNT_next * (alpha_post_prev / beta_post_prev)
df['pred_exits_mean'] = df['HEADCOUNT_next_est'] * (df['alpha_post_prev'] / df['beta_post_prev'])

# 8. predictive 95% credible interval using Negative-Binomial parameterization:
#    NB parameters: r = alpha_post_prev, p = beta_post_prev / (beta_post_prev + HEADCOUNT_next_est)
r = df['alpha_post_prev'].values
p = df['beta_post_prev'].values / (df['beta_post_prev'].values + df['HEADCOUNT_next_est'].values)

# use scipy nbinom: nbinom.ppf returns number of failures (exits) quantile
lower_q = stats.nbinom.ppf(0.025, r, p)
upper_q = stats.nbinom.ppf(0.975, r, p)

df['pred_exits_ci_lower'] = lower_q
df['pred_exits_ci_upper'] = upper_q

# 9. predicted attrition rate (mean and CI)
df['pred_monthly_attrition_rate_mean'] = df['pred_exits_mean'] / df['HEADCOUNT_next_est']
df['pred_attr_rate_ci_lower'] = df['pred_exits_ci_lower'] / df['HEADCOUNT_next_est']
df['pred_attr_rate_ci_upper'] = df['pred_exits_ci_upper'] / df['HEADCOUNT_next_est']

# 10. view results for latest month in each BUSINESS_GROUPS_TA (example)
latest_preds = df.groupby('BUSINESS_GROUPS_TA').tail(1)[[
    'BUSINESS_GROUPS_TA','MONTH','HEADCOUNT','VOLUNTARY_EXITS','MONTHLY_ATTRITION_RATE',
    'pred_exits_mean','pred_exits_ci_lower','pred_exits_ci_upper',
    'pred_monthly_attrition_rate_mean','pred_attr_rate_ci_lower','pred_attr_rate_ci_upper'
]]
print(latest_preds.to_string(index=False))






import matplotlib.pyplot as plt
import seaborn as sns

# Example: pick one BUSINESS_GROUPS_TA to plot (change as needed)
group_to_plot = df['BUSINESS_GROUPS_TA'].unique()[0]

plot_df = df[df['BUSINESS_GROUPS_TA'] == group_to_plot].copy()

plt.figure(figsize=(14,6))

# Actual voluntary exits
sns.lineplot(
    data=plot_df,
    x='MONTH', y='VOLUNTARY_EXITS',
    label='Actual Exits', marker='o'
)

# Predicted mean
sns.lineplot(
    data=plot_df,
    x='MONTH', y='pred_exits_mean',
    label='Predicted Exits (mean)', marker='o'
)

# Fill between lower/upper CI
plt.fill_between(
    plot_df['MONTH'],
    plot_df['pred_exits_ci_lower'],
    plot_df['pred_exits_ci_upper'],
    color='gray', alpha=0.2, label='95% Predicted Interval'
)

plt.title(f'Predicted vs Actual Voluntary Exits – {group_to_plot}')
plt.xticks(rotation=45)
plt.ylabel('Number of Exits')
plt.legend()
plt.tight_layout()
plt.show()
