Here's a technical 2-liner description for each fraud detection feature based on the formulas:

1. high_dollar_velocity_3hr:
Ratio of high-value (‚â•300) txn count in 3hr to total txn count in 3hr. Captures rapid bursts of high-value activity.


2. high_dollar_ratio_24hr:
Ratio of high-value txn amount to total txn amount in 24hr. Highlights dominance of large-value txns in a short time.


3. exp_date_mismatch_ratio_7days:
Count of expiration date mismatches over total txn count in 7 days. Detects frequent CVV/expiry mismatches often tied to synthetic fraud.


4. zero_auth_mismatch_ratio_24hr:
Ratio of zero-auth mismatch count to zero-auth txn count in 24hr. Flags abnormal patterns in pre-auth probes.


5. foreign_high_dollar_ratio:
Ratio of high-value foreign txn amount to total txn amount in 24hr. Indicates risky cross-border high-value txns.


6. weekend_high_dollar_ratio:
Ratio of weekend high-value txn amount to total weekend txn amount in 24hr. Targets off-hour fraud attempts when oversight is low.


7. nighttime_high_dollar_velocity:
High-value txn velocity during 10pm‚Äì6am, normalized by txn count. Tracks abnormal night-time high-value behavior.


8. high_dollar_swipe_ratio:
Proportion of swipe high-value txn amount to total amount. Flags high-value in-person swipes that deviate from profile.


9. ratio_avg_swipe_amt_24hr_by_60days:
Average swipe txn amount in 24hr normalized by 60-day average. Highlights short-term spending anomalies.


10. transaction_acceleration_24hr:
Ratio of 24hr txn amount to 48hr trailing window. Quantifies sharp spending increases.


11. avmismatch_ratio_30days:
Count of AV (address verification) mismatches to total txns over 30 days. Flags consistent identity verification failures.


12. zero_auth_high_dollar_ratio:
Ratio of high-value zero-auth txn amount to total zero-auth txn amount. Targets abnormal testing behaviors with large amounts.


13. high_val_tx_velocity:
Ratio of high-value txns to txn count at the same merchant in 24hr. Detects merchant-specific fraud bursts.


14. fraud_trx_drift:
Ratio of fraudulent txn amount to average txn amount at merchant over 60 days. Captures fraud drift toward certain merchants.


15. cnt_same_mid_termid_24hr:
Count of txns at same merchant-terminal combo in 24hr. Detects excessive usage of specific terminal for fraud.


16. cnt_for_swipe_24hr:
Count of swiped txns in 24hr window. Useful for card-present fraud detection.


17. velocity_merchant_risk:
Velocity feature capturing merchant-level txn acceleration. Quantifies merchant risk based on usage pattern deviation.


18. cnt_same_merc_id_24hr:
Count of txns at same merchant ID in 24hr. Flags repeated usage of a merchant which may be fraud hotspot.


19. cnt_same_merc_id_30min:
Same as above but in 30min interval. Captures short-burst fraud attempts.


20. hot_mer_mcc_bayesian_encoded_1/11/21/24hr:
Bayesian encoding of merchant MCC risk based on target leakage in various time windows. Encodes historical MCC fraud likelihood in rolling fashion.



Let me know if you want this exported to a table or file format for presentation.









Here is the feature-wise technical description with exact formula context from your image:


---

1. high_dollar_fraud_mer_ratio
Formula: (amt_same_mid_termid_30min) / (amt_same_mid_termid_24hr + 1)
Description: Ratio of high-value txn amount within 30min to total 24hr amount at the same MID-Terminal. Detects rapid high-value fraud bursts.


2. fraud_zero_auth_ratio
Formula: (cnt_za_ris_24hr * cnt_za_mis_7days) / (tot_zero_auth_cnt_24hr + 1)
Description: Product of recent risky zero-auth txn count and 7-day mismatch count, normalized by total 24hr zero-auth volume. Captures anomaly in zero-auth behavior.


3. high_risk_mer8_name_shift
Formula: new_merch_ind * hct_mer_mcc_bayesian_encoded_8
Description: Binary merchant shift indicator weighted by MCC Bayesian risk. Highlights movement toward historically risky MCCs.


4. swipe_anomaly_score
Formula: abs(amt_swipe_tdy - avg_swipe_amt_24hr) / (avg_swipe_amt_60days + epsilon1)
Description: Absolute swipe txn deviation from 24hr avg, scaled by long-term mean. Detects sudden abnormal swipe patterns.


5. time_since_last_high_dollar
Formula: amt_ge500_ind * time_since_last_trxn
Description: High-value txn flag weighted by time since last txn. Flags spacing between large amounts.


6. mcc_shift_30d_index
Formula: abs(new_mcc_ind - avg_same_mcc_amt_30days)
Description: Deviation of MCC switch from 30-day avg. Highlights abrupt merchant pattern changes.


7. mcc_shift_24hr_index
Formula: abs(new_mcc_ind - avg_same_mcc_amt_24hr)
Description: Captures short-term MCC pattern deviation. Flags recent merchant usage anomalies.


8. auth_decline_ratio
Formula: cnt_same_merc_id_6hr / (cnt_same_merc_id_6hr + 1)
Description: Simple ratio capturing dense merchant hits post-auth decline. Signifies retry patterns.


9. transaction_amt_drift
Formula: (amt_swipe_tdy * avg_trxn_amt_30days) / (avg_trxn_amt_60days + epsilon1)
Description: Recent swipe txn amount scaled by 30d avg, normalized by 60d avg. Detects shifts in txn amount profiles.


10. high_density_swipe_score
Formula: cnt_swipe_24hr / (cnt_swipe_60days + 1)
Description: Swipe txn density score across timeframes. Flags bursts of card-present activity.


11. zero_auth_high_risk_mer_score
Formula: zero_auth_diff_mer_nm_ind * hct_mer_mcc_bayesian_encoded_1
Description: Flags zero-auth attempts at new merchants with MCC-based risk weighting.


12. mcc_high_dollar_ratio
Formula: (amt_5310_5411_24hr * amt_ge500_ind) / (tot_amt_24hr + 1)
Description: Proportion of high-value txns at MCC 5310/5411 (grocery/retail) to total. Detects large-value retail spikes.


13. ratio_high_dollar_cnp_24hr
Formula: (amt_cnp_24hr * amt_ge500_ind) / (tot_amt_24hr + 1)
Description: High-value card-not-present txn ratio over total 24hr value. Flags e-comm fraud risk.


14. multi_window_zero_auth_velocity
Formula: tot_zero_auth_cnt_24hr * tot_zero_auth_cnt_48hr * tot_zero_auth_cnt_72hr
Description: Velocity of zero-auth transactions across 3 time windows. Captures bot-like probing.


15. trxn_hour_foreign_amt_ge500
Formula: trxn_hour * foreign_ind * amt_ge500_ind
Description: Time-weighted risk score for high-value foreign txns. Flags suspicious cross-border behavior.




---

Would you like this as a structured Excel or PDF for documentation?








import os
import xlsxwriter

# Define paths for PDP and Bivariate plots
pdp_folder = "pdp_plot"
bvp_folder = "bvp_plot"
output_excel = "Combined_Plots.xlsx"

# Get the list of feature names (assuming both folders have same naming convention)
feature_names = sorted([f.replace(".png", "") for f in os.listdir(pdp_folder) if f.endswith(".png")])

# Create an Excel workbook
workbook = xlsxwriter.Workbook(output_excel)

# Loop through each feature and add PDP + BVP to the sheet
for feature in feature_names:
    worksheet = workbook.add_worksheet(feature)  # Create a new sheet for each feature
    
    # Define image paths
    pdp_path = os.path.join(pdp_folder, feature + ".png")
    bvp_path = os.path.join(bvp_folder, feature + ".png")
    
    # Insert images into the worksheet
    worksheet.insert_image("A1", pdp_path, {'x_scale': 0.5, 'y_scale': 0.5})  # Adjust size if needed
    worksheet.insert_image("J1", bvp_path, {'x_scale': 0.5, 'y_scale': 0.5})  # Insert BVP next to PDP

# Close and save the Excel file
workbook.close()

print(f"‚úÖ Excel file '{output_excel}' created successfully with 36 sheets!")











import h2o
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from pdpbox import pdp
from h2o.estimators.gbm import H2OGradientBoostingEstimator

# Initialize H2O
h2o.init()

# Load H2O DataFrames (Replace with actual files)
train = h2o.import_file("train.csv")
train_rs = h2o.import_file("train_rs.csv")
test = h2o.import_file("test.csv")
oot = h2o.import_file("oot.csv")

# Convert to Pandas
train_df = train.as_data_frame()
train_rs_df = train_rs.as_data_frame()
test_df = test.as_data_frame()
oot_df = oot.as_data_frame()

# Define Target and Feature List
target = "fraud_label"  # Replace with your actual target column
features = ["feature_1", "feature_2", ..., "feature_30"]  # Replace with your selected variables

# Train H2O GBM Model
gbm_model = H2OGradientBoostingEstimator(ntrees=50, max_depth=5, learn_rate=0.1)
gbm_model.train(x=features, y=target, training_frame=train)

# Convert Data for PDPBox
train_df["dataset"] = "Train"
train_rs_df["dataset"] = "Train_RS"
test_df["dataset"] = "Test"
oot_df["dataset"] = "OOT"

# Combine All Datasets for Analysis
combined_df = pd.concat([train_df, train_rs_df, test_df, oot_df], axis=0)

### üìå Generate Plots for Each Feature ###
for feature in features:
    fig, ax1 = plt.subplots(figsize=(10, 5))

    # 1Ô∏è‚É£ PDP Plot
    pdp_goals = pdp.pdp_isolate(model=gbm_model, dataset=train_df, model_features=features, feature=feature)
    pdp.pdp_plot(pdp_goals, feature, plot_lines=False, ax=ax1)
    ax1.set_title(f"PDP & Fraud Rate Trend - {feature}")
    ax1.set_ylabel("Partial Dependence")
    
    # 2Ô∏è‚É£ Bivariate Plot (Fraud Rate per bin across datasets)
    ax2 = ax1.twinx()
    combined_df["bin"] = pd.qcut(combined_df[feature], q=10, duplicates="drop")  # Binning
    fraud_rates = combined_df.groupby(["bin", "dataset"])[target].mean().unstack()
    fraud_rates.plot(marker="o", ax=ax2)
    
    ax2.set_ylabel("Fraud Rate")
    ax2.legend(title="Dataset")

    plt.show()










import h2o
h2o.init()

# Load your dataset
df = h2o.import_file("path_to_your_dataset.csv")

# Create interaction features
df['trxn_freq_anomaly'] = df['tot_cnt_24hr'] / df['avg_trxn_cnt_30days']
df['time_since_last_trxn_anomaly'] = df['time_since_last_trxn'] / df['avg_time_between_trxns_30days']
df['wkend_wkday_ratio'] = df['tot_cnt_30days_wkend'] / df['tot_cnt_30days_wkday']
df['large_amt_anomaly'] = df['amt_swipe_ge_1000_24hr'] / df['avg_swipe_amt_24hr']
df['swipe_cnp_amt_ratio'] = df['amt_swipe_24hr'] / df['amt_cnp_24hr']
df['zero_auth_ratio'] = df['tot_zero_auth_cnt_24hr'] / df['tot_cnt_24hr']
df['dist_btw_swp_anomaly'] = df['dist_miles_btw_swp'] / df['avg_dist_btw_swp_30days']
df['foreign_trxn_ratio'] = df['cnt_fore_swipe_24hr'] / df['tot_cnt_24hr']
df['new_merch_ratio'] = df['new_merch_ind'] / df['tot_cnt_24hr']
df['mcc_anomaly_score'] = df['same_mcc_cnt_24hr'] / df['avg_same_mcc_cnt_30days']
df['cvv_mismatch_ratio'] = df['cnt_cvv2_mismatch_24hr'] / df['tot_cnt_24hr']
df['avs_mismatch_ratio'] = df['cnt_avs_mismatch_24hr'] / df['tot_cnt_24hr']
df['swipe_cnp_ratio'] = df['cnt_swipe_24hr'] / df['cnt_cnp_24hr']
df['visa_mastercard_ratio'] = df['visa_ind'] / df['mastercard_ind']
df['fraud_risk_score'] = df['cvv_mismatch_ratio'] * df['avs_mismatch_ratio'] * df['new_merch_ratio']
df['geo_temp_anomaly_score'] = df['dist_btw_swp_anomaly'] * df['time_since_last_trxn_anomaly']

# Save the updated dataset
df.save("path_to_updated_dataset.csv")






# Create interaction features
df['trxn_velocity_anomaly'] = df['tot_cnt_24hr'] / df['tot_cnt_30days']
df['zero_auth_time_gap_anomaly'] = df['mins_since_zero_auth'] / df['avg_mins_between_zero_auth_30days']
df['same_merc_cluster_score'] = df['same_merc_cnt_24hr'] / df['same_merc_cnt_30days']
df['cvv_mismatch_time_gap'] = df['time_since_cvv2_mismatch'] / df['avg_time_between_cvv_mismatch_30days']
df['avs_mismatch_time_gap'] = df['time_since_avs_mismatch'] / df['avg_time_between_avs_mismatch_30days']
df['exp_mismatch_ratio'] = df['cnt_exp_date_mismatch_24hr'] / df['tot_cnt_24hr']
df['high_risk_mcc_ratio'] = df['cnt_5310_5411_24hr'] / df['tot_cnt_24hr']
df['new_mcc_ratio'] = df['new_mcc_ind'] / df['tot_cnt_24hr']
df['merch_id_cluster_anomaly'] = df['same_merc_cnt_24hr'] / df['same_merc_cnt_30days']
df['cross_state_anomaly'] = df['diff_state_swp_24hr_ind'] / df['tot_cnt_24hr']
df['time_btw_swp_anomaly'] = df['time_btw_swp'] / df['avg_time_btw_swp_30days']
df['foreign_trxn_time_gap'] = df['time_since_last_foreign_cnp'] / df['avg_time_between_foreign_trxns_30days']
df['fraud_risk_score_2'] = df['cvv_mismatch_ratio'] * df['avs_mismatch_ratio'] * df['exp_mismatch_ratio'] * df['new_merch_ratio']
df['geo_mcc_anomaly_score'] = df['cross_state_anomaly'] * df['high_risk_mcc_ratio']

# Save the updated dataset
df.save("path_to_updated_dataset.csv")





# Create interaction features
df['zero_auth_to_auth_ratio'] = df['tot_zero_auth_cnt_24hr'] / df['tot_cnt_24hr']
df['small_amt_testing_score'] = df['cnt_sameamt_auth_6h'] / df['tot_cnt_24hr']
df['cvv_mismatch_after_zero_auth'] = df['cnt_cvv2_mismatch_24hr'] / df['tot_zero_auth_cnt_24hr']
df['trxn_burst_score'] = df['tot_cnt_30min'] / df['tot_cnt_24hr']
df['same_merc_velocity_anomaly'] = df['same_merc_cnt_30min'] / df['same_merc_cnt_24hr']
df['mcc_velocity_anomaly'] = df['same_mcc_cnt_30min'] / df['same_mcc_cnt_24hr']
df['dist_to_last_trxn_anomaly'] = df['dist_miles_btw_swp'] / df['avg_dist_btw_swp_30days']
df['cross_country_ratio'] = df['cnt_cntry_24hr'] / df['tot_cnt_24hr']
df['time_btw_geo_hops'] = df['time_btw_swp'] / df['dist_miles_btw_swp']
df['same_merc_to_term_ratio'] = df['cnt_same_mid_termid_24hr'] / df['same_merc_cnt_24hr']
df['new_merc_to_term_ratio'] = df['new_merch_id'] / df['cnt_same_mid_termid_24hr']
df['high_risk_merc_cluster'] = (df['same_merc_cnt_24hr'] / df['tot_cnt_24hr']) * df['high_risk_mcc_ratio']
df['fraud_risk_score_3'] = df['zero_auth_to_auth_ratio'] * df['small_amt_testing_score'] * df['cvv_mismatch_after_zero_auth'] * df['trxn_burst_score']
df['geo_velocity_anomaly_score'] = df['dist_to_last_trxn_anomaly'] * df['trxn_burst_score']

# Save the updated dataset
df.save("path_to_updated_dataset.csv")





# Create interaction features
df['swipe_amt_deviation'] = (df['amt_swipe_24hr'] - df['avg_swipe_amt_24hr']) / df['std_swipe_amt_24hr']
df['trxn_hour_anomaly'] = (df['trxn_hour'] - df['avg_trxn_hour_30days']) / df['std_trxn_hour_30days']
df['swipe_cnp_ratio_deviation'] = (df['swipe_cnp_ratio'] - df['avg_swipe_cnp_ratio_30days']) / df['std_swipe_cnp_ratio_30days']
df['high_risk_mcc_bayesian_interaction'] = df['hct_term_mcc_bayesian_encoded_5'] * df['hct_term_mcc_bayesian_encoded_7']
df['new_merch_risk_score'] = df['new_mech_ind'] * (1 + df['cnt_same_merc_id_24hr'] / df['tot_cnt_24hr'])
df['mcc_risk_weighted_cnt'] = df['same_mcc_cnt_24hr'] * df['hct_term_mcc_bayesian_encoded_5']
df['geo_dispersion_score'] = (df['cnt_dom_states_24hr'] / df['tot_cnt_24hr']) * df['dist_miles_btw_swp']
df['time_btw_swp_cvv_mismatch'] = df['time_since_last_swipe'] / df['time_since_cvv2_mismatch']
df['foreign_trxn_risk_score'] = df['foreign_ind'] * (1 + df['cnt_fore_swipe_24hr'] / df['tot_cnt_24hr'])
df['cvv_verf_magn_interaction'] = df['cvv_verf_magn_ind'] * (1 + df['cnt_cvv2_mismatch_24hr'] / df['tot_cnt_24hr'])
df['avs_mismatch_risk_score'] = df['avs_addr_mismatch_ind'] * (1 + df['cnt_avs_mismatch_24hr'] / df['tot_cnt_24hr'])
df['exp_mismatch_risk_score'] = df['exp_date_mismatch_ind'] * (1 + df['cnt_exp_date_mismatch_24hr'] / df['tot_cnt_24hr'])
df['fraud_risk_score_4'] = df['swipe_amt_deviation'] * df['trxn_hour_anomaly'] * df['high_risk_mcc_bayesian_interaction'] * df['new_merch_risk_score']
df['geo_temp_anomaly_score_2'] = df['geo_dispersion_score'] * df['time_btw_swp_cvv_mismatch']

# Save the updated dataset
df.save("path_to_updated_dataset.csv")










GPT


import h2o
import h2o.frame
from h2o.frame import H2OFrame

# Initialize H2O
h2o.init()

# Load dataset
df = h2o.import_file("your_fraud_data.csv")

# Creating advanced interaction features
df["risk_weighted_swipe_intensity"] = (df["cnt_swipe_24hr"] * (1 + df["cvv_verf_magn_ind"])) / (df["cnt_swipe_60days"] + 1)
df["mcc_velocity_risk"] = df["same_mcc_cnt_24hr"] / (df["same_mcc_cnt_60days"] + 1)
df["terminal_deviation"] = (df["cnt_same_mid_termid_24hr"] - df["cnt_same_mid_termid_60days"]) / (df["cnt_same_mid_termid_60days"] + 1)
df["swipe_dist_time_anomaly"] = df["dist_miles_btw_swp"] / (df["time_btw_swp"] + 1)
df["fraud_amt_scaling"] = (df["amt_swipe_24hr"] / (df["amt_swipe_60days"] + 1)) * (1 / (df["pct_swipe_amt_60days"] + 0.01))
df["avs_exp_mismatch_score"] = (df["cnt_avs_mismatch_24hr"] + df["cnt_exp_date_mismatch_24hr"]) / (df["cnt_avs_mismatch_60days"] + df["cnt_exp_date_mismatch_60days"] + 1)
df["card_testing_ratio"] = df["cnt_zero_auth_24hr"] / (df["cnt_zero_auth_60days"] + 1)
df["high_risk_merchant_score"] = df["cnt_5310_5411_24hr"] / (df["cnt_5310_5411_60days"] + 1)

# Save the enhanced dataset
h2o.export_file(df, "fraud_data_with_interaction_features.csv")
