import pandas as pd
import numpy as np

# Step 1: Replace None with NaN for consistency
df1 = df1.replace({None: np.nan})
df2 = df2.replace({None: np.nan})

# Step 2: Sort by all columns (so identical rows align)
df1_sorted = df1.sort_values(by=list(df1.columns)).reset_index(drop=True)
df2_sorted = df2.sort_values(by=list(df2.columns)).reset_index(drop=True)

# Step 3: Ensure same columns and shape
assert list(df1_sorted.columns) == list(df2_sorted.columns), "‚ùå Column names differ!"
assert df1_sorted.shape == df2_sorted.shape, "‚ùå Row counts differ!"

# Step 4: Normalize values ‚Äî handle numeric equivalence (-64.0 == -64)
def normalize_value(v):
    if pd.isna(v):
        return np.nan
    # Convert strings that look numeric
    if isinstance(v, str):
        try:
            return float(v)
        except ValueError:
            return v.strip()
    # Convert ints to float for fair comparison
    if isinstance(v, (int, np.integer)):
        return float(v)
    return v

df1_sorted = df1_sorted.applymap(normalize_value)
df2_sorted = df2_sorted.applymap(normalize_value)

# Step 5: Compare DataFrames row by row
mismatch_records = []

for i in range(len(df1_sorted)):
    row1 = df1_sorted.iloc[i]
    row2 = df2_sorted.iloc[i]
    
    # Create a pseudo-row identifier (like hash of all values)
    row_identifier = f"ROW_{i+1}"
    
    for col in df1_sorted.columns:
        val1, val2 = row1[col], row2[col]
        
        # Skip if both NaN
        if pd.isna(val1) and pd.isna(val2):
            continue
        
        # Compare numeric types safely
        if isinstance(val1, (int, float)) and isinstance(val2, (int, float)):
            if not np.isclose(val1, val2, equal_nan=True):
                mismatch_records.append({'Row': row_identifier, 'Column': col, 'df1_Value': val1, 'df2_Value': val2})
        else:
            if val1 != val2:
                mismatch_records.append({'Row': row_identifier, 'Column': col, 'df1_Value': val1, 'df2_Value': val2})

# Step 6: Create final mismatch DataFrame
mismatch_df = pd.DataFrame(mismatch_records)

if mismatch_df.empty:
    print("‚úÖ All rows and columns match perfectly!")
else:
    print(f"‚ùå Found {len(mismatch_df)} mismatches:")
    display(mismatch_df)












df1_sorted['ROW_HASH'] = df1_sorted.astype(str).agg('|'.join, axis=1)
df2_sorted['ROW_HASH'] = df2_sorted.astype(str).agg('|'.join, axis=1)

common = set(df1_sorted['ROW_HASH']).intersection(set(df2_sorted['ROW_HASH']))
print(f"‚úÖ {len(common)} rows are exactly matching by all column values.")





















import pandas as pd
import numpy as np

# Step 1: Sort both DataFrames by SOEID
df1_sorted = df1.sort_values(by='SOEID').reset_index(drop=True)
df2_sorted = df2.sort_values(by='SOEID').reset_index(drop=True)

# Step 2: Replace None with NaN for consistency
df1_sorted = df1_sorted.replace({None: np.nan})
df2_sorted = df2_sorted.replace({None: np.nan})

# Step 3: Ensure both DataFrames have same columns
assert list(df1_sorted.columns) == list(df2_sorted.columns), "‚ùå Column names differ!"
assert df1_sorted.shape == df2_sorted.shape, "‚ùå Shapes differ!"

# Step 4: Normalize numeric types (so -64 and -64.0 are treated same)
def normalize_value(v):
    if pd.isna(v):
        return np.nan
    # Try converting numeric-like strings to float
    if isinstance(v, str) and v.replace('.', '', 1).replace('-', '', 1).isdigit():
        try:
            return float(v)
        except:
            return v
    # Convert ints to float (so types match)
    if isinstance(v, (int, np.integer)):
        return float(v)
    return v

df1_sorted = df1_sorted.applymap(normalize_value)
df2_sorted = df2_sorted.applymap(normalize_value)

# Step 5: Compare and collect mismatches
mismatch_records = []

for i in range(len(df1_sorted)):
    soe_id = df1_sorted.iloc[i]['SOEID']
    row1 = df1_sorted.iloc[i]
    row2 = df2_sorted.iloc[i]
    
    for col in df1_sorted.columns:
        val1, val2 = row1[col], row2[col]
        
        # Skip if both NaN
        if pd.isna(val1) and pd.isna(val2):
            continue
        
        # Numeric comparison tolerance (so -64.0 == -64)
        if isinstance(val1, (int, float)) and isinstance(val2, (int, float)):
            if not np.isclose(val1, val2, equal_nan=True):
                mismatch_records.append({'SOEID': soe_id, 'Column': col, 'df1_Value': val1, 'df2_Value': val2})
        else:
            if val1 != val2:
                mismatch_records.append({'SOEID': soe_id, 'Column': col, 'df1_Value': val1, 'df2_Value': val2})

# Step 6: Create final mismatch dataframe
mismatch_df = pd.DataFrame(mismatch_records)

if mismatch_df.empty:
    print("‚úÖ All rows and columns match perfectly!")
else:
    print(f"‚ùå Found {len(mismatch_df)} mismatches:")
    display(mismatch_df)

























import pandas as pd
import numpy as np

# Step 1: Sort both DataFrames by SOEID
df1_sorted = df1.sort_values(by='SOEID').reset_index(drop=True)
df2_sorted = df2.sort_values(by='SOEID').reset_index(drop=True)

# Step 2: Ensure NaN consistency (treat None as NaN)
df1_sorted = df1_sorted.replace({None: np.nan})
df2_sorted = df2_sorted.replace({None: np.nan})

# Step 3: Ensure both DataFrames have same columns and shape
assert list(df1_sorted.columns) == list(df2_sorted.columns), "‚ùå Column names differ!"
assert df1_sorted.shape == df2_sorted.shape, "‚ùå Shapes differ!"

# Step 4: Identify mismatched cells
mismatch_records = []

for i in range(len(df1_sorted)):
    soe_id = df1_sorted.iloc[i]['SOEID']
    row1 = df1_sorted.iloc[i]
    row2 = df2_sorted.iloc[i]
    
    for col in df1_sorted.columns:
        val1, val2 = row1[col], row2[col]
        
        # Compare NaN safely
        if (pd.isna(val1) and pd.isna(val2)):
            continue
        elif val1 != val2:
            mismatch_records.append({
                'SOEID': soe_id,
                'Column': col,
                'df1_Value': val1,
                'df2_Value': val2
            })

# Step 5: Create DataFrame of mismatches
mismatch_df = pd.DataFrame(mismatch_records)

if mismatch_df.empty:
    print("‚úÖ All rows and columns match perfectly!")
else:
    print("‚ùå Found mismatches:")
    display(mismatch_df)






















import pandas as pd
import numpy as np

# Step 1: Sort both dataframes by SOE_ID
df1_sorted = df1.sort_values(by="SOE_ID").reset_index(drop=True)
df2_sorted = df2.sort_values(by="SOE_ID").reset_index(drop=True)

# Step 2: Replace None with np.nan so both are comparable
df1_sorted = df1_sorted.replace({None: np.nan})
df2_sorted = df2_sorted.replace({None: np.nan})

# Step 3: Check if both dataframes are exactly equal
are_equal = df1_sorted.equals(df2_sorted)

print(f"‚úÖ DataFrames Match: {are_equal}")

# Step 4: If not equal, find mismatched rows and columns
if not are_equal:
    # Create a mask of where values differ
    mismatch_mask = (df1_sorted != df2_sorted) & ~(df1_sorted.isna() & df2_sorted.isna())
    
    # Get rows with at least one mismatch
    mismatched_rows = df1_sorted[mismatch_mask.any(axis=1)].copy()
    mismatched_rows['SOE_ID'] = df1_sorted.loc[mismatch_mask.any(axis=1), 'SOE_ID']
    
    # For clarity, also show which columns differ
    mismatched_columns = mismatch_mask.any()
    diff_columns = mismatch_mask.columns[mismatched_columns].tolist()
    
    print("\n‚ö†Ô∏è Mismatched Columns:")
    print(diff_columns)
    
    print("\n‚ö†Ô∏è Mismatched Rows with Differences:")
    display(pd.concat([
        df1_sorted[mismatch_mask.any(axis=1)].add_suffix('_DF1'),
        df2_sorted[mismatch_mask.any(axis=1)].add_suffix('_DF2')
    ], axis=1))















import pandas as pd
import numpy as np

# Ensure same shape, column order, and reset index
df1 = df1.reset_index(drop=True)
df2 = df2.reset_index(drop=True)
df1 = df1[df2.columns]

# Compare while treating NaN/None as equal
comparison = (df1.eq(df2)) | (df1.isna() & df2.isna())

# Find rows with any mismatched column
diff_rows = ~comparison.all(axis=1)

records = []

# Loop through mismatched rows
for idx in df1.index[diff_rows]:
    mismatched_cols = df1.columns[~comparison.loc[idx]].tolist()
    row_data = {
        "Row_Index": idx,
        "Mismatched_Columns": mismatched_cols
    }
    
    # Add mismatched column values side-by-side
    for col in mismatched_cols:
        row_data[f"df1_{col}"] = df1.at[idx, col]
        row_data[f"df2_{col}"] = df2.at[idx, col]
    
    records.append(row_data)

# Create the mismatch summary dataframe
mismatch_df = pd.DataFrame(records)

print(f"‚úÖ Total mismatched rows: {len(mismatch_df)}")
display(mismatch_df)





















import pandas as pd

# Ensure same column order and length
df1 = df1.reset_index(drop=True)
df2 = df2.reset_index(drop=True)
df1 = df1[df2.columns]

# Identify mismatched rows
comparison = df1 == df2
diff_rows = ~comparison.all(axis=1)

# Collect mismatched info
records = []

for idx in df1.index[diff_rows]:
    mismatched_cols = df1.columns[df1.loc[idx] != df2.loc[idx]].tolist()
    row_data = {
        "Row_Index": idx,
        "Mismatched_Columns": mismatched_cols
    }
    
    # Add df1 and df2 values for mismatched columns only
    for col in mismatched_cols:
        row_data[f"df1_{col}"] = df1.at[idx, col]
        row_data[f"df2_{col}"] = df2.at[idx, col]
    
    records.append(row_data)

# Create the summary dataframe
mismatch_df = pd.DataFrame(records)

print(f"Total mismatched rows: {len(mismatch_df)}")
display(mismatch_df)





















import pandas as pd
import numpy as np

# Ensure same column order
df1 = df1[df2.columns]

# Compare element-wise
comparison = df1 == df2

# Rows where any column differs
diff_rows = ~comparison.all(axis=1)

# Show rows that differ
diff_report = pd.concat(
    [df1[diff_rows].add_prefix('df1_'), df2[diff_rows].add_prefix('df2_')],
    axis=1
)

print("Total rows mismatched:", diff_rows.sum())
display(diff_report)





mismatch_details = {}

for idx in df1.index:
    mismatched_cols = df1.columns[df1.loc[idx] != df2.loc[idx]].tolist()
    if mismatched_cols:
        mismatch_details[idx] = mismatched_cols

print("Mismatched rows and columns:")
for i, cols in mismatch_details.items():
    print(f"Row {i}: columns {cols}")















import pandas as pd

key = 'POSITION_ID'

# --- Step 1: Filter only duplicates
dupes = df[df.duplicated(subset=[key], keep=False)].copy()

# --- Step 2: Group and find differences
records = []

for pid, group in dupes.groupby(key):
    group = group.reset_index(drop=True)
    diff_cols = [col for col in group.columns if col != key and group[col].nunique(dropna=False) > 1]

    if diff_cols:
        diff_data = {}
        diff_data[key] = pid
        diff_data['Columns_Not_Matching'] = ', '.join(diff_cols)

        # Collect differing values for each mismatched column
        for col in diff_cols:
            diff_data[f'{col}_Values'] = group[col].astype(str).unique().tolist()

        records.append(diff_data)

# --- Step 3: Create result DataFrame
mismatch_report = pd.DataFrame(records)

print("\n‚ö†Ô∏è Duplicate IDs with non-matching column values:")
print(mismatch_report)



















import pandas as pd

key = 'POSITION_ID'

# --- Find duplicates
dupes = df[df.duplicated(subset=[key], keep=False)].copy()

# --- Group by POSITION_ID and check if all rows within that group are identical
# We do this by comparing number of unique rows per group
def check_group_identical(group):
    return group.drop(columns=[key]).nunique(dropna=False).eq(1).all()

duplicate_summary = (
    dupes.groupby(key)
         .apply(check_group_identical)
         .reset_index(name='All_Columns_Same')
)

# --- Split into two categories
true_duplicates = duplicate_summary[duplicate_summary['All_Columns_Same']]
conflicting_duplicates = duplicate_summary[~duplicate_summary['All_Columns_Same']]

print("\n‚úÖ True duplicates (completely identical rows):")
print(true_duplicates)

print("\n‚ö†Ô∏è Conflicting duplicates (same ID, but different column values):")
print(conflicting_duplicates)






















import pandas as pd
import numpy as np

# Example: df1 and df2 already loaded (from Excel or other sources)
# Common key column
key = 'POSITION_ID'

# Align on POSITION_ID to ensure same row order for comparison
df1 = df1.set_index(key).sort_index()
df2 = df2.set_index(key).sort_index()

# Ensure both have same columns
common_cols = df1.columns.intersection(df2.columns)

# Compare values treating NaN/None as equal
comparison = (df1[common_cols].fillna('__NULL__') == df2[common_cols].fillna('__NULL__'))

# Summary per column
summary = comparison.sum().to_frame(name='Matching_Count')
summary['Total_Rows'] = len(comparison)
summary['Match_%'] = (summary['Matching_Count'] / summary['Total_Rows'] * 100).round(2)

# Identify mismatched rows for detailed check
mismatch_details = (
    ~comparison
).any(axis=1)

mismatch_report = pd.DataFrame({
    key: df1.index[mismatch_details],
    'Columns_Not_Matching': comparison.columns[~comparison.loc[mismatch_details].all()].tolist()
})

# Final summary output
print("\nüîç Column-wise match summary:")
print(summary)

print("\n‚ö†Ô∏è Mismatch details (rows where at least one column differs):")
print(mismatch_report)



















import pandas as pd

# Example: assuming df1 and df2 have same columns including POSITION_ID

# Merge both dataframes on POSITION_ID
merged = df1.merge(df2, on='POSITION_ID', suffixes=('_df1', '_df2'))

# List of columns to compare (excluding POSITION_ID)
columns_to_compare = [col for col in df1.columns if col != 'POSITION_ID']

# For each column, check if they match
for col in columns_to_compare:
    merged[f'{col}_MATCH'] = merged[f'{col}_df1'] == merged[f'{col}_df2']

# Compute summary match status per POSITION_ID
summary = merged[['POSITION_ID'] + [f'{col}_MATCH' for col in columns_to_compare]]

# Add an overall flag if all columns matched
summary['ALL_COLUMNS_MATCH'] = summary[[f'{col}_MATCH' for col in columns_to_compare]].all(axis=1)

# Display result
print(summary)





mismatched = summary[~summary['ALL_COLUMNS_MATCH']]
print(mismatched)






















SET SERVEROUTPUT ON;

DECLARE
    v_sql CLOB := '';
BEGIN
    FOR rec IN (
        SELECT column_name
        FROM all_tab_columns
        WHERE owner = 'WFA_APP_USR_TALD'
          AND table_name = 'BASE_WD_REPORT_VIEW_POSITION'
          AND column_name <> 'PERIOD'  -- ignore period filter column
          AND column_name <> 'POSITION_ID'  -- key column
        ORDER BY column_id
    ) LOOP
        v_sql := v_sql || 'WHEN a.' || rec.column_name || ' <> b.' || rec.column_name ||
                 ' THEN ''' || rec.column_name || '''' || CHR(10);
    END LOOP;

    DBMS_OUTPUT.PUT_LINE('--- Generated Comparison Query ---');
    DBMS_OUTPUT.PUT_LINE('SELECT a.POSITION_ID, CASE ' || CHR(10) || v_sql || 
        ' END AS MISMATCH_COLUMN
FROM WFA_APP_USR_TALD.BASE_WD_REPORT_VIEW_POSITION a
JOIN WFA_APP_USR_TALD.BASE_WD_REPORT_VIEW_POSITION_PYTHON b
  ON a.POSITION_ID = b.POSITION_ID
WHERE a.PERIOD = ''10/27/2025''
  AND b.PERIOD = ''10/27/2025''
  AND ( ');

    FOR rec IN (
        SELECT column_name
        FROM all_tab_columns
        WHERE owner = 'WFA_APP_USR_TALD'
          AND table_name = 'BASE_WD_REPORT_VIEW_POSITION'
          AND column_name <> 'PERIOD'
          AND column_name <> 'POSITION_ID'
        ORDER BY column_id
    ) LOOP
        v_sql := v_sql || 'a.' || rec.column_name || ' <> b.' || rec.column_name || ' OR ';
    END LOOP;

    DBMS_OUTPUT.PUT_LINE(RTRIM(v_sql, ' OR ') || ' );');
END;
/






















SELECT
    a.POSITION_ID,
    CASE
        WHEN a.POSITION_NAME != b.POSITION_NAME THEN 'POSITION_NAME'
        WHEN a.MANAGER_NAME  != b.MANAGER_NAME  THEN 'MANAGER_NAME'
        WHEN a.COUNTRY       != b.COUNTRY       THEN 'COUNTRY'
        WHEN a.DEPARTMENT    != b.DEPARTMENT    THEN 'DEPARTMENT'
        WHEN a.LEVEL         != b.LEVEL         THEN 'LEVEL'
    END AS MISMATCH_COLUMN
FROM WFA_APP_USR_TALD.BASE_WD_REPORT_VIEW_POSITION a
JOIN WFA_APP_USR_TALD.BASE_WD_REPORT_VIEW_POSITION_PYTHON b
  ON a.POSITION_ID = b.POSITION_ID
WHERE a.PERIOD = '10/27/2025'
  AND b.PERIOD = '10/27/2025'
  AND (
        a.POSITION_NAME != b.POSITION_NAME OR
        a.MANAGER_NAME  != b.MANAGER_NAME OR
        a.COUNTRY       != b.COUNTRY OR
        a.DEPARTMENT    != b.DEPARTMENT OR
        a.LEVEL         != b.LEVEL
      );



























SELECT POSITION_ID, 'MISSING_IN_PYTHON' AS STATUS
FROM WFA_APP_USR_TALD_BASE_WD_REPORT_VIEW_POSITION
WHERE POSITION_ID NOT IN (
    SELECT POSITION_ID FROM WFA_APP_USR_TALD_BASE_WD_REPORT_VIEW_POSITION_PYTHON
)
UNION ALL
SELECT POSITION_ID, 'EXTRA_IN_PYTHON' AS STATUS
FROM WFA_APP_USR_TALD_BASE_WD_REPORT_VIEW_POSITION_PYTHON
WHERE POSITION_ID NOT IN (
    SELECT POSITION_ID FROM WFA_APP_USR_TALD_BASE_WD_REPORT_VIEW_POSITION
);











SET SERVEROUTPUT ON
DECLARE
    v_sql   CLOB;
BEGIN
    -- 1Ô∏è‚É£ Drop existing summary table if exists
    BEGIN
        EXECUTE IMMEDIATE 'DROP TABLE POSITION_MISMATCH_SUMMARY';
    EXCEPTION
        WHEN OTHERS THEN
            NULL; -- ignore if table doesn‚Äôt exist
    END;

    -- 2Ô∏è‚É£ Create new summary table
    EXECUTE IMMEDIATE '
        CREATE TABLE POSITION_MISMATCH_SUMMARY (
            POSITION_ID VARCHAR2(200),
            MISMATCHED_COLUMNS VARCHAR2(4000)
        )
    ';

    -- 3Ô∏è‚É£ Start building dynamic comparison SQL
    v_sql := 'INSERT INTO POSITION_MISMATCH_SUMMARY
              SELECT a.POSITION_ID, ';

    -- Generate CASE-based concatenation for mismatched columns
    FOR rec IN (
        SELECT COLUMN_NAME
        FROM USER_TAB_COLUMNS
        WHERE TABLE_NAME = 'WFA_APP_USR_TALD_BASE_WD_REPORT_VIEW_POSITION'
          AND COLUMN_NAME <> 'POSITION_ID'
        ORDER BY COLUMN_ID
    )
    LOOP
        v_sql := v_sql ||
            'CASE WHEN NVL(a.' || rec.COLUMN_NAME || ', ''X'') != NVL(b.' || rec.COLUMN_NAME || ', ''X'') 
                  THEN ''' || rec.COLUMN_NAME || ' '' ELSE '''' END || ';
    END LOOP;

    -- Remove trailing concatenation operator
    v_sql := RTRIM(v_sql, ' || ');

    -- Continue query body
    v_sql := v_sql || ' AS MISMATCHED_COLUMNS
        FROM WFA_APP_USR_TALD_BASE_WD_REPORT_VIEW_POSITION a
        JOIN WFA_APP_USR_TALD_BASE_WD_REPORT_VIEW_POSITION_PYTHON b
          ON a.POSITION_ID = b.POSITION_ID
        WHERE ';

    -- Generate WHERE clause dynamically
    FOR rec IN (
        SELECT COLUMN_NAME
        FROM USER_TAB_COLUMNS
        WHERE TABLE_NAME = 'WFA_APP_USR_TALD_BASE_WD_REPORT_VIEW_POSITION'
          AND COLUMN_NAME <> 'POSITION_ID'
        ORDER BY COLUMN_ID
    )
    LOOP
        v_sql := v_sql || 'NVL(a.' || rec.COLUMN_NAME || ', ''X'') != NVL(b.' || rec.COLUMN_NAME || ', ''X'') OR ';
    END LOOP;

    -- Remove trailing OR
    v_sql := RTRIM(v_sql, ' OR ');

    -- 4Ô∏è‚É£ Execute the dynamic SQL
    EXECUTE IMMEDIATE v_sql;

    DBMS_OUTPUT.PUT_LINE('‚úÖ Comparison completed successfully.');
    DBMS_OUTPUT.PUT_LINE('‚û°Ô∏è  Use: SELECT * FROM POSITION_MISMATCH_SUMMARY;');
EXCEPTION
    WHEN OTHERS THEN
        DBMS_OUTPUT.PUT_LINE('‚ùå Error: ' || SQLERRM);
END;
/



SELECT * FROM POSITION_MISMATCH_SUMMARY;











Here's a technical 2-liner description for each fraud detection feature based on the formulas:

1. high_dollar_velocity_3hr:
Ratio of high-value (‚â•300) txn count in 3hr to total txn count in 3hr. Captures rapid bursts of high-value activity.


2. high_dollar_ratio_24hr:
Ratio of high-value txn amount to total txn amount in 24hr. Highlights dominance of large-value txns in a short time.


3. exp_date_mismatch_ratio_7days:
Count of expiration date mismatches over total txn count in 7 days. Detects frequent CVV/expiry mismatches often tied to synthetic fraud.


4. zero_auth_mismatch_ratio_24hr:
Ratio of zero-auth mismatch count to zero-auth txn count in 24hr. Flags abnormal patterns in pre-auth probes.


5. foreign_high_dollar_ratio:
Ratio of high-value foreign txn amount to total txn amount in 24hr. Indicates risky cross-border high-value txns.


6. weekend_high_dollar_ratio:
Ratio of weekend high-value txn amount to total weekend txn amount in 24hr. Targets off-hour fraud attempts when oversight is low.


7. nighttime_high_dollar_velocity:
High-value txn velocity during 10pm‚Äì6am, normalized by txn count. Tracks abnormal night-time high-value behavior.


8. high_dollar_swipe_ratio:
Proportion of swipe high-value txn amount to total amount. Flags high-value in-person swipes that deviate from profile.


9. ratio_avg_swipe_amt_24hr_by_60days:
Average swipe txn amount in 24hr normalized by 60-day average. Highlights short-term spending anomalies.


10. transaction_acceleration_24hr:
Ratio of 24hr txn amount to 48hr trailing window. Quantifies sharp spending increases.


11. avmismatch_ratio_30days:
Count of AV (address verification) mismatches to total txns over 30 days. Flags consistent identity verification failures.


12. zero_auth_high_dollar_ratio:
Ratio of high-value zero-auth txn amount to total zero-auth txn amount. Targets abnormal testing behaviors with large amounts.


13. high_val_tx_velocity:
Ratio of high-value txns to txn count at the same merchant in 24hr. Detects merchant-specific fraud bursts.


14. fraud_trx_drift:
Ratio of fraudulent txn amount to average txn amount at merchant over 60 days. Captures fraud drift toward certain merchants.


15. cnt_same_mid_termid_24hr:
Count of txns at same merchant-terminal combo in 24hr. Detects excessive usage of specific terminal for fraud.


16. cnt_for_swipe_24hr:
Count of swiped txns in 24hr window. Useful for card-present fraud detection.


17. velocity_merchant_risk:
Velocity feature capturing merchant-level txn acceleration. Quantifies merchant risk based on usage pattern deviation.


18. cnt_same_merc_id_24hr:
Count of txns at same merchant ID in 24hr. Flags repeated usage of a merchant which may be fraud hotspot.


19. cnt_same_merc_id_30min:
Same as above but in 30min interval. Captures short-burst fraud attempts.


20. hot_mer_mcc_bayesian_encoded_1/11/21/24hr:
Bayesian encoding of merchant MCC risk based on target leakage in various time windows. Encodes historical MCC fraud likelihood in rolling fashion.



Let me know if you want this exported to a table or file format for presentation.









Here is the feature-wise technical description with exact formula context from your image:


---

1. high_dollar_fraud_mer_ratio
Formula: (amt_same_mid_termid_30min) / (amt_same_mid_termid_24hr + 1)
Description: Ratio of high-value txn amount within 30min to total 24hr amount at the same MID-Terminal. Detects rapid high-value fraud bursts.


2. fraud_zero_auth_ratio
Formula: (cnt_za_ris_24hr * cnt_za_mis_7days) / (tot_zero_auth_cnt_24hr + 1)
Description: Product of recent risky zero-auth txn count and 7-day mismatch count, normalized by total 24hr zero-auth volume. Captures anomaly in zero-auth behavior.


3. high_risk_mer8_name_shift
Formula: new_merch_ind * hct_mer_mcc_bayesian_encoded_8
Description: Binary merchant shift indicator weighted by MCC Bayesian risk. Highlights movement toward historically risky MCCs.


4. swipe_anomaly_score
Formula: abs(amt_swipe_tdy - avg_swipe_amt_24hr) / (avg_swipe_amt_60days + epsilon1)
Description: Absolute swipe txn deviation from 24hr avg, scaled by long-term mean. Detects sudden abnormal swipe patterns.


5. time_since_last_high_dollar
Formula: amt_ge500_ind * time_since_last_trxn
Description: High-value txn flag weighted by time since last txn. Flags spacing between large amounts.


6. mcc_shift_30d_index
Formula: abs(new_mcc_ind - avg_same_mcc_amt_30days)
Description: Deviation of MCC switch from 30-day avg. Highlights abrupt merchant pattern changes.


7. mcc_shift_24hr_index
Formula: abs(new_mcc_ind - avg_same_mcc_amt_24hr)
Description: Captures short-term MCC pattern deviation. Flags recent merchant usage anomalies.


8. auth_decline_ratio
Formula: cnt_same_merc_id_6hr / (cnt_same_merc_id_6hr + 1)
Description: Simple ratio capturing dense merchant hits post-auth decline. Signifies retry patterns.


9. transaction_amt_drift
Formula: (amt_swipe_tdy * avg_trxn_amt_30days) / (avg_trxn_amt_60days + epsilon1)
Description: Recent swipe txn amount scaled by 30d avg, normalized by 60d avg. Detects shifts in txn amount profiles.


10. high_density_swipe_score
Formula: cnt_swipe_24hr / (cnt_swipe_60days + 1)
Description: Swipe txn density score across timeframes. Flags bursts of card-present activity.


11. zero_auth_high_risk_mer_score
Formula: zero_auth_diff_mer_nm_ind * hct_mer_mcc_bayesian_encoded_1
Description: Flags zero-auth attempts at new merchants with MCC-based risk weighting.


12. mcc_high_dollar_ratio
Formula: (amt_5310_5411_24hr * amt_ge500_ind) / (tot_amt_24hr + 1)
Description: Proportion of high-value txns at MCC 5310/5411 (grocery/retail) to total. Detects large-value retail spikes.


13. ratio_high_dollar_cnp_24hr
Formula: (amt_cnp_24hr * amt_ge500_ind) / (tot_amt_24hr + 1)
Description: High-value card-not-present txn ratio over total 24hr value. Flags e-comm fraud risk.


14. multi_window_zero_auth_velocity
Formula: tot_zero_auth_cnt_24hr * tot_zero_auth_cnt_48hr * tot_zero_auth_cnt_72hr
Description: Velocity of zero-auth transactions across 3 time windows. Captures bot-like probing.


15. trxn_hour_foreign_amt_ge500
Formula: trxn_hour * foreign_ind * amt_ge500_ind
Description: Time-weighted risk score for high-value foreign txns. Flags suspicious cross-border behavior.




---

Would you like this as a structured Excel or PDF for documentation?








import os
import xlsxwriter

# Define paths for PDP and Bivariate plots
pdp_folder = "pdp_plot"
bvp_folder = "bvp_plot"
output_excel = "Combined_Plots.xlsx"

# Get the list of feature names (assuming both folders have same naming convention)
feature_names = sorted([f.replace(".png", "") for f in os.listdir(pdp_folder) if f.endswith(".png")])

# Create an Excel workbook
workbook = xlsxwriter.Workbook(output_excel)

# Loop through each feature and add PDP + BVP to the sheet
for feature in feature_names:
    worksheet = workbook.add_worksheet(feature)  # Create a new sheet for each feature
    
    # Define image paths
    pdp_path = os.path.join(pdp_folder, feature + ".png")
    bvp_path = os.path.join(bvp_folder, feature + ".png")
    
    # Insert images into the worksheet
    worksheet.insert_image("A1", pdp_path, {'x_scale': 0.5, 'y_scale': 0.5})  # Adjust size if needed
    worksheet.insert_image("J1", bvp_path, {'x_scale': 0.5, 'y_scale': 0.5})  # Insert BVP next to PDP

# Close and save the Excel file
workbook.close()

print(f"‚úÖ Excel file '{output_excel}' created successfully with 36 sheets!")











import h2o
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from pdpbox import pdp
from h2o.estimators.gbm import H2OGradientBoostingEstimator

# Initialize H2O
h2o.init()

# Load H2O DataFrames (Replace with actual files)
train = h2o.import_file("train.csv")
train_rs = h2o.import_file("train_rs.csv")
test = h2o.import_file("test.csv")
oot = h2o.import_file("oot.csv")

# Convert to Pandas
train_df = train.as_data_frame()
train_rs_df = train_rs.as_data_frame()
test_df = test.as_data_frame()
oot_df = oot.as_data_frame()

# Define Target and Feature List
target = "fraud_label"  # Replace with your actual target column
features = ["feature_1", "feature_2", ..., "feature_30"]  # Replace with your selected variables

# Train H2O GBM Model
gbm_model = H2OGradientBoostingEstimator(ntrees=50, max_depth=5, learn_rate=0.1)
gbm_model.train(x=features, y=target, training_frame=train)

# Convert Data for PDPBox
train_df["dataset"] = "Train"
train_rs_df["dataset"] = "Train_RS"
test_df["dataset"] = "Test"
oot_df["dataset"] = "OOT"

# Combine All Datasets for Analysis
combined_df = pd.concat([train_df, train_rs_df, test_df, oot_df], axis=0)

### üìå Generate Plots for Each Feature ###
for feature in features:
    fig, ax1 = plt.subplots(figsize=(10, 5))

    # 1Ô∏è‚É£ PDP Plot
    pdp_goals = pdp.pdp_isolate(model=gbm_model, dataset=train_df, model_features=features, feature=feature)
    pdp.pdp_plot(pdp_goals, feature, plot_lines=False, ax=ax1)
    ax1.set_title(f"PDP & Fraud Rate Trend - {feature}")
    ax1.set_ylabel("Partial Dependence")
    
    # 2Ô∏è‚É£ Bivariate Plot (Fraud Rate per bin across datasets)
    ax2 = ax1.twinx()
    combined_df["bin"] = pd.qcut(combined_df[feature], q=10, duplicates="drop")  # Binning
    fraud_rates = combined_df.groupby(["bin", "dataset"])[target].mean().unstack()
    fraud_rates.plot(marker="o", ax=ax2)
    
    ax2.set_ylabel("Fraud Rate")
    ax2.legend(title="Dataset")

    plt.show()










import h2o
h2o.init()

# Load your dataset
df = h2o.import_file("path_to_your_dataset.csv")

# Create interaction features
df['trxn_freq_anomaly'] = df['tot_cnt_24hr'] / df['avg_trxn_cnt_30days']
df['time_since_last_trxn_anomaly'] = df['time_since_last_trxn'] / df['avg_time_between_trxns_30days']
df['wkend_wkday_ratio'] = df['tot_cnt_30days_wkend'] / df['tot_cnt_30days_wkday']
df['large_amt_anomaly'] = df['amt_swipe_ge_1000_24hr'] / df['avg_swipe_amt_24hr']
df['swipe_cnp_amt_ratio'] = df['amt_swipe_24hr'] / df['amt_cnp_24hr']
df['zero_auth_ratio'] = df['tot_zero_auth_cnt_24hr'] / df['tot_cnt_24hr']
df['dist_btw_swp_anomaly'] = df['dist_miles_btw_swp'] / df['avg_dist_btw_swp_30days']
df['foreign_trxn_ratio'] = df['cnt_fore_swipe_24hr'] / df['tot_cnt_24hr']
df['new_merch_ratio'] = df['new_merch_ind'] / df['tot_cnt_24hr']
df['mcc_anomaly_score'] = df['same_mcc_cnt_24hr'] / df['avg_same_mcc_cnt_30days']
df['cvv_mismatch_ratio'] = df['cnt_cvv2_mismatch_24hr'] / df['tot_cnt_24hr']
df['avs_mismatch_ratio'] = df['cnt_avs_mismatch_24hr'] / df['tot_cnt_24hr']
df['swipe_cnp_ratio'] = df['cnt_swipe_24hr'] / df['cnt_cnp_24hr']
df['visa_mastercard_ratio'] = df['visa_ind'] / df['mastercard_ind']
df['fraud_risk_score'] = df['cvv_mismatch_ratio'] * df['avs_mismatch_ratio'] * df['new_merch_ratio']
df['geo_temp_anomaly_score'] = df['dist_btw_swp_anomaly'] * df['time_since_last_trxn_anomaly']

# Save the updated dataset
df.save("path_to_updated_dataset.csv")






# Create interaction features
df['trxn_velocity_anomaly'] = df['tot_cnt_24hr'] / df['tot_cnt_30days']
df['zero_auth_time_gap_anomaly'] = df['mins_since_zero_auth'] / df['avg_mins_between_zero_auth_30days']
df['same_merc_cluster_score'] = df['same_merc_cnt_24hr'] / df['same_merc_cnt_30days']
df['cvv_mismatch_time_gap'] = df['time_since_cvv2_mismatch'] / df['avg_time_between_cvv_mismatch_30days']
df['avs_mismatch_time_gap'] = df['time_since_avs_mismatch'] / df['avg_time_between_avs_mismatch_30days']
df['exp_mismatch_ratio'] = df['cnt_exp_date_mismatch_24hr'] / df['tot_cnt_24hr']
df['high_risk_mcc_ratio'] = df['cnt_5310_5411_24hr'] / df['tot_cnt_24hr']
df['new_mcc_ratio'] = df['new_mcc_ind'] / df['tot_cnt_24hr']
df['merch_id_cluster_anomaly'] = df['same_merc_cnt_24hr'] / df['same_merc_cnt_30days']
df['cross_state_anomaly'] = df['diff_state_swp_24hr_ind'] / df['tot_cnt_24hr']
df['time_btw_swp_anomaly'] = df['time_btw_swp'] / df['avg_time_btw_swp_30days']
df['foreign_trxn_time_gap'] = df['time_since_last_foreign_cnp'] / df['avg_time_between_foreign_trxns_30days']
df['fraud_risk_score_2'] = df['cvv_mismatch_ratio'] * df['avs_mismatch_ratio'] * df['exp_mismatch_ratio'] * df['new_merch_ratio']
df['geo_mcc_anomaly_score'] = df['cross_state_anomaly'] * df['high_risk_mcc_ratio']

# Save the updated dataset
df.save("path_to_updated_dataset.csv")





# Create interaction features
df['zero_auth_to_auth_ratio'] = df['tot_zero_auth_cnt_24hr'] / df['tot_cnt_24hr']
df['small_amt_testing_score'] = df['cnt_sameamt_auth_6h'] / df['tot_cnt_24hr']
df['cvv_mismatch_after_zero_auth'] = df['cnt_cvv2_mismatch_24hr'] / df['tot_zero_auth_cnt_24hr']
df['trxn_burst_score'] = df['tot_cnt_30min'] / df['tot_cnt_24hr']
df['same_merc_velocity_anomaly'] = df['same_merc_cnt_30min'] / df['same_merc_cnt_24hr']
df['mcc_velocity_anomaly'] = df['same_mcc_cnt_30min'] / df['same_mcc_cnt_24hr']
df['dist_to_last_trxn_anomaly'] = df['dist_miles_btw_swp'] / df['avg_dist_btw_swp_30days']
df['cross_country_ratio'] = df['cnt_cntry_24hr'] / df['tot_cnt_24hr']
df['time_btw_geo_hops'] = df['time_btw_swp'] / df['dist_miles_btw_swp']
df['same_merc_to_term_ratio'] = df['cnt_same_mid_termid_24hr'] / df['same_merc_cnt_24hr']
df['new_merc_to_term_ratio'] = df['new_merch_id'] / df['cnt_same_mid_termid_24hr']
df['high_risk_merc_cluster'] = (df['same_merc_cnt_24hr'] / df['tot_cnt_24hr']) * df['high_risk_mcc_ratio']
df['fraud_risk_score_3'] = df['zero_auth_to_auth_ratio'] * df['small_amt_testing_score'] * df['cvv_mismatch_after_zero_auth'] * df['trxn_burst_score']
df['geo_velocity_anomaly_score'] = df['dist_to_last_trxn_anomaly'] * df['trxn_burst_score']

# Save the updated dataset
df.save("path_to_updated_dataset.csv")





# Create interaction features
df['swipe_amt_deviation'] = (df['amt_swipe_24hr'] - df['avg_swipe_amt_24hr']) / df['std_swipe_amt_24hr']
df['trxn_hour_anomaly'] = (df['trxn_hour'] - df['avg_trxn_hour_30days']) / df['std_trxn_hour_30days']
df['swipe_cnp_ratio_deviation'] = (df['swipe_cnp_ratio'] - df['avg_swipe_cnp_ratio_30days']) / df['std_swipe_cnp_ratio_30days']
df['high_risk_mcc_bayesian_interaction'] = df['hct_term_mcc_bayesian_encoded_5'] * df['hct_term_mcc_bayesian_encoded_7']
df['new_merch_risk_score'] = df['new_mech_ind'] * (1 + df['cnt_same_merc_id_24hr'] / df['tot_cnt_24hr'])
df['mcc_risk_weighted_cnt'] = df['same_mcc_cnt_24hr'] * df['hct_term_mcc_bayesian_encoded_5']
df['geo_dispersion_score'] = (df['cnt_dom_states_24hr'] / df['tot_cnt_24hr']) * df['dist_miles_btw_swp']
df['time_btw_swp_cvv_mismatch'] = df['time_since_last_swipe'] / df['time_since_cvv2_mismatch']
df['foreign_trxn_risk_score'] = df['foreign_ind'] * (1 + df['cnt_fore_swipe_24hr'] / df['tot_cnt_24hr'])
df['cvv_verf_magn_interaction'] = df['cvv_verf_magn_ind'] * (1 + df['cnt_cvv2_mismatch_24hr'] / df['tot_cnt_24hr'])
df['avs_mismatch_risk_score'] = df['avs_addr_mismatch_ind'] * (1 + df['cnt_avs_mismatch_24hr'] / df['tot_cnt_24hr'])
df['exp_mismatch_risk_score'] = df['exp_date_mismatch_ind'] * (1 + df['cnt_exp_date_mismatch_24hr'] / df['tot_cnt_24hr'])
df['fraud_risk_score_4'] = df['swipe_amt_deviation'] * df['trxn_hour_anomaly'] * df['high_risk_mcc_bayesian_interaction'] * df['new_merch_risk_score']
df['geo_temp_anomaly_score_2'] = df['geo_dispersion_score'] * df['time_btw_swp_cvv_mismatch']

# Save the updated dataset
df.save("path_to_updated_dataset.csv")










GPT


import h2o
import h2o.frame
from h2o.frame import H2OFrame

# Initialize H2O
h2o.init()

# Load dataset
df = h2o.import_file("your_fraud_data.csv")

# Creating advanced interaction features
df["risk_weighted_swipe_intensity"] = (df["cnt_swipe_24hr"] * (1 + df["cvv_verf_magn_ind"])) / (df["cnt_swipe_60days"] + 1)
df["mcc_velocity_risk"] = df["same_mcc_cnt_24hr"] / (df["same_mcc_cnt_60days"] + 1)
df["terminal_deviation"] = (df["cnt_same_mid_termid_24hr"] - df["cnt_same_mid_termid_60days"]) / (df["cnt_same_mid_termid_60days"] + 1)
df["swipe_dist_time_anomaly"] = df["dist_miles_btw_swp"] / (df["time_btw_swp"] + 1)
df["fraud_amt_scaling"] = (df["amt_swipe_24hr"] / (df["amt_swipe_60days"] + 1)) * (1 / (df["pct_swipe_amt_60days"] + 0.01))
df["avs_exp_mismatch_score"] = (df["cnt_avs_mismatch_24hr"] + df["cnt_exp_date_mismatch_24hr"]) / (df["cnt_avs_mismatch_60days"] + df["cnt_exp_date_mismatch_60days"] + 1)
df["card_testing_ratio"] = df["cnt_zero_auth_24hr"] / (df["cnt_zero_auth_60days"] + 1)
df["high_risk_merchant_score"] = df["cnt_5310_5411_24hr"] / (df["cnt_5310_5411_60days"] + 1)

# Save the enhanced dataset
h2o.export_file(df, "fraud_data_with_interaction_features.csv")













