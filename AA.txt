import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error

# -----------------------------
# Weighted alpha helper
# -----------------------------
def compute_weighted_alpha_from_base(headcount_array, alpha_base, alpha_min=0.02, alpha_max=0.9, power=1.0):
    hc = np.array(headcount_array, dtype=float)
    # protect zero mean
    mean_hc = hc.mean() if hc.mean() != 0 else 1.0
    hc_norm = (hc / mean_hc) ** power
    alphas = alpha_base * hc_norm
    return np.clip(alphas, alpha_min, alpha_max)

# -----------------------------
# EMA & DEMA (weighted alphas)
# -----------------------------
def weighted_ema(values, alphas):
    ema = np.zeros_like(values, dtype=float)
    ema[0] = values[0]
    for t in range(1, len(values)):
        a = alphas[t]
        ema[t] = a * values[t] + (1 - a) * ema[t-1]
    return ema

def weighted_dema(values, alphas):
    e1 = weighted_ema(values, alphas)
    e2 = weighted_ema(e1, alphas)
    return 2 * e1 - e2

# -----------------------------
# Quarter-wise alpha search
# -----------------------------
def find_best_alpha_per_quarter(df,
                                value_col='VOLUNTARY_EXITS',
                                hc_col='HEADCOUNT',
                                date_col='MONTH',
                                candidate_Ns=None,
                                alpha_min=0.02,
                                alpha_max=0.9,
                                power=1.0,
                                min_train_periods=24):
    """
    For each quarter (1..4), run expanding-window CV only on months that belong to that quarter
    and select the best alpha_base (mapped from N -> alpha_base = 2/(N+1)).
    Returns dict: quarter -> {best_N, best_alpha_base, error_by_N}
    """
    if candidate_Ns is None:
        candidate_Ns = list(range(2, 13))   # map to alpha_base = 2/(N+1) for N=2..12

    df = df.copy()
    df[date_col] = pd.to_datetime(df[date_col])
    df = df.sort_values(date_col).reset_index(drop=True)
    df['quarter'] = df[date_col].dt.quarter

    y = df[value_col].values.astype(float)
    hc = df[hc_col].values.astype(float)
    months = df[date_col]

    results = {}

    n = len(df)
    # We'll perform an expanding-window CV; for each test index t where t >= min_train_periods and we evaluate forecasting t
    test_indices = list(range(min_train_periods, n))

    for q in [1,2,3,4]:
        errors_by_candidate = {}
        # find all test indices that correspond to quarter q
        test_idx_q = [t for t in test_indices if months.iloc[t].quarter == q]
        if len(test_idx_q) == 0:
            # Not enough history for this quarter
            results[q] = {'best_N': None, 'best_alpha_base': None, 'errors': {}}
            continue

        for N in candidate_Ns:
            alpha_base = 2.0 / (N + 1.0)  # classic mapping
            errs = []
            for t in test_idx_q:
                # train on 0..t-1, predict t using flat DEMA forecast (last dema)
                train_slice = slice(0, t)
                y_train = y[train_slice]
                hc_train = hc[train_slice]

                # need at least 2 points for DEMA
                if len(y_train) < 3:
                    continue

                alphas_train = compute_weighted_alpha_from_base(hc_train, alpha_base,
                                                                alpha_min=alpha_min, alpha_max=alpha_max,
                                                                power=power)
                dema_train = weighted_dema(y_train, alphas_train)
                # forecast for t (flat DEMA forecast) = last dema value
                yhat = dema_train[-1]
                errs.append(abs(y[t] - yhat))

            if len(errs) == 0:
                # no folds for this candidate (rare)
                mean_err = np.nan
            else:
                mean_err = float(np.mean(errs))
            errors_by_candidate[N] = mean_err

        # pick best N (min error), ignore NaNs
        valid = {N: e for N, e in errors_by_candidate.items() if not np.isnan(e)}
        if len(valid) == 0:
            results[q] = {'best_N': None, 'best_alpha_base': None, 'errors': errors_by_candidate}
            continue

        best_N = min(valid.keys(), key=lambda k: valid[k])
        best_alpha = 2.0 / (best_N + 1.0)
        results[q] = {'best_N': best_N, 'best_alpha_base': best_alpha, 'errors': errors_by_candidate}

    return results

# -----------------------------
# Forecast using quarter-wise alpha_base
# -----------------------------
def forecast_with_quarter_alphas(df, quarter_alpha_map, N_forecast=3,
                                 alpha_min=0.02, alpha_max=0.9, power=1.0,
                                 trend_extension=False):
    """
    quarter_alpha_map: dict mapping quarter -> alpha_base (float)
    N_forecast: number of months to forecast out
    trend_extension: if True, use DEMA trend extension F_{t+k} = L + k*T (where T = last - prev)
    """
    df = df.copy()
    df['MONTH'] = pd.to_datetime(df['MONTH'])
    df = df.sort_values('MONTH').reset_index(drop=True)

    y = df['VOLUNTARY_EXITS'].values.astype(float)
    hc = df['HEADCOUNT'].values.astype(float)
    last_date = df['MONTH'].iloc[-1]

    # Build alphas for historical data using quarter-specific alpha_base
    quarters = df['MONTH'].dt.quarter.values
    alphas_hist = np.zeros_like(y, dtype=float)
    for i, q in enumerate(quarters):
        alpha_base = quarter_alpha_map.get(q, None)
        if alpha_base is None:
            # fall back to a reasonable default
            alpha_base = 2.0 / (5 + 1.0)  # equivalent to N=5
        # per-period alpha uses the historical HC array mean for stability
        # We'll compute normalization using whole-historical headcount mean
        # (this mirrors compute_weighted_alpha_from_base behavior)
        # compute per-row alpha after loop once mean is known
        alphas_hist[i] = alpha_base  # placeholder; we'll normalize below

    # Now compute normalized alpha using the same headcount normalization as function
    mean_hc = hc.mean() if hc.mean() != 0 else 1.0
    # compute per-row alpha = alpha_base * (hc/mean_hc)**power clipped
    for i in range(len(y)):
        base = alphas_hist[i]
        a = base * (hc[i] / mean_hc) ** power
        a = max(alpha_min, min(alpha_max, a))
        alphas_hist[i] = a

    # Compute W-DEMA on historical data
    dema_hist = weighted_dema(y, alphas_hist)

    # Forecast months one-by-one using quarter-wise alpha_base (for the future months we need to decide alpha per future month)
    forecasts = []
    forecast_months = []
    L = dema_hist[-1]
    T = dema_hist[-1] - dema_hist[-2] if len(dema_hist) >= 2 else 0.0

    # For future months we need a headcount input ‚Äî we will use last known headcount as proxy,
    # or you can pass a headcount_forecast list to this function (extension).
    last_hc = hc[-1]
    for k in range(1, N_forecast + 1):
        f_month = last_date + pd.DateOffset(months=k)
        q = f_month.quarter
        alpha_base = quarter_alpha_map.get(q, 2.0 / (5 + 1.0))

        # compute alpha for the forecast period using last_hc (proxy)
        a_future = alpha_base * (last_hc / mean_hc) ** power
        a_future = max(alpha_min, min(alpha_max, a_future))

        if trend_extension:
            yhat = L + k * T
        else:
            # flat DEMA forecast (classic) -- though alpha is computed for future month, DEMA flat forecast equals last dema
            yhat = L

        # Optionally, you could apply a small adjustment if you want the alpha to affect multi-step forecast;
        # but classic DEMA-flat forecast is yhat = L. We'll keep that but return the alpha used for info.
        forecasts.append({'month': f_month, 'forecast': yhat, 'alpha_used': a_future, 'quarter': q})
        forecast_months.append(f_month)

    return pd.DataFrame(forecasts)


# -----------------------------
# Example usage
# -----------------------------
if __name__ == "__main__":
    # load your data
    df = pd.read_excel("Projections Analysis Oct 2025.xlsx")   # must have MONTH, HEADCOUNT, VOLUNTARY_EXITS

    # find best alpha per quarter
    res = find_best_alpha_per_quarter(df, candidate_Ns=range(2,13), alpha_min=0.02, alpha_max=0.9, power=1.0,
                                      min_train_periods=24)
    print("Quarter-wise alpha search results:")
    for q, info in res.items():
        print(q, info)

    # build a quarter -> alpha_base map from results (fallback to N=5 if missing)
    quarter_alpha_map = {}
    for q, info in res.items():
        if info['best_N'] is None:
            quarter_alpha_map[q] = 2.0 / (5 + 1.0)
        else:
            quarter_alpha_map[q] = info['best_alpha_base']

    # Forecast using quarter-wise alphas
    forecast_df = forecast_with_quarter_alphas(df, quarter_alpha_map, N_forecast=6, trend_extension=True)
    print("\nForecasts:")
    print(forecast_df)























import pandas as pd
import numpy as np

# ==========================================
#   CORE: EMA + DEMA
# ==========================================

def ema(values, alphas):
    ema_vals = np.zeros_like(values, dtype=float)
    ema_vals[0] = values[0]
    for t in range(1, len(values)):
        ema_vals[t] = alphas[t] * values[t] + (1 - alphas[t]) * ema_vals[t - 1]
    return ema_vals


def dema(values, alphas):
    ema1 = ema(values, alphas)
    ema2 = ema(ema1, alphas)
    return 2 * ema1 - ema2


# ==========================================
#   A. FIXED ALPHA DEMA
# ==========================================

def fixed_alpha_dema(values, alpha=0.5):
    alphas = np.full(len(values), alpha)
    return dema(values, alphas)


# ==========================================
#   B. WEIGHTED DEMA (HEADCOUNT)
# ==========================================

def weighted_alpha(headcount, alpha_base=0.3, alpha_min=0.05, alpha_max=0.8, power=1.0):
    hc = np.array(headcount, dtype=float)
    hc_norm = (hc / hc.mean()) ** power
    alphas = alpha_base * hc_norm
    return np.clip(alphas, alpha_min, alpha_max)


def weighted_dema(values, headcount, alpha_base=0.3, alpha_min=0.05, alpha_max=0.8, power=1.0):
    alphas = weighted_alpha(headcount, alpha_base, alpha_min, alpha_max, power)
    return dema(values, alphas)


# ==========================================
#   C. STRICT 3-PERIOD DEMA
# ==========================================
# Œ± = 2 / (3 + 1) = 0.5

def dema_3period(values):
    alphas = np.full(len(values), 0.5)
    return dema(values, alphas)


# ==========================================
#   D. TREND-BASED FORECASTING
# ==========================================

def dema_forecast_trend(dema_series, N):
    L = dema_series[-1]          # Last level
    T = dema_series[-1] - dema_series[-2]   # Trend from last two values
    return np.array([L + k * T for k in range(1, N + 1)])


# ==========================================
#   E. SEASONALITY-ADJUSTED DEMA
# ==========================================

def compute_monthly_seasonality(df, actual_col, dema_col):
    df = df.copy()
    df["month"] = df["MONTH"].dt.month
    df["ratio"] = df[actual_col] / df[dema_col]

    season = df.groupby("month")["ratio"].mean().to_dict()

    # normalize to avoid level drifting
    avg = np.mean(list(season.values()))
    season = {m: v / avg for m, v in season.items()}
    return season


def dema_forecast_seasonal(dema_series, seasonality, last_date, N):
    forecasts = []
    months = []
    L = dema_series[-1]

    for k in range(1, N+1):
        next_month = last_date + pd.DateOffset(months=k)
        m = next_month.month
        S = seasonality.get(m, 1.0)
        forecasts.append(L * S)
        months.append(next_month)

    return months, forecasts


# ==========================================
# ALL-IN-ONE FORECAST WRAPPER
# ==========================================

def forecast_dema(df, method="weighted", N=3):
    """
    method options:
        'fixed'
        'weighted'
        '3period'
        'fixed_trend'
        'weighted_trend'
        '3period_trend'
        'seasonal_weighted'
    """

    df = df.copy()
    df["MONTH"] = pd.to_datetime(df["MONTH"])
    df = df.sort_values("MONTH")

    y = df["VOLUNTARY_EXITS"].astype(float).values
    hc = df["HEADCOUNT"].astype(float).values

    # --------------- Choose DEMA Method ---------------
    if method == "fixed":
        d = fixed_alpha_dema(y, alpha=0.3)

    elif method == "weighted":
        d = weighted_dema(y, hc)

    elif method == "3period":
        d = dema_3period(y)

    elif method.endswith("_trend"):
        if method.startswith("fixed"):
            d = fixed_alpha_dema(y, alpha=0.3)
        elif method.startswith("weighted"):
            d = weighted_dema(y, hc)
        elif method.startswith("3period"):
            d = dema_3period(y)
        forecast = dema_forecast_trend(d, N)
        return forecast, d

    elif method == "seasonal_weighted":
        d = weighted_dema(y, hc)
        season = compute_monthly_seasonality(df, "VOLUNTARY_EXITS", "DEMA_TMP" if "DEMA_TMP" in df else "VOLUNTARY_EXITS")
        months, f = dema_forecast_seasonal(d, season, df["MONTH"].iloc[-1], N)
        return f, months, d

    else:
        raise ValueError("Invalid method selected.")

    # --------------- Simple Flat Forecast ---------------
    forecast = np.full(N, d[-1])
    return forecast, d


# ==========================================
#           EXAMPLE USAGE
# ==========================================

if __name__ == "__main__":
    df = pd.read_excel("Projections Analysis Oct 2025.xlsx")

    # Try multiple methods
    methods = [
        "3period",
        "fixed",
        "weighted",
        "fixed_trend",
        "weighted_trend",
        "3period_trend",
    ]

    for m in methods:
        print(f"\n=== METHOD: {m} ===")
        f, d = forecast_dema(df, method=m, N=6)
        print("Forecast next 6 months:", f)

















# ===========================
# Trend-based DEMA Forecast
# ===========================

N = 6   # number of future months you want

wdema_last = df['WDEMA'].iloc[-1]
wdema_prev = df['WDEMA'].iloc[-2]

# Trend = difference of last two WDEMA values
trend = wdema_last - wdema_prev

# Forecast k steps ahead: WDEMA_t + k * trend
future_forecasts = [wdema_last + k * trend for k in range(1, N + 1)]

print(f"\nNext {N} Months Forecast (Weighted DEMA with Trend):")
for i, f in enumerate(future_forecasts, start=1):
    print(f"Month +{i}: {f}")








import pandas as pd
import numpy as np

def compute_weighted_alpha(headcount_series, alpha_base=0.3, alpha_min=0.05, alpha_max=0.8):
    hc = headcount_series.values.astype(float)
    hc_norm = hc / hc.mean()
    alphas = alpha_base * hc_norm
    return np.clip(alphas, alpha_min, alpha_max)

def weighted_ema(values, alphas):
    ema = np.zeros_like(values, dtype=float)
    ema[0] = values[0]
    for t in range(1, len(values)):
        ema[t] = alphas[t] * values[t] + (1 - alphas[t]) * ema[t - 1]
    return ema

def weighted_dema(values, alphas):
    ema1 = weighted_ema(values, alphas)
    ema2 = weighted_ema(ema1, alphas)
    return 2 * ema1 - ema2


# ============================
# Seasonal Index Computation
# ============================
def compute_monthly_seasonality(df, value_col, wdema_col):
    df['month_num'] = df['MONTH'].dt.month
    df['ratio'] = df[value_col] / df[wdema_col]

    # Seasonal index = avg(actual/WDEMA) per month
    seasonality = df.groupby('month_num')['ratio'].mean().to_dict()

    # Normalize to avoid drift
    mean_val = np.mean(list(seasonality.values()))
    seasonality = {m: v / mean_val for m, v in seasonality.items()}

    return seasonality


# ============================
# Forecast Next N Periods
# ============================
def forecast_wdema_next_n(df, N):
    df = df.copy()
    df['MONTH'] = pd.to_datetime(df['MONTH'])
    df = df.sort_values("MONTH")

    y = df['VOLUNTARY_EXITS'].values.astype(float)
    hc = df['HEADCOUNT']

    # Step 1 ‚Üí WDEMA
    alphas = compute_weighted_alpha(hc)
    wd = weighted_dema(y, alphas)
    df['WDEMA'] = wd

    # Step 2 ‚Üí Seasonal factors
    season = compute_monthly_seasonality(df, value_col='VOLUNTARY_EXITS', wdema_col='WDEMA')

    # Step 3 ‚Üí Trend
    L = wd[-1]
    T = wd[-1] - wd[-2]

    last_month = df['MONTH'].iloc[-1]

    forecasts = []
    months = []

    for k in range(1, N + 1):
        next_month = last_month + pd.DateOffset(months=k)
        m = next_month.month
        S = season.get(m, 1)

        # Final adjusted forecast
        f = (L + k * T) * S

        forecasts.append(f)
        months.append(next_month)

    result = pd.DataFrame({
        "Forecast_Month": months,
        "Forecast_Value": forecasts
    })

    return result, df


# ============================
# Example Usage
# ============================
df = pd.read_excel("Projections Analysis Oct 2025.xlsx")

forecast_df, full_df = forecast_wdema_next_n(df, N=6)

print(forecast_df)





















import pandas as pd
import numpy as np

# ===========================
# Weighted DEMA Implementation
# ===========================

def compute_weighted_alpha(headcount_series, alpha_base=0.3, alpha_min=0.05, alpha_max=0.8):
    hc = headcount_series.values.astype(float)
    hc_norm = hc / hc.mean()

    alphas = alpha_base * hc_norm
    alphas = np.clip(alphas, alpha_min, alpha_max)

    return alphas


def weighted_ema(values, alphas):
    ema = np.zeros_like(values, dtype=float)
    ema[0] = values[0]
    for t in range(1, len(values)):
        ema[t] = alphas[t] * values[t] + (1 - alphas[t]) * ema[t - 1]
    return ema


def weighted_dema(values, alphas):
    ema1 = weighted_ema(values, alphas)
    ema2 = weighted_ema(ema1, alphas)
    return 2 * ema1 - ema2


# ===========================
# Dynamic Forecast Function
# ===========================
def forecast_wdema_next_n(df, N):
    """
    df must contain columns: MONTH, HEADCOUNT, VOLUNTARY_EXITS
    Returns array of N-step ahead forecasts using WDEMA level + trend.
    """
    
    # Sort dataset
    df = df.copy()
    df['MONTH'] = pd.to_datetime(df['MONTH'])
    df = df.sort_values('MONTH').reset_index(drop=True)

    y = df['VOLUNTARY_EXITS'].values.astype(float)
    hc = df['HEADCOUNT']

    # Compute alphas and WDEMA
    alphas = compute_weighted_alpha(hc, alpha_base=0.3)
    wd = weighted_dema(y, alphas)

    # Last level
    L = wd[-1]

    # Trend estimated from last 2 values
    T = wd[-1] - wd[-2]

    # Dynamic next N predictions
    forecasts = np.array([L + k * T for k in range(1, N + 1)], dtype=float)

    return forecasts, wd


# ===========================
# Example Usage
# ===========================
if __name__ == "__main__":
    df = pd.read_excel("Projections Analysis Oct 2025.xlsx")

    N = 3   # you can change to any value: 1, 3, 6, 12

    next_forecasts, wdema_series = forecast_wdema_next_n(df, N)

    print(f"\nNext {N} Month Forecast:")
    for i, val in enumerate(next_forecasts, 1):
        print(f"Month +{i}: {val}")

    # optional: append to dataframe
    df['WDEMA'] = wdema_series
    print("\nWeighted DEMA (last few months):")
    print(df[['MONTH', 'VOLUNTARY_EXITS', 'HEADCOUNT', 'WDEMA']].tail())






























import pandas as pd
import numpy as np

# ===========================
# Weighted DEMA Implementation
# ===========================

def compute_weighted_alpha(headcount_series, alpha_base=0.3, alpha_min=0.05, alpha_max=0.8):
    hc = headcount_series.values.astype(float)
    hc_norm = hc / hc.mean()

    alphas = alpha_base * hc_norm
    alphas = np.clip(alphas, alpha_min, alpha_max)

    return alphas


def weighted_ema(values, alphas):
    ema = np.zeros_like(values, dtype=float)
    ema[0] = values[0]
    for t in range(1, len(values)):
        ema[t] = alphas[t] * values[t] + (1 - alphas[t]) * ema[t - 1]
    return ema


def weighted_dema(values, alphas):
    ema1 = weighted_ema(values, alphas)
    ema2 = weighted_ema(ema1, alphas)
    dema = 2 * ema1 - ema2
    return dema


# ================================
# Load and process your dataset
# ================================
# Your columns from screenshot:
# MONTH (YYYY-MM), HEADCOUNT, VOLUNTARY_EXITS, MONTHLY_ATTRITION_RATE

df = pd.read_excel("Projections Analysis Oct 2025.xlsx")

# Sort by month (required)
df['MONTH'] = pd.to_datetime(df['MONTH'])
df = df.sort_values('MONTH').reset_index(drop=True)

# Target series = VOLUNTARY_EXITS (counts)
y = df['VOLUNTARY_EXITS'].values.astype(float)

# Compute time-varying alphas via headcount weights
alphas = compute_weighted_alpha(df['HEADCOUNT'], alpha_base=0.3)

# Compute Weighted DEMA
df['WDEMA'] = weighted_dema(y, alphas)

print(df[['MONTH', 'VOLUNTARY_EXITS', 'HEADCOUNT', 'WDEMA']].tail(12))

# Next N months forecast (simple method = last WDEMA as forward projection)
next_month_forecast = df['WDEMA'].iloc[-1]
print("\nNext Month Forecast (Weighted DEMA):", next_month_forecast)




# Compute simple trend on last 12 months of WDEMA
last_12 = df['WDEMA'].tail(12).reset_index(drop=True)

x = np.arange(len(last_12))
coef = np.polyfit(x, last_12, 1)  # linear trend
slope = coef[0]
intercept = coef[1]

# Forecast next 3 months with trend
forecast_1 = intercept + slope * (len(last_12))
forecast_2 = intercept + slope * (len(last_12) + 1)
forecast_3 = intercept + slope * (len(last_12) + 2)

print("Next 3-month Forecast with Trend:")
print("Month +1:", forecast_1)
print("Month +2:", forecast_2)
print("Month +3:", forecast_3)



# True DEMA-based trend extension
# L_t is last WDEMA
L_t = df['WDEMA'].iloc[-1]

# Trend estimate = difference between last two WDEMA values
T_t = df['WDEMA'].iloc[-1] - df['WDEMA'].iloc[-2]

# Forecast k steps ahead
forecast_1 = L_t + 1 * T_t
forecast_2 = L_t + 2 * T_t
forecast_3 = L_t + 3 * T_t

print("Next 3-month Forecast (DEMA level + trend):")
print("Month +1:", forecast_1)
print("Month +2:", forecast_2)
print("Month +3:", forecast_3)


















import pandas as pd
from statsmodels.tsa.holtwinters import ExponentialSmoothing

# ============================
# STEP 1 ‚Äî Load Dataset
# ============================
# If reading from Excel:
# df = pd.read_excel("Projections Analysis Oct 2025.xlsx")

# For now using a dummy sample matching your screenshot:
data = {
    "MONTH": [
        "2020-12", "2021-01", "2021-02", "2021-03", "2021-04", "2021-05", "2021-06",
        "2021-07", "2021-08", "2021-09", "2021-10", "2021-11", "2021-12",
        "2022-01", "2022-02", "2022-03", "2022-04", "2022-05", "2022-06"
    ],
    "HEADCOUNT": [
        210007, 210404, 210198, 210660, 211687, 213377, 217431, 218203, 217892,
        218454, 221995, 222818, 223387, 225766, 226836, 228888, 229702, 230604,
        231349
    ],
    "MONTHLY_ATTRITION_RATE": [
        0.760927017,0.5755594,0.746430972,0.8570696,0.989196331,
        0.984202127,1.118049431,1.182780847,1.272007782,
        1.198398526,0.98745097,0.967157052,1.0666857,
        0.839807588,0.993225882,1.2254488,1.170898393,
        1.237158825,1.269078319
    ]
}

df = pd.DataFrame(data)
df["MONTH"] = pd.to_datetime(df["MONTH"])
df = df.set_index("MONTH")

# ============================
# STEP 2 ‚Äî Choose series for Holt-Winters
# ============================
ts = df["MONTHLY_ATTRITION_RATE"]

# ============================
# STEP 3 ‚Äî Build Holt-Winters Model
# ============================
# Monthly data ‚Üí seasonal_periods = 12
model = ExponentialSmoothing(
    ts,
    trend="add",
    seasonal="add",
    seasonal_periods=12
)

fit_model = model.fit(optimized=True)

print("Model summary:")
print(fit_model.summary())

# ============================
# STEP 4 ‚Äî Forecast N months
# ============================
N = 6  # forecast next 6 months
forecast_attrition = fit_model.forecast(N)

# ============================
# STEP 5 ‚Äî Convert Attrition ‚Üí Voluntary Exits
# ============================
# Define dynamic base headcount
BASE_HEADCOUNT = 232000    # You can change dynamically

forecast_vol_exits = forecast_attrition * BASE_HEADCOUNT

# ============================
# STEP 6 ‚Äî Prepare final output table
# ============================
future_dates = pd.date_range(df.index[-1] + pd.offsets.MonthBegin(),
                             periods=N, freq="MS")

forecast_df = pd.DataFrame({
    "FORECAST_MONTH": future_dates,
    "FORECAST_ATTRITION_RATE": forecast_attrition.values,
    "BASE_HEADCOUNT_USED": BASE_HEADCOUNT,
    "FORECAST_VOLUNTARY_EXITS": forecast_vol_exits.round(0).astype(int)
})

print("\n=== FINAL FORECAST OUTPUT ===")
print(forecast_df)





















import pandas as pd
from difflib import get_close_matches

# === Step 1. Read all sheets ===
file_path = "TopDataDict_WFA_APP_USR_TALD_test.xlsx"
xls = pd.ExcelFile(file_path)

# The first sheet is the main sheet
main_sheet_name = xls.sheet_names[0]
main_df = pd.read_excel(xls, main_sheet_name)

# === Step 2. Find tables assigned to 'Mayur' ===
mayur_tables = main_df.loc[main_df['Assigned to'] == 'Mayur', 'L4_TABLE_LIST'].dropna().tolist()
print("Tables assigned to Mayur:", mayur_tables)

# === Step 3. Build a global COLUMN_NAME ‚Üí DESCRIPTION mapping ===
global_desc_map = {}

for sheet in xls.sheet_names[1:]:  # skip main sheet
    df = pd.read_excel(xls, sheet)
    if {'COLUMN_NAME', 'DESCRIPTION'}.issubset(df.columns):
        for _, row in df[['COLUMN_NAME', 'DESCRIPTION']].dropna().iterrows():
            col_name = str(row['COLUMN_NAME']).strip()
            desc = str(row['DESCRIPTION']).strip()
            if col_name and desc:
                global_desc_map.setdefault(col_name, desc)

print(f"Total unique columns found for mapping: {len(global_desc_map)}")

# === Step 4. Update missing descriptions + create AI-suggested column ===
updated_tables = {}

for table_name in mayur_tables:
    if table_name in xls.sheet_names:
        df = pd.read_excel(xls, table_name)

        if {'COLUMN_NAME', 'DESCRIPTION'}.issubset(df.columns):

            # Fill missing descriptions from global map
            df['DESCRIPTION'] = df.apply(
                lambda row: global_desc_map.get(str(row['COLUMN_NAME']).strip(), row['DESCRIPTION'])
                if pd.isna(row['DESCRIPTION']) or row['DESCRIPTION'] in [None, '', 'nan']
                else row['DESCRIPTION'],
                axis=1
            )

            # === Step 4a. Add AI-suggested descriptions for remaining missing ===
            suggested_descriptions = []
            all_known_columns = list(global_desc_map.keys())

            for _, row in df.iterrows():
                col = str(row['COLUMN_NAME']).strip()
                desc = str(row['DESCRIPTION']).strip() if not pd.isna(row['DESCRIPTION']) else ''
                
                if desc:  # Already has a description
                    suggested_descriptions.append('')
                else:
                    # Find similar column names
                    matches = get_close_matches(col, all_known_columns, n=1, cutoff=0.7)
                    if matches:
                        matched_col = matches[0]
                        suggested_desc = global_desc_map[matched_col]
                        suggested_descriptions.append(f"{suggested_desc} (suggested from {matched_col})")
                    else:
                        suggested_descriptions.append('')

            df['AI_SUGGESTED_DESCRIPTION'] = suggested_descriptions

            updated_tables[table_name] = df
            print(f"‚úÖ Updated {table_name} with filled and suggested descriptions.")

        else:
            print(f"‚ö†Ô∏è Skipping {table_name} ‚Äî required columns not found.")
    else:
        print(f"‚ö†Ô∏è Sheet {table_name} not found in workbook.")

# === Step 5. Save only Mayur‚Äôs updated tables ===
output_file = "Mayur_Tables_Updated_AI_Suggested.xlsx"
with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:
    for table_name, df in updated_tables.items():
        df.to_excel(writer, sheet_name=table_name, index=False)

print(f"\n‚úÖ Done! Output file created: {output_file}")




























import pandas as pd

# === Step 1. Read all sheets ===
file_path = "TopDataDict_WFA_APP_USR_TALD_test.xlsx"
xls = pd.ExcelFile(file_path)

# The first sheet is the main sheet
main_sheet_name = xls.sheet_names[0]
main_df = pd.read_excel(xls, main_sheet_name)

# === Step 2. Find tables assigned to 'Mayur' ===
mayur_tables = main_df.loc[main_df['Assigned to'] == 'Mayur', 'L4_TABLE_LIST'].dropna().tolist()
print("Tables assigned to Mayur:", mayur_tables)

# === Step 3. Build a global COLUMN_NAME ‚Üí DESCRIPTION mapping from all other sheets ===
global_desc_map = {}

for sheet in xls.sheet_names[1:]:  # skip main sheet
    df = pd.read_excel(xls, sheet)
    # ensure relevant columns exist
    if {'COLUMN_NAME', 'DESCRIPTION'}.issubset(df.columns):
        for _, row in df[['COLUMN_NAME', 'DESCRIPTION']].dropna().iterrows():
            col_name = str(row['COLUMN_NAME']).strip()
            desc = str(row['DESCRIPTION']).strip()
            if col_name and desc:
                # Prefer first non-empty description encountered
                global_desc_map.setdefault(col_name, desc)

print(f"Total unique columns found for mapping: {len(global_desc_map)}")

# === Step 4. Update missing descriptions in Mayur‚Äôs tables ===
updated_tables = {}

for table_name in mayur_tables:
    if table_name in xls.sheet_names:
        df = pd.read_excel(xls, table_name)

        if {'COLUMN_NAME', 'DESCRIPTION'}.issubset(df.columns):
            df['DESCRIPTION'] = df.apply(
                lambda row: global_desc_map.get(str(row['COLUMN_NAME']).strip(), row['DESCRIPTION'])
                if pd.isna(row['DESCRIPTION']) or row['DESCRIPTION'] in [None, '', 'nan']
                else row['DESCRIPTION'],
                axis=1
            )

            updated_tables[table_name] = df
            print(f"‚úÖ Updated {table_name} with descriptions from global map.")
        else:
            print(f"‚ö†Ô∏è Skipping {table_name} ‚Äî required columns not found.")
    else:
        print(f"‚ö†Ô∏è Sheet {table_name} not found in workbook.")

# === Step 5. Create new Excel with only Mayur‚Äôs updated tables ===
output_file = "Mayur_Tables_Updated.xlsx"
with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:
    for table_name, df in updated_tables.items():
        df.to_excel(writer, sheet_name=table_name, index=False)

print(f"\n‚úÖ Done! Output file created: {output_file}")


























# -----------------------
# PARAMETERS
# -----------------------
HC_REFERENCE_MONTH = "2024-12"  # <-- You can change this month as per your reference
# -----------------------

# Convert to datetime for comparison
hc_ref_dt = pd.to_datetime(HC_REFERENCE_MONTH + "-01")

# last known headcount from specific reference month (fallbacks handled)
pre_forecast_hc_series = grp[grp.index == hc_ref_dt]['HEADCOUNT']

if not pre_forecast_hc_series.empty:
    # Use HC from the reference month directly
    last_known_hc = pre_forecast_hc_series.iloc[-1]
else:
    # Fallback to last available before forecast start
    pre_forecast_hc_series = grp[grp.index < pd.to_datetime(FORECAST_START)]['HEADCOUNT']
    if not pre_forecast_hc_series.empty:
        last_known_hc = pre_forecast_hc_series.iloc[-1]
    else:
        # Final fallback to latest known in dataset
        last_known_hc = grp['HEADCOUNT'].iloc[-1] if (not grp['HEADCOUNT'].empty) else BASE_MONTH_HC


















import pandas as pd
import numpy as np

# Step 1: Replace None with NaN for consistency
df1 = df1.replace({None: np.nan})
df2 = df2.replace({None: np.nan})

# Step 2: Sort by all columns (so identical rows align)
df1_sorted = df1.sort_values(by=list(df1.columns)).reset_index(drop=True)
df2_sorted = df2.sort_values(by=list(df2.columns)).reset_index(drop=True)

# Step 3: Ensure same columns and shape
assert list(df1_sorted.columns) == list(df2_sorted.columns), "‚ùå Column names differ!"
assert df1_sorted.shape == df2_sorted.shape, "‚ùå Row counts differ!"

# Step 4: Normalize values ‚Äî handle numeric equivalence (-64.0 == -64)
def normalize_value(v):
    if pd.isna(v):
        return np.nan
    # Convert strings that look numeric
    if isinstance(v, str):
        try:
            return float(v)
        except ValueError:
            return v.strip()
    # Convert ints to float for fair comparison
    if isinstance(v, (int, np.integer)):
        return float(v)
    return v

df1_sorted = df1_sorted.applymap(normalize_value)
df2_sorted = df2_sorted.applymap(normalize_value)

# Step 5: Compare DataFrames row by row
mismatch_records = []

for i in range(len(df1_sorted)):
    row1 = df1_sorted.iloc[i]
    row2 = df2_sorted.iloc[i]
    
    # Create a pseudo-row identifier (like hash of all values)
    row_identifier = f"ROW_{i+1}"
    
    for col in df1_sorted.columns:
        val1, val2 = row1[col], row2[col]
        
        # Skip if both NaN
        if pd.isna(val1) and pd.isna(val2):
            continue
        
        # Compare numeric types safely
        if isinstance(val1, (int, float)) and isinstance(val2, (int, float)):
            if not np.isclose(val1, val2, equal_nan=True):
                mismatch_records.append({'Row': row_identifier, 'Column': col, 'df1_Value': val1, 'df2_Value': val2})
        else:
            if val1 != val2:
                mismatch_records.append({'Row': row_identifier, 'Column': col, 'df1_Value': val1, 'df2_Value': val2})

# Step 6: Create final mismatch DataFrame
mismatch_df = pd.DataFrame(mismatch_records)

if mismatch_df.empty:
    print("‚úÖ All rows and columns match perfectly!")
else:
    print(f"‚ùå Found {len(mismatch_df)} mismatches:")
    display(mismatch_df)












df1_sorted['ROW_HASH'] = df1_sorted.astype(str).agg('|'.join, axis=1)
df2_sorted['ROW_HASH'] = df2_sorted.astype(str).agg('|'.join, axis=1)

common = set(df1_sorted['ROW_HASH']).intersection(set(df2_sorted['ROW_HASH']))
print(f"‚úÖ {len(common)} rows are exactly matching by all column values.")





















import pandas as pd
import numpy as np

# Step 1: Sort both DataFrames by SOEID
df1_sorted = df1.sort_values(by='SOEID').reset_index(drop=True)
df2_sorted = df2.sort_values(by='SOEID').reset_index(drop=True)

# Step 2: Replace None with NaN for consistency
df1_sorted = df1_sorted.replace({None: np.nan})
df2_sorted = df2_sorted.replace({None: np.nan})

# Step 3: Ensure both DataFrames have same columns
assert list(df1_sorted.columns) == list(df2_sorted.columns), "‚ùå Column names differ!"
assert df1_sorted.shape == df2_sorted.shape, "‚ùå Shapes differ!"

# Step 4: Normalize numeric types (so -64 and -64.0 are treated same)
def normalize_value(v):
    if pd.isna(v):
        return np.nan
    # Try converting numeric-like strings to float
    if isinstance(v, str) and v.replace('.', '', 1).replace('-', '', 1).isdigit():
        try:
            return float(v)
        except:
            return v
    # Convert ints to float (so types match)
    if isinstance(v, (int, np.integer)):
        return float(v)
    return v

df1_sorted = df1_sorted.applymap(normalize_value)
df2_sorted = df2_sorted.applymap(normalize_value)

# Step 5: Compare and collect mismatches
mismatch_records = []

for i in range(len(df1_sorted)):
    soe_id = df1_sorted.iloc[i]['SOEID']
    row1 = df1_sorted.iloc[i]
    row2 = df2_sorted.iloc[i]
    
    for col in df1_sorted.columns:
        val1, val2 = row1[col], row2[col]
        
        # Skip if both NaN
        if pd.isna(val1) and pd.isna(val2):
            continue
        
        # Numeric comparison tolerance (so -64.0 == -64)
        if isinstance(val1, (int, float)) and isinstance(val2, (int, float)):
            if not np.isclose(val1, val2, equal_nan=True):
                mismatch_records.append({'SOEID': soe_id, 'Column': col, 'df1_Value': val1, 'df2_Value': val2})
        else:
            if val1 != val2:
                mismatch_records.append({'SOEID': soe_id, 'Column': col, 'df1_Value': val1, 'df2_Value': val2})

# Step 6: Create final mismatch dataframe
mismatch_df = pd.DataFrame(mismatch_records)

if mismatch_df.empty:
    print("‚úÖ All rows and columns match perfectly!")
else:
    print(f"‚ùå Found {len(mismatch_df)} mismatches:")
    display(mismatch_df)

























import pandas as pd
import numpy as np

# Step 1: Sort both DataFrames by SOEID
df1_sorted = df1.sort_values(by='SOEID').reset_index(drop=True)
df2_sorted = df2.sort_values(by='SOEID').reset_index(drop=True)

# Step 2: Ensure NaN consistency (treat None as NaN)
df1_sorted = df1_sorted.replace({None: np.nan})
df2_sorted = df2_sorted.replace({None: np.nan})

# Step 3: Ensure both DataFrames have same columns and shape
assert list(df1_sorted.columns) == list(df2_sorted.columns), "‚ùå Column names differ!"
assert df1_sorted.shape == df2_sorted.shape, "‚ùå Shapes differ!"

# Step 4: Identify mismatched cells
mismatch_records = []

for i in range(len(df1_sorted)):
    soe_id = df1_sorted.iloc[i]['SOEID']
    row1 = df1_sorted.iloc[i]
    row2 = df2_sorted.iloc[i]
    
    for col in df1_sorted.columns:
        val1, val2 = row1[col], row2[col]
        
        # Compare NaN safely
        if (pd.isna(val1) and pd.isna(val2)):
            continue
        elif val1 != val2:
            mismatch_records.append({
                'SOEID': soe_id,
                'Column': col,
                'df1_Value': val1,
                'df2_Value': val2
            })

# Step 5: Create DataFrame of mismatches
mismatch_df = pd.DataFrame(mismatch_records)

if mismatch_df.empty:
    print("‚úÖ All rows and columns match perfectly!")
else:
    print("‚ùå Found mismatches:")
    display(mismatch_df)






















import pandas as pd
import numpy as np

# Step 1: Sort both dataframes by SOE_ID
df1_sorted = df1.sort_values(by="SOE_ID").reset_index(drop=True)
df2_sorted = df2.sort_values(by="SOE_ID").reset_index(drop=True)

# Step 2: Replace None with np.nan so both are comparable
df1_sorted = df1_sorted.replace({None: np.nan})
df2_sorted = df2_sorted.replace({None: np.nan})

# Step 3: Check if both dataframes are exactly equal
are_equal = df1_sorted.equals(df2_sorted)

print(f"‚úÖ DataFrames Match: {are_equal}")

# Step 4: If not equal, find mismatched rows and columns
if not are_equal:
    # Create a mask of where values differ
    mismatch_mask = (df1_sorted != df2_sorted) & ~(df1_sorted.isna() & df2_sorted.isna())
    
    # Get rows with at least one mismatch
    mismatched_rows = df1_sorted[mismatch_mask.any(axis=1)].copy()
    mismatched_rows['SOE_ID'] = df1_sorted.loc[mismatch_mask.any(axis=1), 'SOE_ID']
    
    # For clarity, also show which columns differ
    mismatched_columns = mismatch_mask.any()
    diff_columns = mismatch_mask.columns[mismatched_columns].tolist()
    
    print("\n‚ö†Ô∏è Mismatched Columns:")
    print(diff_columns)
    
    print("\n‚ö†Ô∏è Mismatched Rows with Differences:")
    display(pd.concat([
        df1_sorted[mismatch_mask.any(axis=1)].add_suffix('_DF1'),
        df2_sorted[mismatch_mask.any(axis=1)].add_suffix('_DF2')
    ], axis=1))















import pandas as pd
import numpy as np

# Ensure same shape, column order, and reset index
df1 = df1.reset_index(drop=True)
df2 = df2.reset_index(drop=True)
df1 = df1[df2.columns]

# Compare while treating NaN/None as equal
comparison = (df1.eq(df2)) | (df1.isna() & df2.isna())

# Find rows with any mismatched column
diff_rows = ~comparison.all(axis=1)

records = []

# Loop through mismatched rows
for idx in df1.index[diff_rows]:
    mismatched_cols = df1.columns[~comparison.loc[idx]].tolist()
    row_data = {
        "Row_Index": idx,
        "Mismatched_Columns": mismatched_cols
    }
    
    # Add mismatched column values side-by-side
    for col in mismatched_cols:
        row_data[f"df1_{col}"] = df1.at[idx, col]
        row_data[f"df2_{col}"] = df2.at[idx, col]
    
    records.append(row_data)

# Create the mismatch summary dataframe
mismatch_df = pd.DataFrame(records)

print(f"‚úÖ Total mismatched rows: {len(mismatch_df)}")
display(mismatch_df)





















import pandas as pd

# Ensure same column order and length
df1 = df1.reset_index(drop=True)
df2 = df2.reset_index(drop=True)
df1 = df1[df2.columns]

# Identify mismatched rows
comparison = df1 == df2
diff_rows = ~comparison.all(axis=1)

# Collect mismatched info
records = []

for idx in df1.index[diff_rows]:
    mismatched_cols = df1.columns[df1.loc[idx] != df2.loc[idx]].tolist()
    row_data = {
        "Row_Index": idx,
        "Mismatched_Columns": mismatched_cols
    }
    
    # Add df1 and df2 values for mismatched columns only
    for col in mismatched_cols:
        row_data[f"df1_{col}"] = df1.at[idx, col]
        row_data[f"df2_{col}"] = df2.at[idx, col]
    
    records.append(row_data)

# Create the summary dataframe
mismatch_df = pd.DataFrame(records)

print(f"Total mismatched rows: {len(mismatch_df)}")
display(mismatch_df)





















import pandas as pd
import numpy as np

# Ensure same column order
df1 = df1[df2.columns]

# Compare element-wise
comparison = df1 == df2

# Rows where any column differs
diff_rows = ~comparison.all(axis=1)

# Show rows that differ
diff_report = pd.concat(
    [df1[diff_rows].add_prefix('df1_'), df2[diff_rows].add_prefix('df2_')],
    axis=1
)

print("Total rows mismatched:", diff_rows.sum())
display(diff_report)





mismatch_details = {}

for idx in df1.index:
    mismatched_cols = df1.columns[df1.loc[idx] != df2.loc[idx]].tolist()
    if mismatched_cols:
        mismatch_details[idx] = mismatched_cols

print("Mismatched rows and columns:")
for i, cols in mismatch_details.items():
    print(f"Row {i}: columns {cols}")















import pandas as pd

key = 'POSITION_ID'

# --- Step 1: Filter only duplicates
dupes = df[df.duplicated(subset=[key], keep=False)].copy()

# --- Step 2: Group and find differences
records = []

for pid, group in dupes.groupby(key):
    group = group.reset_index(drop=True)
    diff_cols = [col for col in group.columns if col != key and group[col].nunique(dropna=False) > 1]

    if diff_cols:
        diff_data = {}
        diff_data[key] = pid
        diff_data['Columns_Not_Matching'] = ', '.join(diff_cols)

        # Collect differing values for each mismatched column
        for col in diff_cols:
            diff_data[f'{col}_Values'] = group[col].astype(str).unique().tolist()

        records.append(diff_data)

# --- Step 3: Create result DataFrame
mismatch_report = pd.DataFrame(records)

print("\n‚ö†Ô∏è Duplicate IDs with non-matching column values:")
print(mismatch_report)



















import pandas as pd

key = 'POSITION_ID'

# --- Find duplicates
dupes = df[df.duplicated(subset=[key], keep=False)].copy()

# --- Group by POSITION_ID and check if all rows within that group are identical
# We do this by comparing number of unique rows per group
def check_group_identical(group):
    return group.drop(columns=[key]).nunique(dropna=False).eq(1).all()

duplicate_summary = (
    dupes.groupby(key)
         .apply(check_group_identical)
         .reset_index(name='All_Columns_Same')
)

# --- Split into two categories
true_duplicates = duplicate_summary[duplicate_summary['All_Columns_Same']]
conflicting_duplicates = duplicate_summary[~duplicate_summary['All_Columns_Same']]

print("\n‚úÖ True duplicates (completely identical rows):")
print(true_duplicates)

print("\n‚ö†Ô∏è Conflicting duplicates (same ID, but different column values):")
print(conflicting_duplicates)






















import pandas as pd
import numpy as np

# Example: df1 and df2 already loaded (from Excel or other sources)
# Common key column
key = 'POSITION_ID'

# Align on POSITION_ID to ensure same row order for comparison
df1 = df1.set_index(key).sort_index()
df2 = df2.set_index(key).sort_index()

# Ensure both have same columns
common_cols = df1.columns.intersection(df2.columns)

# Compare values treating NaN/None as equal
comparison = (df1[common_cols].fillna('__NULL__') == df2[common_cols].fillna('__NULL__'))

# Summary per column
summary = comparison.sum().to_frame(name='Matching_Count')
summary['Total_Rows'] = len(comparison)
summary['Match_%'] = (summary['Matching_Count'] / summary['Total_Rows'] * 100).round(2)

# Identify mismatched rows for detailed check
mismatch_details = (
    ~comparison
).any(axis=1)

mismatch_report = pd.DataFrame({
    key: df1.index[mismatch_details],
    'Columns_Not_Matching': comparison.columns[~comparison.loc[mismatch_details].all()].tolist()
})

# Final summary output
print("\nüîç Column-wise match summary:")
print(summary)

print("\n‚ö†Ô∏è Mismatch details (rows where at least one column differs):")
print(mismatch_report)



















import pandas as pd

# Example: assuming df1 and df2 have same columns including POSITION_ID

# Merge both dataframes on POSITION_ID
merged = df1.merge(df2, on='POSITION_ID', suffixes=('_df1', '_df2'))

# List of columns to compare (excluding POSITION_ID)
columns_to_compare = [col for col in df1.columns if col != 'POSITION_ID']

# For each column, check if they match
for col in columns_to_compare:
    merged[f'{col}_MATCH'] = merged[f'{col}_df1'] == merged[f'{col}_df2']

# Compute summary match status per POSITION_ID
summary = merged[['POSITION_ID'] + [f'{col}_MATCH' for col in columns_to_compare]]

# Add an overall flag if all columns matched
summary['ALL_COLUMNS_MATCH'] = summary[[f'{col}_MATCH' for col in columns_to_compare]].all(axis=1)

# Display result
print(summary)





mismatched = summary[~summary['ALL_COLUMNS_MATCH']]
print(mismatched)






















SET SERVEROUTPUT ON;

DECLARE
    v_sql CLOB := '';
BEGIN
    FOR rec IN (
        SELECT column_name
        FROM all_tab_columns
        WHERE owner = 'WFA_APP_USR_TALD'
          AND table_name = 'BASE_WD_REPORT_VIEW_POSITION'
          AND column_name <> 'PERIOD'  -- ignore period filter column
          AND column_name <> 'POSITION_ID'  -- key column
        ORDER BY column_id
    ) LOOP
        v_sql := v_sql || 'WHEN a.' || rec.column_name || ' <> b.' || rec.column_name ||
                 ' THEN ''' || rec.column_name || '''' || CHR(10);
    END LOOP;

    DBMS_OUTPUT.PUT_LINE('--- Generated Comparison Query ---');
    DBMS_OUTPUT.PUT_LINE('SELECT a.POSITION_ID, CASE ' || CHR(10) || v_sql || 
        ' END AS MISMATCH_COLUMN
FROM WFA_APP_USR_TALD.BASE_WD_REPORT_VIEW_POSITION a
JOIN WFA_APP_USR_TALD.BASE_WD_REPORT_VIEW_POSITION_PYTHON b
  ON a.POSITION_ID = b.POSITION_ID
WHERE a.PERIOD = ''10/27/2025''
  AND b.PERIOD = ''10/27/2025''
  AND ( ');

    FOR rec IN (
        SELECT column_name
        FROM all_tab_columns
        WHERE owner = 'WFA_APP_USR_TALD'
          AND table_name = 'BASE_WD_REPORT_VIEW_POSITION'
          AND column_name <> 'PERIOD'
          AND column_name <> 'POSITION_ID'
        ORDER BY column_id
    ) LOOP
        v_sql := v_sql || 'a.' || rec.column_name || ' <> b.' || rec.column_name || ' OR ';
    END LOOP;

    DBMS_OUTPUT.PUT_LINE(RTRIM(v_sql, ' OR ') || ' );');
END;
/






















SELECT
    a.POSITION_ID,
    CASE
        WHEN a.POSITION_NAME != b.POSITION_NAME THEN 'POSITION_NAME'
        WHEN a.MANAGER_NAME  != b.MANAGER_NAME  THEN 'MANAGER_NAME'
        WHEN a.COUNTRY       != b.COUNTRY       THEN 'COUNTRY'
        WHEN a.DEPARTMENT    != b.DEPARTMENT    THEN 'DEPARTMENT'
        WHEN a.LEVEL         != b.LEVEL         THEN 'LEVEL'
    END AS MISMATCH_COLUMN
FROM WFA_APP_USR_TALD.BASE_WD_REPORT_VIEW_POSITION a
JOIN WFA_APP_USR_TALD.BASE_WD_REPORT_VIEW_POSITION_PYTHON b
  ON a.POSITION_ID = b.POSITION_ID
WHERE a.PERIOD = '10/27/2025'
  AND b.PERIOD = '10/27/2025'
  AND (
        a.POSITION_NAME != b.POSITION_NAME OR
        a.MANAGER_NAME  != b.MANAGER_NAME OR
        a.COUNTRY       != b.COUNTRY OR
        a.DEPARTMENT    != b.DEPARTMENT OR
        a.LEVEL         != b.LEVEL
      );



























SELECT POSITION_ID, 'MISSING_IN_PYTHON' AS STATUS
FROM WFA_APP_USR_TALD_BASE_WD_REPORT_VIEW_POSITION
WHERE POSITION_ID NOT IN (
    SELECT POSITION_ID FROM WFA_APP_USR_TALD_BASE_WD_REPORT_VIEW_POSITION_PYTHON
)
UNION ALL
SELECT POSITION_ID, 'EXTRA_IN_PYTHON' AS STATUS
FROM WFA_APP_USR_TALD_BASE_WD_REPORT_VIEW_POSITION_PYTHON
WHERE POSITION_ID NOT IN (
    SELECT POSITION_ID FROM WFA_APP_USR_TALD_BASE_WD_REPORT_VIEW_POSITION
);











SET SERVEROUTPUT ON
DECLARE
    v_sql   CLOB;
BEGIN
    -- 1Ô∏è‚É£ Drop existing summary table if exists
    BEGIN
        EXECUTE IMMEDIATE 'DROP TABLE POSITION_MISMATCH_SUMMARY';
    EXCEPTION
        WHEN OTHERS THEN
            NULL; -- ignore if table doesn‚Äôt exist
    END;

    -- 2Ô∏è‚É£ Create new summary table
    EXECUTE IMMEDIATE '
        CREATE TABLE POSITION_MISMATCH_SUMMARY (
            POSITION_ID VARCHAR2(200),
            MISMATCHED_COLUMNS VARCHAR2(4000)
        )
    ';

    -- 3Ô∏è‚É£ Start building dynamic comparison SQL
    v_sql := 'INSERT INTO POSITION_MISMATCH_SUMMARY
              SELECT a.POSITION_ID, ';

    -- Generate CASE-based concatenation for mismatched columns
    FOR rec IN (
        SELECT COLUMN_NAME
        FROM USER_TAB_COLUMNS
        WHERE TABLE_NAME = 'WFA_APP_USR_TALD_BASE_WD_REPORT_VIEW_POSITION'
          AND COLUMN_NAME <> 'POSITION_ID'
        ORDER BY COLUMN_ID
    )
    LOOP
        v_sql := v_sql ||
            'CASE WHEN NVL(a.' || rec.COLUMN_NAME || ', ''X'') != NVL(b.' || rec.COLUMN_NAME || ', ''X'') 
                  THEN ''' || rec.COLUMN_NAME || ' '' ELSE '''' END || ';
    END LOOP;

    -- Remove trailing concatenation operator
    v_sql := RTRIM(v_sql, ' || ');

    -- Continue query body
    v_sql := v_sql || ' AS MISMATCHED_COLUMNS
        FROM WFA_APP_USR_TALD_BASE_WD_REPORT_VIEW_POSITION a
        JOIN WFA_APP_USR_TALD_BASE_WD_REPORT_VIEW_POSITION_PYTHON b
          ON a.POSITION_ID = b.POSITION_ID
        WHERE ';

    -- Generate WHERE clause dynamically
    FOR rec IN (
        SELECT COLUMN_NAME
        FROM USER_TAB_COLUMNS
        WHERE TABLE_NAME = 'WFA_APP_USR_TALD_BASE_WD_REPORT_VIEW_POSITION'
          AND COLUMN_NAME <> 'POSITION_ID'
        ORDER BY COLUMN_ID
    )
    LOOP
        v_sql := v_sql || 'NVL(a.' || rec.COLUMN_NAME || ', ''X'') != NVL(b.' || rec.COLUMN_NAME || ', ''X'') OR ';
    END LOOP;

    -- Remove trailing OR
    v_sql := RTRIM(v_sql, ' OR ');

    -- 4Ô∏è‚É£ Execute the dynamic SQL
    EXECUTE IMMEDIATE v_sql;

    DBMS_OUTPUT.PUT_LINE('‚úÖ Comparison completed successfully.');
    DBMS_OUTPUT.PUT_LINE('‚û°Ô∏è  Use: SELECT * FROM POSITION_MISMATCH_SUMMARY;');
EXCEPTION
    WHEN OTHERS THEN
        DBMS_OUTPUT.PUT_LINE('‚ùå Error: ' || SQLERRM);
END;
/



SELECT * FROM POSITION_MISMATCH_SUMMARY;











Here's a technical 2-liner description for each fraud detection feature based on the formulas:

1. high_dollar_velocity_3hr:
Ratio of high-value (‚â•300) txn count in 3hr to total txn count in 3hr. Captures rapid bursts of high-value activity.


2. high_dollar_ratio_24hr:
Ratio of high-value txn amount to total txn amount in 24hr. Highlights dominance of large-value txns in a short time.


3. exp_date_mismatch_ratio_7days:
Count of expiration date mismatches over total txn count in 7 days. Detects frequent CVV/expiry mismatches often tied to synthetic fraud.


4. zero_auth_mismatch_ratio_24hr:
Ratio of zero-auth mismatch count to zero-auth txn count in 24hr. Flags abnormal patterns in pre-auth probes.


5. foreign_high_dollar_ratio:
Ratio of high-value foreign txn amount to total txn amount in 24hr. Indicates risky cross-border high-value txns.


6. weekend_high_dollar_ratio:
Ratio of weekend high-value txn amount to total weekend txn amount in 24hr. Targets off-hour fraud attempts when oversight is low.


7. nighttime_high_dollar_velocity:
High-value txn velocity during 10pm‚Äì6am, normalized by txn count. Tracks abnormal night-time high-value behavior.


8. high_dollar_swipe_ratio:
Proportion of swipe high-value txn amount to total amount. Flags high-value in-person swipes that deviate from profile.


9. ratio_avg_swipe_amt_24hr_by_60days:
Average swipe txn amount in 24hr normalized by 60-day average. Highlights short-term spending anomalies.


10. transaction_acceleration_24hr:
Ratio of 24hr txn amount to 48hr trailing window. Quantifies sharp spending increases.


11. avmismatch_ratio_30days:
Count of AV (address verification) mismatches to total txns over 30 days. Flags consistent identity verification failures.


12. zero_auth_high_dollar_ratio:
Ratio of high-value zero-auth txn amount to total zero-auth txn amount. Targets abnormal testing behaviors with large amounts.


13. high_val_tx_velocity:
Ratio of high-value txns to txn count at the same merchant in 24hr. Detects merchant-specific fraud bursts.


14. fraud_trx_drift:
Ratio of fraudulent txn amount to average txn amount at merchant over 60 days. Captures fraud drift toward certain merchants.


15. cnt_same_mid_termid_24hr:
Count of txns at same merchant-terminal combo in 24hr. Detects excessive usage of specific terminal for fraud.


16. cnt_for_swipe_24hr:
Count of swiped txns in 24hr window. Useful for card-present fraud detection.


17. velocity_merchant_risk:
Velocity feature capturing merchant-level txn acceleration. Quantifies merchant risk based on usage pattern deviation.


18. cnt_same_merc_id_24hr:
Count of txns at same merchant ID in 24hr. Flags repeated usage of a merchant which may be fraud hotspot.


19. cnt_same_merc_id_30min:
Same as above but in 30min interval. Captures short-burst fraud attempts.


20. hot_mer_mcc_bayesian_encoded_1/11/21/24hr:
Bayesian encoding of merchant MCC risk based on target leakage in various time windows. Encodes historical MCC fraud likelihood in rolling fashion.



Let me know if you want this exported to a table or file format for presentation.









Here is the feature-wise technical description with exact formula context from your image:


---

1. high_dollar_fraud_mer_ratio
Formula: (amt_same_mid_termid_30min) / (amt_same_mid_termid_24hr + 1)
Description: Ratio of high-value txn amount within 30min to total 24hr amount at the same MID-Terminal. Detects rapid high-value fraud bursts.


2. fraud_zero_auth_ratio
Formula: (cnt_za_ris_24hr * cnt_za_mis_7days) / (tot_zero_auth_cnt_24hr + 1)
Description: Product of recent risky zero-auth txn count and 7-day mismatch count, normalized by total 24hr zero-auth volume. Captures anomaly in zero-auth behavior.


3. high_risk_mer8_name_shift
Formula: new_merch_ind * hct_mer_mcc_bayesian_encoded_8
Description: Binary merchant shift indicator weighted by MCC Bayesian risk. Highlights movement toward historically risky MCCs.


4. swipe_anomaly_score
Formula: abs(amt_swipe_tdy - avg_swipe_amt_24hr) / (avg_swipe_amt_60days + epsilon1)
Description: Absolute swipe txn deviation from 24hr avg, scaled by long-term mean. Detects sudden abnormal swipe patterns.


5. time_since_last_high_dollar
Formula: amt_ge500_ind * time_since_last_trxn
Description: High-value txn flag weighted by time since last txn. Flags spacing between large amounts.


6. mcc_shift_30d_index
Formula: abs(new_mcc_ind - avg_same_mcc_amt_30days)
Description: Deviation of MCC switch from 30-day avg. Highlights abrupt merchant pattern changes.


7. mcc_shift_24hr_index
Formula: abs(new_mcc_ind - avg_same_mcc_amt_24hr)
Description: Captures short-term MCC pattern deviation. Flags recent merchant usage anomalies.


8. auth_decline_ratio
Formula: cnt_same_merc_id_6hr / (cnt_same_merc_id_6hr + 1)
Description: Simple ratio capturing dense merchant hits post-auth decline. Signifies retry patterns.


9. transaction_amt_drift
Formula: (amt_swipe_tdy * avg_trxn_amt_30days) / (avg_trxn_amt_60days + epsilon1)
Description: Recent swipe txn amount scaled by 30d avg, normalized by 60d avg. Detects shifts in txn amount profiles.


10. high_density_swipe_score
Formula: cnt_swipe_24hr / (cnt_swipe_60days + 1)
Description: Swipe txn density score across timeframes. Flags bursts of card-present activity.


11. zero_auth_high_risk_mer_score
Formula: zero_auth_diff_mer_nm_ind * hct_mer_mcc_bayesian_encoded_1
Description: Flags zero-auth attempts at new merchants with MCC-based risk weighting.


12. mcc_high_dollar_ratio
Formula: (amt_5310_5411_24hr * amt_ge500_ind) / (tot_amt_24hr + 1)
Description: Proportion of high-value txns at MCC 5310/5411 (grocery/retail) to total. Detects large-value retail spikes.


13. ratio_high_dollar_cnp_24hr
Formula: (amt_cnp_24hr * amt_ge500_ind) / (tot_amt_24hr + 1)
Description: High-value card-not-present txn ratio over total 24hr value. Flags e-comm fraud risk.


14. multi_window_zero_auth_velocity
Formula: tot_zero_auth_cnt_24hr * tot_zero_auth_cnt_48hr * tot_zero_auth_cnt_72hr
Description: Velocity of zero-auth transactions across 3 time windows. Captures bot-like probing.


15. trxn_hour_foreign_amt_ge500
Formula: trxn_hour * foreign_ind * amt_ge500_ind
Description: Time-weighted risk score for high-value foreign txns. Flags suspicious cross-border behavior.




---

Would you like this as a structured Excel or PDF for documentation?








import os
import xlsxwriter

# Define paths for PDP and Bivariate plots
pdp_folder = "pdp_plot"
bvp_folder = "bvp_plot"
output_excel = "Combined_Plots.xlsx"

# Get the list of feature names (assuming both folders have same naming convention)
feature_names = sorted([f.replace(".png", "") for f in os.listdir(pdp_folder) if f.endswith(".png")])

# Create an Excel workbook
workbook = xlsxwriter.Workbook(output_excel)

# Loop through each feature and add PDP + BVP to the sheet
for feature in feature_names:
    worksheet = workbook.add_worksheet(feature)  # Create a new sheet for each feature
    
    # Define image paths
    pdp_path = os.path.join(pdp_folder, feature + ".png")
    bvp_path = os.path.join(bvp_folder, feature + ".png")
    
    # Insert images into the worksheet
    worksheet.insert_image("A1", pdp_path, {'x_scale': 0.5, 'y_scale': 0.5})  # Adjust size if needed
    worksheet.insert_image("J1", bvp_path, {'x_scale': 0.5, 'y_scale': 0.5})  # Insert BVP next to PDP

# Close and save the Excel file
workbook.close()

print(f"‚úÖ Excel file '{output_excel}' created successfully with 36 sheets!")











import h2o
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from pdpbox import pdp
from h2o.estimators.gbm import H2OGradientBoostingEstimator

# Initialize H2O
h2o.init()

# Load H2O DataFrames (Replace with actual files)
train = h2o.import_file("train.csv")
train_rs = h2o.import_file("train_rs.csv")
test = h2o.import_file("test.csv")
oot = h2o.import_file("oot.csv")

# Convert to Pandas
train_df = train.as_data_frame()
train_rs_df = train_rs.as_data_frame()
test_df = test.as_data_frame()
oot_df = oot.as_data_frame()

# Define Target and Feature List
target = "fraud_label"  # Replace with your actual target column
features = ["feature_1", "feature_2", ..., "feature_30"]  # Replace with your selected variables

# Train H2O GBM Model
gbm_model = H2OGradientBoostingEstimator(ntrees=50, max_depth=5, learn_rate=0.1)
gbm_model.train(x=features, y=target, training_frame=train)

# Convert Data for PDPBox
train_df["dataset"] = "Train"
train_rs_df["dataset"] = "Train_RS"
test_df["dataset"] = "Test"
oot_df["dataset"] = "OOT"

# Combine All Datasets for Analysis
combined_df = pd.concat([train_df, train_rs_df, test_df, oot_df], axis=0)

### üìå Generate Plots for Each Feature ###
for feature in features:
    fig, ax1 = plt.subplots(figsize=(10, 5))

    # 1Ô∏è‚É£ PDP Plot
    pdp_goals = pdp.pdp_isolate(model=gbm_model, dataset=train_df, model_features=features, feature=feature)
    pdp.pdp_plot(pdp_goals, feature, plot_lines=False, ax=ax1)
    ax1.set_title(f"PDP & Fraud Rate Trend - {feature}")
    ax1.set_ylabel("Partial Dependence")
    
    # 2Ô∏è‚É£ Bivariate Plot (Fraud Rate per bin across datasets)
    ax2 = ax1.twinx()
    combined_df["bin"] = pd.qcut(combined_df[feature], q=10, duplicates="drop")  # Binning
    fraud_rates = combined_df.groupby(["bin", "dataset"])[target].mean().unstack()
    fraud_rates.plot(marker="o", ax=ax2)
    
    ax2.set_ylabel("Fraud Rate")
    ax2.legend(title="Dataset")

    plt.show()










import h2o
h2o.init()

# Load your dataset
df = h2o.import_file("path_to_your_dataset.csv")

# Create interaction features
df['trxn_freq_anomaly'] = df['tot_cnt_24hr'] / df['avg_trxn_cnt_30days']
df['time_since_last_trxn_anomaly'] = df['time_since_last_trxn'] / df['avg_time_between_trxns_30days']
df['wkend_wkday_ratio'] = df['tot_cnt_30days_wkend'] / df['tot_cnt_30days_wkday']
df['large_amt_anomaly'] = df['amt_swipe_ge_1000_24hr'] / df['avg_swipe_amt_24hr']
df['swipe_cnp_amt_ratio'] = df['amt_swipe_24hr'] / df['amt_cnp_24hr']
df['zero_auth_ratio'] = df['tot_zero_auth_cnt_24hr'] / df['tot_cnt_24hr']
df['dist_btw_swp_anomaly'] = df['dist_miles_btw_swp'] / df['avg_dist_btw_swp_30days']
df['foreign_trxn_ratio'] = df['cnt_fore_swipe_24hr'] / df['tot_cnt_24hr']
df['new_merch_ratio'] = df['new_merch_ind'] / df['tot_cnt_24hr']
df['mcc_anomaly_score'] = df['same_mcc_cnt_24hr'] / df['avg_same_mcc_cnt_30days']
df['cvv_mismatch_ratio'] = df['cnt_cvv2_mismatch_24hr'] / df['tot_cnt_24hr']
df['avs_mismatch_ratio'] = df['cnt_avs_mismatch_24hr'] / df['tot_cnt_24hr']
df['swipe_cnp_ratio'] = df['cnt_swipe_24hr'] / df['cnt_cnp_24hr']
df['visa_mastercard_ratio'] = df['visa_ind'] / df['mastercard_ind']
df['fraud_risk_score'] = df['cvv_mismatch_ratio'] * df['avs_mismatch_ratio'] * df['new_merch_ratio']
df['geo_temp_anomaly_score'] = df['dist_btw_swp_anomaly'] * df['time_since_last_trxn_anomaly']

# Save the updated dataset
df.save("path_to_updated_dataset.csv")






# Create interaction features
df['trxn_velocity_anomaly'] = df['tot_cnt_24hr'] / df['tot_cnt_30days']
df['zero_auth_time_gap_anomaly'] = df['mins_since_zero_auth'] / df['avg_mins_between_zero_auth_30days']
df['same_merc_cluster_score'] = df['same_merc_cnt_24hr'] / df['same_merc_cnt_30days']
df['cvv_mismatch_time_gap'] = df['time_since_cvv2_mismatch'] / df['avg_time_between_cvv_mismatch_30days']
df['avs_mismatch_time_gap'] = df['time_since_avs_mismatch'] / df['avg_time_between_avs_mismatch_30days']
df['exp_mismatch_ratio'] = df['cnt_exp_date_mismatch_24hr'] / df['tot_cnt_24hr']
df['high_risk_mcc_ratio'] = df['cnt_5310_5411_24hr'] / df['tot_cnt_24hr']
df['new_mcc_ratio'] = df['new_mcc_ind'] / df['tot_cnt_24hr']
df['merch_id_cluster_anomaly'] = df['same_merc_cnt_24hr'] / df['same_merc_cnt_30days']
df['cross_state_anomaly'] = df['diff_state_swp_24hr_ind'] / df['tot_cnt_24hr']
df['time_btw_swp_anomaly'] = df['time_btw_swp'] / df['avg_time_btw_swp_30days']
df['foreign_trxn_time_gap'] = df['time_since_last_foreign_cnp'] / df['avg_time_between_foreign_trxns_30days']
df['fraud_risk_score_2'] = df['cvv_mismatch_ratio'] * df['avs_mismatch_ratio'] * df['exp_mismatch_ratio'] * df['new_merch_ratio']
df['geo_mcc_anomaly_score'] = df['cross_state_anomaly'] * df['high_risk_mcc_ratio']

# Save the updated dataset
df.save("path_to_updated_dataset.csv")





# Create interaction features
df['zero_auth_to_auth_ratio'] = df['tot_zero_auth_cnt_24hr'] / df['tot_cnt_24hr']
df['small_amt_testing_score'] = df['cnt_sameamt_auth_6h'] / df['tot_cnt_24hr']
df['cvv_mismatch_after_zero_auth'] = df['cnt_cvv2_mismatch_24hr'] / df['tot_zero_auth_cnt_24hr']
df['trxn_burst_score'] = df['tot_cnt_30min'] / df['tot_cnt_24hr']
df['same_merc_velocity_anomaly'] = df['same_merc_cnt_30min'] / df['same_merc_cnt_24hr']
df['mcc_velocity_anomaly'] = df['same_mcc_cnt_30min'] / df['same_mcc_cnt_24hr']
df['dist_to_last_trxn_anomaly'] = df['dist_miles_btw_swp'] / df['avg_dist_btw_swp_30days']
df['cross_country_ratio'] = df['cnt_cntry_24hr'] / df['tot_cnt_24hr']
df['time_btw_geo_hops'] = df['time_btw_swp'] / df['dist_miles_btw_swp']
df['same_merc_to_term_ratio'] = df['cnt_same_mid_termid_24hr'] / df['same_merc_cnt_24hr']
df['new_merc_to_term_ratio'] = df['new_merch_id'] / df['cnt_same_mid_termid_24hr']
df['high_risk_merc_cluster'] = (df['same_merc_cnt_24hr'] / df['tot_cnt_24hr']) * df['high_risk_mcc_ratio']
df['fraud_risk_score_3'] = df['zero_auth_to_auth_ratio'] * df['small_amt_testing_score'] * df['cvv_mismatch_after_zero_auth'] * df['trxn_burst_score']
df['geo_velocity_anomaly_score'] = df['dist_to_last_trxn_anomaly'] * df['trxn_burst_score']

# Save the updated dataset
df.save("path_to_updated_dataset.csv")





# Create interaction features
df['swipe_amt_deviation'] = (df['amt_swipe_24hr'] - df['avg_swipe_amt_24hr']) / df['std_swipe_amt_24hr']
df['trxn_hour_anomaly'] = (df['trxn_hour'] - df['avg_trxn_hour_30days']) / df['std_trxn_hour_30days']
df['swipe_cnp_ratio_deviation'] = (df['swipe_cnp_ratio'] - df['avg_swipe_cnp_ratio_30days']) / df['std_swipe_cnp_ratio_30days']
df['high_risk_mcc_bayesian_interaction'] = df['hct_term_mcc_bayesian_encoded_5'] * df['hct_term_mcc_bayesian_encoded_7']
df['new_merch_risk_score'] = df['new_mech_ind'] * (1 + df['cnt_same_merc_id_24hr'] / df['tot_cnt_24hr'])
df['mcc_risk_weighted_cnt'] = df['same_mcc_cnt_24hr'] * df['hct_term_mcc_bayesian_encoded_5']
df['geo_dispersion_score'] = (df['cnt_dom_states_24hr'] / df['tot_cnt_24hr']) * df['dist_miles_btw_swp']
df['time_btw_swp_cvv_mismatch'] = df['time_since_last_swipe'] / df['time_since_cvv2_mismatch']
df['foreign_trxn_risk_score'] = df['foreign_ind'] * (1 + df['cnt_fore_swipe_24hr'] / df['tot_cnt_24hr'])
df['cvv_verf_magn_interaction'] = df['cvv_verf_magn_ind'] * (1 + df['cnt_cvv2_mismatch_24hr'] / df['tot_cnt_24hr'])
df['avs_mismatch_risk_score'] = df['avs_addr_mismatch_ind'] * (1 + df['cnt_avs_mismatch_24hr'] / df['tot_cnt_24hr'])
df['exp_mismatch_risk_score'] = df['exp_date_mismatch_ind'] * (1 + df['cnt_exp_date_mismatch_24hr'] / df['tot_cnt_24hr'])
df['fraud_risk_score_4'] = df['swipe_amt_deviation'] * df['trxn_hour_anomaly'] * df['high_risk_mcc_bayesian_interaction'] * df['new_merch_risk_score']
df['geo_temp_anomaly_score_2'] = df['geo_dispersion_score'] * df['time_btw_swp_cvv_mismatch']

# Save the updated dataset
df.save("path_to_updated_dataset.csv")










GPT


import h2o
import h2o.frame
from h2o.frame import H2OFrame

# Initialize H2O
h2o.init()

# Load dataset
df = h2o.import_file("your_fraud_data.csv")

# Creating advanced interaction features
df["risk_weighted_swipe_intensity"] = (df["cnt_swipe_24hr"] * (1 + df["cvv_verf_magn_ind"])) / (df["cnt_swipe_60days"] + 1)
df["mcc_velocity_risk"] = df["same_mcc_cnt_24hr"] / (df["same_mcc_cnt_60days"] + 1)
df["terminal_deviation"] = (df["cnt_same_mid_termid_24hr"] - df["cnt_same_mid_termid_60days"]) / (df["cnt_same_mid_termid_60days"] + 1)
df["swipe_dist_time_anomaly"] = df["dist_miles_btw_swp"] / (df["time_btw_swp"] + 1)
df["fraud_amt_scaling"] = (df["amt_swipe_24hr"] / (df["amt_swipe_60days"] + 1)) * (1 / (df["pct_swipe_amt_60days"] + 0.01))
df["avs_exp_mismatch_score"] = (df["cnt_avs_mismatch_24hr"] + df["cnt_exp_date_mismatch_24hr"]) / (df["cnt_avs_mismatch_60days"] + df["cnt_exp_date_mismatch_60days"] + 1)
df["card_testing_ratio"] = df["cnt_zero_auth_24hr"] / (df["cnt_zero_auth_60days"] + 1)
df["high_risk_merchant_score"] = df["cnt_5310_5411_24hr"] / (df["cnt_5310_5411_60days"] + 1)

# Save the enhanced dataset
h2o.export_file(df, "fraud_data_with_interaction_features.csv")
























