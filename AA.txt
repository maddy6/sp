import os
import xlsxwriter

# Define paths for PDP and Bivariate plots
pdp_folder = "pdp_plot"
bvp_folder = "bvp_plot"
output_excel = "Combined_Plots.xlsx"

# Get the list of feature names (assuming both folders have same naming convention)
feature_names = sorted([f.replace(".png", "") for f in os.listdir(pdp_folder) if f.endswith(".png")])

# Create an Excel workbook
workbook = xlsxwriter.Workbook(output_excel)

# Loop through each feature and add PDP + BVP to the sheet
for feature in feature_names:
    worksheet = workbook.add_worksheet(feature)  # Create a new sheet for each feature
    
    # Define image paths
    pdp_path = os.path.join(pdp_folder, feature + ".png")
    bvp_path = os.path.join(bvp_folder, feature + ".png")
    
    # Insert images into the worksheet
    worksheet.insert_image("A1", pdp_path, {'x_scale': 0.5, 'y_scale': 0.5})  # Adjust size if needed
    worksheet.insert_image("J1", bvp_path, {'x_scale': 0.5, 'y_scale': 0.5})  # Insert BVP next to PDP

# Close and save the Excel file
workbook.close()

print(f"‚úÖ Excel file '{output_excel}' created successfully with 36 sheets!")











import h2o
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from pdpbox import pdp
from h2o.estimators.gbm import H2OGradientBoostingEstimator

# Initialize H2O
h2o.init()

# Load H2O DataFrames (Replace with actual files)
train = h2o.import_file("train.csv")
train_rs = h2o.import_file("train_rs.csv")
test = h2o.import_file("test.csv")
oot = h2o.import_file("oot.csv")

# Convert to Pandas
train_df = train.as_data_frame()
train_rs_df = train_rs.as_data_frame()
test_df = test.as_data_frame()
oot_df = oot.as_data_frame()

# Define Target and Feature List
target = "fraud_label"  # Replace with your actual target column
features = ["feature_1", "feature_2", ..., "feature_30"]  # Replace with your selected variables

# Train H2O GBM Model
gbm_model = H2OGradientBoostingEstimator(ntrees=50, max_depth=5, learn_rate=0.1)
gbm_model.train(x=features, y=target, training_frame=train)

# Convert Data for PDPBox
train_df["dataset"] = "Train"
train_rs_df["dataset"] = "Train_RS"
test_df["dataset"] = "Test"
oot_df["dataset"] = "OOT"

# Combine All Datasets for Analysis
combined_df = pd.concat([train_df, train_rs_df, test_df, oot_df], axis=0)

### üìå Generate Plots for Each Feature ###
for feature in features:
    fig, ax1 = plt.subplots(figsize=(10, 5))

    # 1Ô∏è‚É£ PDP Plot
    pdp_goals = pdp.pdp_isolate(model=gbm_model, dataset=train_df, model_features=features, feature=feature)
    pdp.pdp_plot(pdp_goals, feature, plot_lines=False, ax=ax1)
    ax1.set_title(f"PDP & Fraud Rate Trend - {feature}")
    ax1.set_ylabel("Partial Dependence")
    
    # 2Ô∏è‚É£ Bivariate Plot (Fraud Rate per bin across datasets)
    ax2 = ax1.twinx()
    combined_df["bin"] = pd.qcut(combined_df[feature], q=10, duplicates="drop")  # Binning
    fraud_rates = combined_df.groupby(["bin", "dataset"])[target].mean().unstack()
    fraud_rates.plot(marker="o", ax=ax2)
    
    ax2.set_ylabel("Fraud Rate")
    ax2.legend(title="Dataset")

    plt.show()










import h2o
h2o.init()

# Load your dataset
df = h2o.import_file("path_to_your_dataset.csv")

# Create interaction features
df['trxn_freq_anomaly'] = df['tot_cnt_24hr'] / df['avg_trxn_cnt_30days']
df['time_since_last_trxn_anomaly'] = df['time_since_last_trxn'] / df['avg_time_between_trxns_30days']
df['wkend_wkday_ratio'] = df['tot_cnt_30days_wkend'] / df['tot_cnt_30days_wkday']
df['large_amt_anomaly'] = df['amt_swipe_ge_1000_24hr'] / df['avg_swipe_amt_24hr']
df['swipe_cnp_amt_ratio'] = df['amt_swipe_24hr'] / df['amt_cnp_24hr']
df['zero_auth_ratio'] = df['tot_zero_auth_cnt_24hr'] / df['tot_cnt_24hr']
df['dist_btw_swp_anomaly'] = df['dist_miles_btw_swp'] / df['avg_dist_btw_swp_30days']
df['foreign_trxn_ratio'] = df['cnt_fore_swipe_24hr'] / df['tot_cnt_24hr']
df['new_merch_ratio'] = df['new_merch_ind'] / df['tot_cnt_24hr']
df['mcc_anomaly_score'] = df['same_mcc_cnt_24hr'] / df['avg_same_mcc_cnt_30days']
df['cvv_mismatch_ratio'] = df['cnt_cvv2_mismatch_24hr'] / df['tot_cnt_24hr']
df['avs_mismatch_ratio'] = df['cnt_avs_mismatch_24hr'] / df['tot_cnt_24hr']
df['swipe_cnp_ratio'] = df['cnt_swipe_24hr'] / df['cnt_cnp_24hr']
df['visa_mastercard_ratio'] = df['visa_ind'] / df['mastercard_ind']
df['fraud_risk_score'] = df['cvv_mismatch_ratio'] * df['avs_mismatch_ratio'] * df['new_merch_ratio']
df['geo_temp_anomaly_score'] = df['dist_btw_swp_anomaly'] * df['time_since_last_trxn_anomaly']

# Save the updated dataset
df.save("path_to_updated_dataset.csv")






# Create interaction features
df['trxn_velocity_anomaly'] = df['tot_cnt_24hr'] / df['tot_cnt_30days']
df['zero_auth_time_gap_anomaly'] = df['mins_since_zero_auth'] / df['avg_mins_between_zero_auth_30days']
df['same_merc_cluster_score'] = df['same_merc_cnt_24hr'] / df['same_merc_cnt_30days']
df['cvv_mismatch_time_gap'] = df['time_since_cvv2_mismatch'] / df['avg_time_between_cvv_mismatch_30days']
df['avs_mismatch_time_gap'] = df['time_since_avs_mismatch'] / df['avg_time_between_avs_mismatch_30days']
df['exp_mismatch_ratio'] = df['cnt_exp_date_mismatch_24hr'] / df['tot_cnt_24hr']
df['high_risk_mcc_ratio'] = df['cnt_5310_5411_24hr'] / df['tot_cnt_24hr']
df['new_mcc_ratio'] = df['new_mcc_ind'] / df['tot_cnt_24hr']
df['merch_id_cluster_anomaly'] = df['same_merc_cnt_24hr'] / df['same_merc_cnt_30days']
df['cross_state_anomaly'] = df['diff_state_swp_24hr_ind'] / df['tot_cnt_24hr']
df['time_btw_swp_anomaly'] = df['time_btw_swp'] / df['avg_time_btw_swp_30days']
df['foreign_trxn_time_gap'] = df['time_since_last_foreign_cnp'] / df['avg_time_between_foreign_trxns_30days']
df['fraud_risk_score_2'] = df['cvv_mismatch_ratio'] * df['avs_mismatch_ratio'] * df['exp_mismatch_ratio'] * df['new_merch_ratio']
df['geo_mcc_anomaly_score'] = df['cross_state_anomaly'] * df['high_risk_mcc_ratio']

# Save the updated dataset
df.save("path_to_updated_dataset.csv")





# Create interaction features
df['zero_auth_to_auth_ratio'] = df['tot_zero_auth_cnt_24hr'] / df['tot_cnt_24hr']
df['small_amt_testing_score'] = df['cnt_sameamt_auth_6h'] / df['tot_cnt_24hr']
df['cvv_mismatch_after_zero_auth'] = df['cnt_cvv2_mismatch_24hr'] / df['tot_zero_auth_cnt_24hr']
df['trxn_burst_score'] = df['tot_cnt_30min'] / df['tot_cnt_24hr']
df['same_merc_velocity_anomaly'] = df['same_merc_cnt_30min'] / df['same_merc_cnt_24hr']
df['mcc_velocity_anomaly'] = df['same_mcc_cnt_30min'] / df['same_mcc_cnt_24hr']
df['dist_to_last_trxn_anomaly'] = df['dist_miles_btw_swp'] / df['avg_dist_btw_swp_30days']
df['cross_country_ratio'] = df['cnt_cntry_24hr'] / df['tot_cnt_24hr']
df['time_btw_geo_hops'] = df['time_btw_swp'] / df['dist_miles_btw_swp']
df['same_merc_to_term_ratio'] = df['cnt_same_mid_termid_24hr'] / df['same_merc_cnt_24hr']
df['new_merc_to_term_ratio'] = df['new_merch_id'] / df['cnt_same_mid_termid_24hr']
df['high_risk_merc_cluster'] = (df['same_merc_cnt_24hr'] / df['tot_cnt_24hr']) * df['high_risk_mcc_ratio']
df['fraud_risk_score_3'] = df['zero_auth_to_auth_ratio'] * df['small_amt_testing_score'] * df['cvv_mismatch_after_zero_auth'] * df['trxn_burst_score']
df['geo_velocity_anomaly_score'] = df['dist_to_last_trxn_anomaly'] * df['trxn_burst_score']

# Save the updated dataset
df.save("path_to_updated_dataset.csv")





# Create interaction features
df['swipe_amt_deviation'] = (df['amt_swipe_24hr'] - df['avg_swipe_amt_24hr']) / df['std_swipe_amt_24hr']
df['trxn_hour_anomaly'] = (df['trxn_hour'] - df['avg_trxn_hour_30days']) / df['std_trxn_hour_30days']
df['swipe_cnp_ratio_deviation'] = (df['swipe_cnp_ratio'] - df['avg_swipe_cnp_ratio_30days']) / df['std_swipe_cnp_ratio_30days']
df['high_risk_mcc_bayesian_interaction'] = df['hct_term_mcc_bayesian_encoded_5'] * df['hct_term_mcc_bayesian_encoded_7']
df['new_merch_risk_score'] = df['new_mech_ind'] * (1 + df['cnt_same_merc_id_24hr'] / df['tot_cnt_24hr'])
df['mcc_risk_weighted_cnt'] = df['same_mcc_cnt_24hr'] * df['hct_term_mcc_bayesian_encoded_5']
df['geo_dispersion_score'] = (df['cnt_dom_states_24hr'] / df['tot_cnt_24hr']) * df['dist_miles_btw_swp']
df['time_btw_swp_cvv_mismatch'] = df['time_since_last_swipe'] / df['time_since_cvv2_mismatch']
df['foreign_trxn_risk_score'] = df['foreign_ind'] * (1 + df['cnt_fore_swipe_24hr'] / df['tot_cnt_24hr'])
df['cvv_verf_magn_interaction'] = df['cvv_verf_magn_ind'] * (1 + df['cnt_cvv2_mismatch_24hr'] / df['tot_cnt_24hr'])
df['avs_mismatch_risk_score'] = df['avs_addr_mismatch_ind'] * (1 + df['cnt_avs_mismatch_24hr'] / df['tot_cnt_24hr'])
df['exp_mismatch_risk_score'] = df['exp_date_mismatch_ind'] * (1 + df['cnt_exp_date_mismatch_24hr'] / df['tot_cnt_24hr'])
df['fraud_risk_score_4'] = df['swipe_amt_deviation'] * df['trxn_hour_anomaly'] * df['high_risk_mcc_bayesian_interaction'] * df['new_merch_risk_score']
df['geo_temp_anomaly_score_2'] = df['geo_dispersion_score'] * df['time_btw_swp_cvv_mismatch']

# Save the updated dataset
df.save("path_to_updated_dataset.csv")










GPT


import h2o
import h2o.frame
from h2o.frame import H2OFrame

# Initialize H2O
h2o.init()

# Load dataset
df = h2o.import_file("your_fraud_data.csv")

# Creating advanced interaction features
df["risk_weighted_swipe_intensity"] = (df["cnt_swipe_24hr"] * (1 + df["cvv_verf_magn_ind"])) / (df["cnt_swipe_60days"] + 1)
df["mcc_velocity_risk"] = df["same_mcc_cnt_24hr"] / (df["same_mcc_cnt_60days"] + 1)
df["terminal_deviation"] = (df["cnt_same_mid_termid_24hr"] - df["cnt_same_mid_termid_60days"]) / (df["cnt_same_mid_termid_60days"] + 1)
df["swipe_dist_time_anomaly"] = df["dist_miles_btw_swp"] / (df["time_btw_swp"] + 1)
df["fraud_amt_scaling"] = (df["amt_swipe_24hr"] / (df["amt_swipe_60days"] + 1)) * (1 / (df["pct_swipe_amt_60days"] + 0.01))
df["avs_exp_mismatch_score"] = (df["cnt_avs_mismatch_24hr"] + df["cnt_exp_date_mismatch_24hr"]) / (df["cnt_avs_mismatch_60days"] + df["cnt_exp_date_mismatch_60days"] + 1)
df["card_testing_ratio"] = df["cnt_zero_auth_24hr"] / (df["cnt_zero_auth_60days"] + 1)
df["high_risk_merchant_score"] = df["cnt_5310_5411_24hr"] / (df["cnt_5310_5411_60days"] + 1)

# Save the enhanced dataset
h2o.export_file(df, "fraud_data_with_interaction_features.csv")
