import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xgb
from sklearn.model_selection import train_test_split

# üîπ Step 1: Generate Data
np.random.seed(42)
n_samples = 5000
scores = np.random.randint(0, 1000, n_samples)
df = pd.DataFrame({"score": scores})

# üîπ Step 2: Dynamically Compute Parameters for Uplift
mean_score = np.mean(scores)       # Dynamic central point (mean)
median_score = np.median(scores)   # Dynamic peak shift (median)
std_dev = np.std(scores)           # Standard deviation for scaling
variance = np.var(scores)          # Variance for decay control

base_uplift = 1 + (std_dev / mean_score)  # Adaptive base uplift factor
decay_factor = variance / (2 * mean_score)  # Decay factor based on variance

# üîπ Step 3: Dynamic Target Score Computation
df["target_score"] = df["score"] * (
    base_uplift + 0.3 * np.exp(-((df["score"] - median_score) ** 2) / decay_factor)
)

# üîπ Step 4: Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(df[["score"]], df["target_score"], test_size=0.2, random_state=42)

# üîπ Step 5: Train XGBoost Model
model = xgb.XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.05, objective="reg:squarederror")
model.fit(X_train, y_train)

# üîπ Step 6: Predict Adjusted Scores
df["adjusted_score"] = model.predict(df[["score"]])
df["adjusted_score"] = np.clip(df["adjusted_score"], 0, 999)  # Clip within valid range

# üîπ Step 7: Visualization
plt.figure(figsize=(10, 6))
sns.lineplot(x=df["score"], y=df["adjusted_score"], label="Adjusted Scores (XGBoost)", color="blue")
sns.lineplot(x=df["score"], y=df["score"], label="Original Scores", color="red", linestyle="dashed")
plt.xlabel("Original Score")
plt.ylabel("Adjusted Score")
plt.title("Score Shifting Using XGBoost (Dynamic Calculation)")
plt.legend()
plt.grid(True)
plt.show()

# üîπ Display Sample Results
print(df[["score", "adjusted_score"]].head(10))



















import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.special import expit
from sklearn.mixture import GaussianMixture

# üîπ Step 1: Generate Synthetic Data
np.random.seed(42)
n_samples = 5000
scores = np.random.randint(0, 1000, n_samples)  # Original scores (0-999)
probabilities = scores / 999  
labels = np.random.binomial(1, probabilities)  # Fraud labels (0/1)
amounts = np.random.uniform(10, 5000, n_samples)  # Transaction amounts

# Create DataFrame
df = pd.DataFrame({"score": scores, "label": labels, "amount": amounts})

# üîπ Step 2: Fit Gaussian Mixture Model (GMM)
gmm = GaussianMixture(n_components=3, random_state=42)  # 3 clusters (low, mid, high)
df["score_reshaped"] = df["score"].values.reshape(-1, 1)  # Reshape for GMM
gmm.fit(df[["score_reshaped"]])

# Get Gaussian components' means & standard deviations
means = gmm.means_.flatten()
stds = np.sqrt(gmm.covariances_).flatten()

# üîπ Step 3: Define Controlled Score Adjustment Function
def gmm_adjusted_score(score):
    """Smooth score uplift based on Gaussian components."""
    z_score = (score - np.mean(means)) / np.mean(stds)  # Normalize score
    uplift_factor = 1 + 0.5 * expit(z_score)  # Smooth sigmoid-based scaling
    new_score = score * uplift_factor  # Apply uplift
    return min(new_score, 999)  # Clip at 999

df["adjusted_score"] = df["score"].apply(gmm_adjusted_score)

# üîπ Step 4: Visualization - Score Shift
plt.figure(figsize=(10, 6))
sns.lineplot(x=df["score"], y=df["adjusted_score"], label="Adjusted Scores", color="blue")
sns.lineplot(x=df["score"], y=df["score"], label="Original Scores", color="red", linestyle="dashed")
plt.xlabel("Original Score")
plt.ylabel("Adjusted Score")
plt.title("Score Shifting Visualization (GMM-Based Adjustment)")
plt.legend()
plt.grid(True)
plt.show()

# üîπ Display Sample Results
print(df[["score", "adjusted_score"]].head(10))

















import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.special import expit  # Sigmoid function

# üîπ Step 1: Generate Synthetic Data
np.random.seed(42)
n_samples = 5000
scores = np.random.randint(0, 1000, n_samples)  # Original scores (0-999)
probabilities = scores / 999  
labels = np.random.binomial(1, probabilities)  # Fraud labels (0/1)
amounts = np.random.uniform(10, 5000, n_samples)  # Random transaction amounts

# Create DataFrame
df = pd.DataFrame({"score": scores, "label": labels, "amount": amounts})

# üîπ Step 2: Compute Score Percentiles (Statistical Backing)
df["percentile"] = df["score"].rank(pct=True)  # Get percentiles (0 to 1)

# üîπ Step 3: Define a Controlled Scaling Function
def controlled_scaling(percentile):
    """Smooth transformation that prevents extreme inflation."""
    base_alpha = 1.2 + 0.5 * (percentile**1.2)  # Gradual scaling increase
    sigmoid_adjustment = expit((percentile - 0.5) * 8)  # Smoothen transition
    return 1 + (base_alpha - 1) * sigmoid_adjustment  # Ensures smooth uplift

df["scaling_factor"] = df["percentile"].apply(controlled_scaling)  # Compute scaling

# üîπ Step 4: Adjust Scores with Clipping
df["adjusted_score"] = df["score"] * df["scaling_factor"]
df["adjusted_score"] = np.clip(df["adjusted_score"], df["score"], 999)  # Ensure monotonic increase

# üîπ Step 5: Visualization - Score Shift
plt.figure(figsize=(10, 6))
sns.lineplot(x=df["score"], y=df["adjusted_score"], label="Shifted Scores", color="blue")
sns.lineplot(x=df["score"], y=df["score"], label="Original Scores", color="red", linestyle="dashed")
plt.xlabel("Original Score")
plt.ylabel("Adjusted Score")
plt.title("Score Shifting Visualization")
plt.legend()
plt.grid(True)
plt.show()

# üîπ Display Sample Results
print(df[["score", "percentile", "scaling_factor", "adjusted_score"]].head(10))

















import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# üîπ Step 1: Generate Synthetic Data
np.random.seed(42)
n_samples = 5000
scores = np.random.randint(0, 1000, n_samples)  # Original scores (0-999)
probabilities = scores / 999  
labels = np.random.binomial(1, probabilities)  # Fraud labels (0/1)
amounts = np.random.uniform(10, 5000, n_samples)  # Random transaction amounts

# Create DataFrame
df = pd.DataFrame({"score": scores, "label": labels, "amount": amounts})

# üîπ Step 2: Compute Score Percentiles (Statistical Backing)
df["percentile"] = df["score"].rank(pct=True)  # Percentile-based scaling

# üîπ Step 3: Define a Smooth Scaling Function (Polynomial-Based)
def smooth_scaling(percentile):
    """Polynomial-based transformation ensuring smooth upward shift."""
    return 1 + 0.6 * (percentile**0.8)  # Adjust exponent for gradual increase

df["scaling_factor"] = df["percentile"].apply(smooth_scaling)  # Compute scaling factor

# üîπ Step 4: Adjust Scores Based on Scaling Factor
df["adjusted_score"] = df["score"] * df["scaling_factor"]
df["adjusted_score"] = np.clip(df["adjusted_score"], 0, 999)  # Ensure within [0,999]

# üîπ Step 5: Visualization - Score Shift
plt.figure(figsize=(10, 6))
sns.lineplot(x=df["score"], y=df["adjusted_score"], label="Shifted Scores", color="blue")
sns.lineplot(x=df["score"], y=df["score"], label="Original Scores", color="red", linestyle="dashed")
plt.xlabel("Original Score")
plt.ylabel("Adjusted Score")
plt.title("Score Shifting Visualization")
plt.legend()
plt.grid(True)
plt.show()

# üîπ Display Sample Results
print(df[["score", "percentile", "scaling_factor", "adjusted_score"]].head(10))


















import numpy as np
import pandas as pd
from scipy.interpolate import interp1d
from scipy.special import expit
import matplotlib.pyplot as plt

# üîπ Generate synthetic fraud detection dataset
np.random.seed(42)
n_samples = 5000
scores = np.random.randint(0, 1000, n_samples)
amounts = np.random.exponential(scale=500, size=n_samples)
probabilities = scores / 999
labels = np.random.binomial(1, probabilities)

# üîπ Create DataFrame
df = pd.DataFrame({"score": scores, "amount": amounts, "label": labels})

# üîπ Compute Percentiles (Ensuring Uniqueness)
percentiles = np.unique(np.percentile(df["score"], np.linspace(5, 95, 20)))  # Avoids extreme tails
scaling_factors = np.linspace(1.05, 1.25, len(percentiles))  # More controlled range

# üîπ Create smooth percentile-based scaling function
scaling_function = interp1d(percentiles, scaling_factors, kind="linear", fill_value="extrapolate")

# üîπ Adaptive Scaling Factor with Soft Constraints
def adaptive_alpha(score):
    base_alpha = scaling_function(score)
    
    # Apply smooth sigmoid-like transformation to prevent excessive inflation
    adjusted_alpha = 1 + (base_alpha - 1) * expit((score - 500) / 150)  
    return adjusted_alpha

df["alpha"] = df["score"].apply(adaptive_alpha)

# üîπ Step 1: Controlled Inflation
df["inflated_score"] = df["score"] * df["alpha"]

# üîπ Step 2: Quantile-Based Normalization (Ensures shifts follow actual distribution)
sorted_scores = np.sort(df["score"].values)
quantiles = np.linspace(0, 1, len(sorted_scores))
quantile_function = interp1d(sorted_scores, quantiles, kind="linear", fill_value="extrapolate")

df["normalized_score"] = quantile_function(df["inflated_score"]) * 999  # Rescale to [0,999]

# üîπ Step 3: Soft Adjustment with Exponential Control
df["final_score"] = 999 * expit((df["normalized_score"] / 999 - 0.5) * 1.05)

# üîπ Step 4: Clip Values to Maintain [0, 999] Range
df["final_score"] = np.clip(df["final_score"], df["score"], 999)  # Prevent excessive drops

# üîπ Visualization: Before vs After Score Shift
plt.figure(figsize=(10, 5))
plt.hist(df["score"], bins=30, alpha=0.5, label="Original Scores", color="blue")
plt.hist(df["final_score"], bins=30, alpha=0.5, label="Final Adjusted Scores", color="red")
plt.xlabel("Score")
plt.ylabel("Frequency")
plt.title("Score Distribution Before & After Adjustment")
plt.legend()
plt.show()

# üîπ Display Impact
print(df[["score", "alpha", "final_score"]].head(10))















import numpy as np
import pandas as pd
from scipy.interpolate import interp1d
from scipy.special import expit
import matplotlib.pyplot as plt

# üîπ Generate synthetic fraud detection dataset
np.random.seed(42)
n_samples = 5000
scores = np.random.randint(0, 1000, n_samples)
amounts = np.random.exponential(scale=500, size=n_samples)
probabilities = scores / 999
labels = np.random.binomial(1, probabilities)

# üîπ Create DataFrame
df = pd.DataFrame({"score": scores, "amount": amounts, "label": labels})

# üîπ Compute Unique Percentile Values
percentiles = np.unique(np.percentile(df["score"], np.linspace(0, 100, 20)))  # Ensure uniqueness
scaling_factors = np.linspace(1.05, 1.35, len(percentiles))  # Controlled adaptive increase

# üîπ Create smooth percentile-based scaling function
scaling_function = interp1d(percentiles, scaling_factors, kind="linear", fill_value="extrapolate")

# üîπ Apply Adaptive Scaling Factor
df["alpha"] = df["score"].apply(lambda x: scaling_function(x))

# üîπ Step 1: Controlled Inflation
df["inflated_score"] = df["score"] * df["alpha"]

# üîπ Step 2: Quantile-Based Normalization (Ensures shifts follow actual distribution)
sorted_scores = np.sort(df["score"].values)
quantiles = np.linspace(0, 1, len(sorted_scores))
quantile_function = interp1d(sorted_scores, quantiles, kind="linear", fill_value="extrapolate")

df["normalized_score"] = quantile_function(df["inflated_score"]) * 999  # Rescale to [0,999]

# üîπ Step 3: Apply Soft Logistic Smoothing (Final Stabilization)
df["final_score"] = 999 * expit((df["normalized_score"] / 999 - 0.5) * 1.1)

# üîπ Clip values to maintain [0, 999] range
df["final_score"] = np.clip(df["final_score"], 0, 999)

# üîπ Visualization: Before vs After Score Shift
plt.figure(figsize=(10, 5))
plt.hist(df["score"], bins=30, alpha=0.5, label="Original Scores", color="blue")
plt.hist(df["final_score"], bins=30, alpha=0.5, label="Final Adjusted Scores", color="red")
plt.xlabel("Score")
plt.ylabel("Frequency")
plt.title("Score Distribution Before & After Adjustment")
plt.legend()
plt.show()

# üîπ Display Impact
print(df[["score", "alpha", "final_score"]].head(10))


















import numpy as np
import pandas as pd
from scipy.interpolate import interp1d
from scipy.special import expit
import matplotlib.pyplot as plt

# üîπ Generate synthetic fraud detection dataset
np.random.seed(42)
n_samples = 5000
scores = np.random.randint(0, 1000, n_samples)
amounts = np.random.exponential(scale=500, size=n_samples)  
probabilities = scores / 999  
labels = np.random.binomial(1, probabilities)

# üîπ Create DataFrame
df = pd.DataFrame({"score": scores, "amount": amounts, "label": labels})

# üîπ Compute all percentiles dynamically
percentile_values = np.percentile(df["score"], np.arange(0, 101, 5))  # Every 5% step

# üîπ Define dynamic alpha scaling based on percentiles
alpha_values = 1 + 0.4 * np.linspace(0, 1, len(percentile_values))  # Smooth Œ± increase

# üîπ Create a smooth function to assign alpha values based on score percentiles
alpha_function = interp1d(percentile_values, alpha_values, kind="cubic", fill_value="extrapolate")

# üîπ Apply adaptive Œ± to each score
df["alpha"] = df["score"].apply(lambda x: alpha_function(x))

# üîπ Apply Transformation: Scale + Logistic Smoothing
df["adjusted_score"] = df["score"] * df["alpha"]  # Direct Scaling
df["adjusted_score"] = 999 * expit((df["adjusted_score"] / 999 - 0.5) * 1.2)  # Smooth Shift

# üîπ Clip values to ensure scores remain within valid range [0, 999]
df["adjusted_score"] = np.clip(df["adjusted_score"], 0, 999)

# üîπ Visualization: Before vs After Score Shift
plt.figure(figsize=(10, 5))
plt.hist(df["score"], bins=30, alpha=0.5, label="Original Scores", color="blue")
plt.hist(df["adjusted_score"], bins=30, alpha=0.5, label="Adjusted Scores", color="red")
plt.xlabel("Score")
plt.ylabel("Frequency")
plt.title("Score Distribution Before & After Adjustment")
plt.legend()
plt.show()

# üîπ Display Impact
print(df[["score", "alpha", "adjusted_score"]].head(10))








üîπ Step 1: Compute Adaptive Scaling Factor (Œ±)
We will define Œ± dynamically based on the z-score of the original scores, ensuring that the shift is justified statistically.

python
Copy
Edit
import numpy as np
import pandas as pd
from scipy.special import expit

# üîπ Generate synthetic fraud detection dataset
np.random.seed(42)
n_samples = 5000
scores = np.random.randint(0, 1000, n_samples)
amounts = np.random.exponential(scale=500, size=n_samples)  # Sample transaction amounts
probabilities = scores / 999  
labels = np.random.binomial(1, probabilities)

# üîπ Create DataFrame
df = pd.DataFrame({"score": scores, "amount": amounts, "label": labels})

# üîπ Compute statistical properties
mu, sigma = df["score"].mean(), df["score"].std()
q10, q50, q90 = np.percentile(df["score"], [10, 50, 90])  # Deciles for adaptive scaling

# üîπ Define Adaptive Scaling Factor (Œ±) using z-score
def compute_alpha(score):
    z_score = (score - mu) / sigma  # Standardized score
    return 1 + 0.5 * expit(z_score)  # Sigmoid ensures gradual increase

df["alpha"] = df["score"].apply(compute_alpha)
üîπ Step 2: Apply Hybrid Score Transformation
We will use a hybrid transformation that:
‚úÖ Uses multiplication-based scaling to ensure statistical justification.
‚úÖ Uses logistic scaling (expit) to keep shifts smooth & bounded.

python
Copy
Edit
# üîπ Step 2: Hybrid Transformation
df["adjusted_score"] = df["score"] * df["alpha"]  # Direct Scaling
df["adjusted_score"] = 999 * expit((df["adjusted_score"] / 999 - 0.5) * 1.2)  # Logistic Scaling

# üîπ Clip values to ensure range [0, 999]
df["adjusted_score"] = np.clip(df["adjusted_score"], 0, 999)

# üîπ Display impact
print(df[["score", "alpha", "adjusted_score"]].head(10))
‚úÖ Why This Works Best
1Ô∏è‚É£ Low Scores (<500) Now Increase Gradually

The adaptive Œ± (scaling factor) ensures they don‚Äôt drop but instead get a justified uplift.

2Ô∏è‚É£ High Scores (‚â•500) Still Get a Strong Boost But Stay Controlled

The logistic transformation prevents extreme jumps while ensuring better ranking.

3Ô∏è‚É£ Statistical Stability & Explainability

The approach is backed by z-score scaling and percentile-based adaptive uplift.

















import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import QuantileTransformer

# üîπ Generate synthetic dataset (columns: score, amount, label)
np.random.seed(42)
n_samples = 5000
df = pd.DataFrame({
    "score": np.random.randint(0, 1000, n_samples),  # Scores (0-999)
    "amount": np.random.exponential(scale=500, size=n_samples),  # Transaction Amounts
    "label": np.random.binomial(1, 0.1, n_samples)  # Fraud Labels (10% fraud)
})

# üîπ Step 1: Compute Statistical Alpha (Data-Driven)
percentile_90 = np.percentile(df["score"], 90)  # 90th percentile
percentile_10 = np.percentile(df["score"], 10)  # 10th percentile
mean_score = df["score"].mean()
std_score = df["score"].std()

# Compute Œ± dynamically: Controls how much scores are lifted
alpha = np.log(percentile_90 / percentile_10) / np.log(mean_score / std_score)
alpha = max(1.1, min(alpha, 2.0))  # Keep Œ± in reasonable range (1.1 to 2.0)

print(f"üîπ Computed Alpha: {alpha:.4f}")

# üîπ Step 2: Apply Quantile Transformation
qt = QuantileTransformer(output_distribution="uniform")
df["score_uniform"] = qt.fit_transform(df[["score"]])  # Maps to [0,1]

# üîπ Step 3: Exponential Scaling using Dynamic Alpha
df["shifted_score"] = 999 * (df["score_uniform"] ** alpha)

# üîπ Step 4: Ensure scores stay within [0, 999]
df["shifted_score"] = np.clip(df["shifted_score"], 0, 999)

# üîπ Visualization - Score Shift
plt.figure(figsize=(12, 6))
sns.histplot(df["score"], bins=30, label="Original Scores", color="blue", kde=True, alpha=0.5)
sns.histplot(df["shifted_score"], bins=30, label="Shifted Scores", color="red", kde=True, alpha=0.5)
plt.legend()
plt.title("Distribution Shift: Original vs. Transformed Scores")
plt.xlabel("Score")
plt.ylabel("Density")
plt.show()

# üîπ Scatter Plot: Original vs. Shifted Scores
plt.figure(figsize=(12, 5))
plt.scatter(df["score"], df["shifted_score"], alpha=0.5, color="red", label="Shifted Score")
plt.plot([0, 1000], [0, 1000], "--", color="black", alpha=0.7, label="Reference Line")
plt.xlabel("Original Score")
plt.ylabel("Transformed Score")
plt.legend()
plt.title("Quantile-Based Score Shifting: Before vs. After")
plt.show()

# üîπ Print sample output
print(df[["score", "shifted_score", "amount", "label"]].head())








import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import QuantileTransformer

# üîπ Generate synthetic dataset (columns: score, amount, label)
np.random.seed(42)
n_samples = 5000
df = pd.DataFrame({
    "score": np.random.randint(0, 1000, n_samples),  # Scores (0-999)
    "amount": np.random.exponential(scale=500, size=n_samples),  # Transaction Amounts
    "label": np.random.binomial(1, 0.1, n_samples)  # Fraud Labels (10% fraud)
})

# üîπ Step 1: Quantile Transformation to Uniform Distribution
qt = QuantileTransformer(output_distribution="uniform")
df["score_uniform"] = qt.fit_transform(df[["score"]])  # Maps to [0,1]

# üîπ Step 2: Exponential Scaling to Shift Scores Upwards
alpha = 1.5  # Controls how aggressively scores increase
df["shifted_score"] = 999 * (df["score_uniform"] ** alpha)

# üîπ Step 3: Ensure scores stay within [0, 999]
df["shifted_score"] = np.clip(df["shifted_score"], 0, 999)

# üîπ Visualization - Score Shift
plt.figure(figsize=(12, 6))
sns.histplot(df["score"], bins=30, label="Original Scores", color="blue", kde=True, alpha=0.5)
sns.histplot(df["shifted_score"], bins=30, label="Shifted Scores", color="red", kde=True, alpha=0.5)
plt.legend()
plt.title("Distribution Shift: Original vs. Transformed Scores")
plt.xlabel("Score")
plt.ylabel("Density")
plt.show()

# üîπ Scatter Plot: Original vs. Shifted Scores
plt.figure(figsize=(12, 5))
plt.scatter(df["score"], df["shifted_score"], alpha=0.5, color="red", label="Shifted Score")
plt.plot([0, 1000], [0, 1000], "--", color="black", alpha=0.7, label="Reference Line")
plt.xlabel("Original Score")
plt.ylabel("Transformed Score")
plt.legend()
plt.title("Quantile-Based Score Shifting: Before vs. After")
plt.show()

# üîπ Print sample output
print(df[["score", "shifted_score", "amount", "label"]].head())
















If your goal is to shift all scores to higher bins, you need a transformation that pushes scores upward while preserving relative ranking and variance balance.

üöÄ Key Approach: Non-Linear Score Transformation
We will apply a Monotonic Increasing Transformation to ensure:

All scores shift to higher bins

Relative ranking is preserved (higher original scores still remain higher)

Transformation is independent of distribution

No compression at the higher end

üî∑ Advanced Transformation Methods
1Ô∏è‚É£ Exponential Scaling (ùëì(ùë•) = A * exp(B * x))
This method amplifies small differences in lower scores but ensures all values shift upwards.

We set A and B such that the max score remains close to 999 while lower scores rise significantly.

2Ô∏è‚É£ Logistic Sigmoid Transformation (ùëì(ùë•) = 999 / (1 + exp(-ùõº(ùë• - ùõΩ))))
This method compresses lower values and stretches higher values toward 999.

ùõº controls steepness, ùõΩ centers the transformation.

3Ô∏è‚É£ Power-Law Transformation (ùëì(ùë•) = 999 * (x / 999)^Œ≥)
This ensures all scores increase while preserving relative rankings.

Œ≥ > 1 pushes all values upwards.

üî∑ Python Implementation
python
Copy
Edit
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Generate synthetic scores (range 0-999)
np.random.seed(42)
n_samples = 5000
scores = np.random.randint(0, 1000, n_samples)

# Exponential Transformation
A, B = 500, 0.002  # Tuned parameters
exp_transformed = A * np.exp(B * scores)

# Logistic Sigmoid Transformation
alpha, beta = 0.02, 500  # Tuned for smooth shift
logistic_transformed = 999 / (1 + np.exp(-alpha * (scores - beta)))

# Power-Law Transformation
gamma = 1.3  # Higher gamma shifts scores upward
power_transformed = 999 * (scores / 999) ** gamma

# Store in DataFrame
df = pd.DataFrame({"original_score": scores, 
                   "exp_transformed": exp_transformed, 
                   "logistic_transformed": logistic_transformed, 
                   "power_transformed": power_transformed})

# Ensure scores remain within [0, 999]
df = df.clip(0, 999)

# üî∑ Visualization - Shifted Scores
plt.figure(figsize=(12, 6))
sns.histplot(df["original_score"], bins=30, label="Original Score", color="blue", kde=True, alpha=0.5)
sns.histplot(df["exp_transformed"], bins=30, label="Exponential", color="red", kde=True, alpha=0.5)
sns.histplot(df["logistic_transformed"], bins=30, label="Logistic", color="green", kde=True, alpha=0.5)
sns.histplot(df["power_transformed"], bins=30, label="Power-Law", color="purple", kde=True, alpha=0.5)
plt.legend()
plt.title("Distribution Shift: Original vs. Transformed Scores")
plt.xlabel("Score")
plt.ylabel("Density")
plt.show()

# üî∑ Scatter Plot: Original vs. Shifted Scores
plt.figure(figsize=(12, 5))
plt.scatter(df["original_score"], df["exp_transformed"], alpha=0.5, label="Exponential", color="red")
plt.scatter(df["original_score"], df["logistic_transformed"], alpha=0.5, label="Logistic", color="green")
plt.scatter(df["original_score"], df["power_transformed"], alpha=0.5, label="Power-Law", color="purple")
plt.plot([0, 1000], [0, 1000], "--", color="black", alpha=0.7, label="Reference Line")
plt.xlabel("Original Score")
plt.ylabel("Transformed Score")
plt.legend()
plt.title("Score Transformation: Before vs. After")
plt.show()
üî∑ Explanation of Transformations
Transformation	Behavior	Best For
Exponential Scaling	Lower values get amplified significantly	Good for aggressive shifting
Logistic Sigmoid	Lower scores rise quickly, but higher scores approach 999 smoothly	Good for stable, controlled shifts
Power-Law	All values shift up, but keeps shape similar	Best for balanced shifting
üî∑ Which One Should You Use?
‚úÖ If you want a very aggressive shift: Use Exponential Scaling
‚úÖ If you want a smooth, natural shift: Use Logistic Sigmoid
‚úÖ If you want balanced movement without distortion: Use Power-Law

üöÄ Let me know if you need a custom version based on your real data distribution!

















import numpy as np
import pandas as pd
from scipy.special import expit  # Sigmoid function
from scipy.stats import gaussian_kde

# Generate synthetic fraud detection dataset
np.random.seed(42)
n_samples = 5000
scores = np.random.randint(0, 1000, n_samples)
probabilities = scores / 999  
labels = np.random.binomial(1, probabilities)

# Create DataFrame
df = pd.DataFrame({"score": scores, "label": labels})

# Custom Sigmoid Score Transformation
def sigmoid_transform(score, a=0.02, b=-5):
    return expit(a * score + b)

df["sigmoid_scaled"] = sigmoid_transform(df["score"])

# Tail Boosting Using Fraud Quantiles
gamma = 1.2  
df["tail_boosted"] = df["score"] ** gamma  

# Fraud Density Adjustment Using KDE
fraud_scores = df[df["label"] == 1]["score"]
kde = gaussian_kde(fraud_scores)
df["density_adjusted"] = kde(df["score"])

# Adaptive Mean Reversion (Bayesian Smoothing)
fraud_mean = df[df["label"] == 1]["score"].mean()
lambda_factor = 0.7  
df["bayesian_smoothed"] = lambda_factor * df["score"] + (1 - lambda_factor) * fraud_mean

# Dynamic Score Re-Weighting (Gradient-Based)
eta = 0.1
gradient = np.gradient(df["score"])
df["gradient_scaled"] = df["score"] + eta * gradient

# Display results
print(df.head())





import numpy as np
import pandas as pd
from scipy.optimize import minimize

# Generate synthetic fraud detection dataset
np.random.seed(42)
n_samples = 5000
scores = np.random.randint(0, 1000, n_samples)
probabilities = scores / 999  
labels = np.random.binomial(1, probabilities)

# Create DataFrame
df = pd.DataFrame({"score": scores, "label": labels})

# Compute means and variances
fraud_scores = df[df["label"] == 1]["score"]
all_scores = df["score"]

mean_fraud = fraud_scores.mean()
var_fraud = fraud_scores.var()
var_all = all_scores.var()

# 1Ô∏è‚É£ Variance Ratio Method
lambda_var = var_all / (var_all + var_fraud)

# 2Ô∏è‚É£ Maximum Likelihood Estimation (MLE)
def mle_loss(lambda_factor):
    smoothed_scores = lambda_factor * df["score"] + (1 - lambda_factor) * mean_fraud
    residuals = df["score"] - smoothed_scores
    return np.var(residuals)  # Minimize residual variance

lambda_mle = minimize(mle_loss, x0=0.5, bounds=[(0, 1)]).x[0]

# 3Ô∏è‚É£ Bayesian Smoothing with Optimal Œª
df["bayesian_smooth_var"] = lambda_var * df["score"] + (1 - lambda_var) * mean_fraud
df["bayesian_smooth_mle"] = lambda_mle * df["score"] + (1 - lambda_mle) * mean_fraud

# Display results
print(f"Optimal Lambda (Variance Method): {lambda_var:.4f}")
print(f"Optimal Lambda (MLE): {lambda_mle:.4f}")
print(df.head())




import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.optimize import minimize

# Generate synthetic fraud detection dataset
np.random.seed(42)
n_samples = 5000
scores = np.random.randint(0, 1000, n_samples)
probabilities = scores / 999  
labels = np.random.binomial(1, probabilities)

# Create DataFrame
df = pd.DataFrame({"score": scores, "label": labels})

# Compute means and variances
fraud_scores = df[df["label"] == 1]["score"]
all_scores = df["score"]

mean_fraud = fraud_scores.mean()
var_fraud = fraud_scores.var()
var_all = all_scores.var()

# 1Ô∏è‚É£ Variance Ratio Method
lambda_var = var_all / (var_all + var_fraud)

# 2Ô∏è‚É£ Maximum Likelihood Estimation (MLE)
def mle_loss(lambda_factor):
    smoothed_scores = lambda_factor * df["score"] + (1 - lambda_factor) * mean_fraud
    residuals = df["score"] - smoothed_scores
    return np.var(residuals)  # Minimize residual variance

lambda_mle = minimize(mle_loss, x0=0.5, bounds=[(0, 1)]).x[0]

# 3Ô∏è‚É£ Bayesian Smoothing with Optimal Œª
df["bayesian_smooth_var"] = lambda_var * df["score"] + (1 - lambda_var) * mean_fraud
df["bayesian_smooth_mle"] = lambda_mle * df["score"] + (1 - lambda_mle) * mean_fraud

# Display results
print(f"Optimal Lambda (Variance Method): {lambda_var:.4f}")
print(f"Optimal Lambda (MLE): {lambda_mle:.4f}")

# üî∑ Visualization 1: Histogram of Original vs. Smoothed Scores
plt.figure(figsize=(12, 5))
sns.histplot(df["score"], bins=30, color="blue", label="Original Scores", kde=True, alpha=0.5)
sns.histplot(df["bayesian_smooth_var"], bins=30, color="green", label="Smoothed (Variance Method)", kde=True, alpha=0.5)
sns.histplot(df["bayesian_smooth_mle"], bins=30, color="red", label="Smoothed (MLE Method)", kde=True, alpha=0.5)
plt.legend()
plt.title("Histogram: Original vs. Bayesian Smoothed Scores")
plt.xlabel("Score")
plt.ylabel("Density")
plt.show()

# üî∑ Visualization 2: Scatter Plot - Original vs. Smoothed Scores
plt.figure(figsize=(12, 5))
plt.scatter(df["score"], df["bayesian_smooth_var"], alpha=0.5, label="Variance Method", color="green")
plt.scatter(df["score"], df["bayesian_smooth_mle"], alpha=0.5, label="MLE Method", color="red")
plt.plot([0, 1000], [0, 1000], "--", color="black", alpha=0.7, label="Reference Line")
plt.xlabel("Original Score")
plt.ylabel("Smoothed Score")
plt.legend()
plt.title("Scatter Plot: Score Distribution Before & After Smoothing")
plt.show()

# üî∑ Visualization 3: Lambda Impact - Smoothed Scores for Different Lambda Values
lambda_values = np.linspace(0, 1, 10)
plt.figure(figsize=(12, 5))
for lam in lambda_values:
    smoothed_scores = lam * df["score"] + (1 - lam) * mean_fraud
    sns.kdeplot(smoothed_scores, label=f"Œª = {lam:.2f}")

plt.legend()
plt.title("Effect of Different Lambda Values on Score Smoothing")
plt.xlabel("Smoothed Score")
plt.ylabel("Density")
plt.show()








Here's a visualization of different Œ∑ (eta) methods to compare their effects on score re-weighting. The plots will show:

1Ô∏è‚É£ Original Scores vs. Adjusted Scores
2Ô∏è‚É£ Score Differences Across Methods

üìå Python Code for Visualization
python
Copy
Edit
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.optimize import minimize

# Generate synthetic fraud detection dataset
np.random.seed(42)
n_samples = 5000
scores = np.random.randint(0, 1000, n_samples)
probabilities = scores / 999  
labels = np.random.binomial(1, probabilities)

# Create DataFrame
df = pd.DataFrame({"score": scores, "label": labels})

# Compute score gradient
gradient = np.gradient(df["score"])

# 1Ô∏è‚É£ Mean Absolute Gradient Method
eta_mag = 1 / np.mean(np.abs(gradient))

# 2Ô∏è‚É£ Variance Ratio Method
fraud_scores = df[df["label"] == 1]["score"]
std_fraud = fraud_scores.std()
std_all = df["score"].std()
eta_var = std_fraud / std_all

# 3Ô∏è‚É£ Maximum Likelihood Estimation (MLE)
def mle_loss(eta_factor):
    adjusted_scores = df["score"] + eta_factor * gradient
    return np.var(adjusted_scores - df["score"])  # Minimize variance shift

eta_mle = minimize(mle_loss, x0=0.1, bounds=[(0, 1)]).x[0]

# Apply computed eta values
df["gradient_scaled_mag"] = df["score"] + eta_mag * gradient
df["gradient_scaled_var"] = df["score"] + eta_var * gradient
df["gradient_scaled_mle"] = df["score"] + eta_mle * gradient

# üìä Plot Original vs. Adjusted Scores
plt.figure(figsize=(12, 6))
plt.plot(df["score"], label="Original Score", color="black", alpha=0.6)
plt.plot(df["gradient_scaled_mag"], label=f"Mean Abs Gradient (Œ∑={eta_mag:.4f})", linestyle="--")
plt.plot(df["gradient_scaled_var"], label=f"Variance-Based (Œ∑={eta_var:.4f})", linestyle="-.")
plt.plot(df["gradient_scaled_mle"], label=f"MLE Optimized (Œ∑={eta_mle:.4f})", linestyle="dotted")
plt.legend()
plt.title("Comparison of Score Re-Weighting Methods")
plt.xlabel("Transaction Index")
plt.ylabel("Adjusted Score")
plt.show()

# üìä Plot Score Differences Across Methods
plt.figure(figsize=(12, 6))
plt.plot(df["gradient_scaled_mag"] - df["score"], label="Mean Abs Gradient Adjustment", linestyle="--")
plt.plot(df["gradient_scaled_var"] - df["score"], label="Variance-Based Adjustment", linestyle="-.")
plt.plot(df["gradient_scaled_mle"] - df["score"], label="MLE Optimized Adjustment", linestyle="dotted")
plt.legend()
plt.title("Score Adjustments Across Different Œ∑ Methods")
plt.xlabel("Transaction Index")
plt.ylabel("Score Adjustment")
plt.show()




