üîπ Step 1: Compute Adaptive Scaling Factor (Œ±)
We will define Œ± dynamically based on the z-score of the original scores, ensuring that the shift is justified statistically.

python
Copy
Edit
import numpy as np
import pandas as pd
from scipy.special import expit

# üîπ Generate synthetic fraud detection dataset
np.random.seed(42)
n_samples = 5000
scores = np.random.randint(0, 1000, n_samples)
amounts = np.random.exponential(scale=500, size=n_samples)  # Sample transaction amounts
probabilities = scores / 999  
labels = np.random.binomial(1, probabilities)

# üîπ Create DataFrame
df = pd.DataFrame({"score": scores, "amount": amounts, "label": labels})

# üîπ Compute statistical properties
mu, sigma = df["score"].mean(), df["score"].std()
q10, q50, q90 = np.percentile(df["score"], [10, 50, 90])  # Deciles for adaptive scaling

# üîπ Define Adaptive Scaling Factor (Œ±) using z-score
def compute_alpha(score):
    z_score = (score - mu) / sigma  # Standardized score
    return 1 + 0.5 * expit(z_score)  # Sigmoid ensures gradual increase

df["alpha"] = df["score"].apply(compute_alpha)
üîπ Step 2: Apply Hybrid Score Transformation
We will use a hybrid transformation that:
‚úÖ Uses multiplication-based scaling to ensure statistical justification.
‚úÖ Uses logistic scaling (expit) to keep shifts smooth & bounded.

python
Copy
Edit
# üîπ Step 2: Hybrid Transformation
df["adjusted_score"] = df["score"] * df["alpha"]  # Direct Scaling
df["adjusted_score"] = 999 * expit((df["adjusted_score"] / 999 - 0.5) * 1.2)  # Logistic Scaling

# üîπ Clip values to ensure range [0, 999]
df["adjusted_score"] = np.clip(df["adjusted_score"], 0, 999)

# üîπ Display impact
print(df[["score", "alpha", "adjusted_score"]].head(10))
‚úÖ Why This Works Best
1Ô∏è‚É£ Low Scores (<500) Now Increase Gradually

The adaptive Œ± (scaling factor) ensures they don‚Äôt drop but instead get a justified uplift.

2Ô∏è‚É£ High Scores (‚â•500) Still Get a Strong Boost But Stay Controlled

The logistic transformation prevents extreme jumps while ensuring better ranking.

3Ô∏è‚É£ Statistical Stability & Explainability

The approach is backed by z-score scaling and percentile-based adaptive uplift.

















import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import QuantileTransformer

# üîπ Generate synthetic dataset (columns: score, amount, label)
np.random.seed(42)
n_samples = 5000
df = pd.DataFrame({
    "score": np.random.randint(0, 1000, n_samples),  # Scores (0-999)
    "amount": np.random.exponential(scale=500, size=n_samples),  # Transaction Amounts
    "label": np.random.binomial(1, 0.1, n_samples)  # Fraud Labels (10% fraud)
})

# üîπ Step 1: Compute Statistical Alpha (Data-Driven)
percentile_90 = np.percentile(df["score"], 90)  # 90th percentile
percentile_10 = np.percentile(df["score"], 10)  # 10th percentile
mean_score = df["score"].mean()
std_score = df["score"].std()

# Compute Œ± dynamically: Controls how much scores are lifted
alpha = np.log(percentile_90 / percentile_10) / np.log(mean_score / std_score)
alpha = max(1.1, min(alpha, 2.0))  # Keep Œ± in reasonable range (1.1 to 2.0)

print(f"üîπ Computed Alpha: {alpha:.4f}")

# üîπ Step 2: Apply Quantile Transformation
qt = QuantileTransformer(output_distribution="uniform")
df["score_uniform"] = qt.fit_transform(df[["score"]])  # Maps to [0,1]

# üîπ Step 3: Exponential Scaling using Dynamic Alpha
df["shifted_score"] = 999 * (df["score_uniform"] ** alpha)

# üîπ Step 4: Ensure scores stay within [0, 999]
df["shifted_score"] = np.clip(df["shifted_score"], 0, 999)

# üîπ Visualization - Score Shift
plt.figure(figsize=(12, 6))
sns.histplot(df["score"], bins=30, label="Original Scores", color="blue", kde=True, alpha=0.5)
sns.histplot(df["shifted_score"], bins=30, label="Shifted Scores", color="red", kde=True, alpha=0.5)
plt.legend()
plt.title("Distribution Shift: Original vs. Transformed Scores")
plt.xlabel("Score")
plt.ylabel("Density")
plt.show()

# üîπ Scatter Plot: Original vs. Shifted Scores
plt.figure(figsize=(12, 5))
plt.scatter(df["score"], df["shifted_score"], alpha=0.5, color="red", label="Shifted Score")
plt.plot([0, 1000], [0, 1000], "--", color="black", alpha=0.7, label="Reference Line")
plt.xlabel("Original Score")
plt.ylabel("Transformed Score")
plt.legend()
plt.title("Quantile-Based Score Shifting: Before vs. After")
plt.show()

# üîπ Print sample output
print(df[["score", "shifted_score", "amount", "label"]].head())








import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import QuantileTransformer

# üîπ Generate synthetic dataset (columns: score, amount, label)
np.random.seed(42)
n_samples = 5000
df = pd.DataFrame({
    "score": np.random.randint(0, 1000, n_samples),  # Scores (0-999)
    "amount": np.random.exponential(scale=500, size=n_samples),  # Transaction Amounts
    "label": np.random.binomial(1, 0.1, n_samples)  # Fraud Labels (10% fraud)
})

# üîπ Step 1: Quantile Transformation to Uniform Distribution
qt = QuantileTransformer(output_distribution="uniform")
df["score_uniform"] = qt.fit_transform(df[["score"]])  # Maps to [0,1]

# üîπ Step 2: Exponential Scaling to Shift Scores Upwards
alpha = 1.5  # Controls how aggressively scores increase
df["shifted_score"] = 999 * (df["score_uniform"] ** alpha)

# üîπ Step 3: Ensure scores stay within [0, 999]
df["shifted_score"] = np.clip(df["shifted_score"], 0, 999)

# üîπ Visualization - Score Shift
plt.figure(figsize=(12, 6))
sns.histplot(df["score"], bins=30, label="Original Scores", color="blue", kde=True, alpha=0.5)
sns.histplot(df["shifted_score"], bins=30, label="Shifted Scores", color="red", kde=True, alpha=0.5)
plt.legend()
plt.title("Distribution Shift: Original vs. Transformed Scores")
plt.xlabel("Score")
plt.ylabel("Density")
plt.show()

# üîπ Scatter Plot: Original vs. Shifted Scores
plt.figure(figsize=(12, 5))
plt.scatter(df["score"], df["shifted_score"], alpha=0.5, color="red", label="Shifted Score")
plt.plot([0, 1000], [0, 1000], "--", color="black", alpha=0.7, label="Reference Line")
plt.xlabel("Original Score")
plt.ylabel("Transformed Score")
plt.legend()
plt.title("Quantile-Based Score Shifting: Before vs. After")
plt.show()

# üîπ Print sample output
print(df[["score", "shifted_score", "amount", "label"]].head())
















If your goal is to shift all scores to higher bins, you need a transformation that pushes scores upward while preserving relative ranking and variance balance.

üöÄ Key Approach: Non-Linear Score Transformation
We will apply a Monotonic Increasing Transformation to ensure:

All scores shift to higher bins

Relative ranking is preserved (higher original scores still remain higher)

Transformation is independent of distribution

No compression at the higher end

üî∑ Advanced Transformation Methods
1Ô∏è‚É£ Exponential Scaling (ùëì(ùë•) = A * exp(B * x))
This method amplifies small differences in lower scores but ensures all values shift upwards.

We set A and B such that the max score remains close to 999 while lower scores rise significantly.

2Ô∏è‚É£ Logistic Sigmoid Transformation (ùëì(ùë•) = 999 / (1 + exp(-ùõº(ùë• - ùõΩ))))
This method compresses lower values and stretches higher values toward 999.

ùõº controls steepness, ùõΩ centers the transformation.

3Ô∏è‚É£ Power-Law Transformation (ùëì(ùë•) = 999 * (x / 999)^Œ≥)
This ensures all scores increase while preserving relative rankings.

Œ≥ > 1 pushes all values upwards.

üî∑ Python Implementation
python
Copy
Edit
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Generate synthetic scores (range 0-999)
np.random.seed(42)
n_samples = 5000
scores = np.random.randint(0, 1000, n_samples)

# Exponential Transformation
A, B = 500, 0.002  # Tuned parameters
exp_transformed = A * np.exp(B * scores)

# Logistic Sigmoid Transformation
alpha, beta = 0.02, 500  # Tuned for smooth shift
logistic_transformed = 999 / (1 + np.exp(-alpha * (scores - beta)))

# Power-Law Transformation
gamma = 1.3  # Higher gamma shifts scores upward
power_transformed = 999 * (scores / 999) ** gamma

# Store in DataFrame
df = pd.DataFrame({"original_score": scores, 
                   "exp_transformed": exp_transformed, 
                   "logistic_transformed": logistic_transformed, 
                   "power_transformed": power_transformed})

# Ensure scores remain within [0, 999]
df = df.clip(0, 999)

# üî∑ Visualization - Shifted Scores
plt.figure(figsize=(12, 6))
sns.histplot(df["original_score"], bins=30, label="Original Score", color="blue", kde=True, alpha=0.5)
sns.histplot(df["exp_transformed"], bins=30, label="Exponential", color="red", kde=True, alpha=0.5)
sns.histplot(df["logistic_transformed"], bins=30, label="Logistic", color="green", kde=True, alpha=0.5)
sns.histplot(df["power_transformed"], bins=30, label="Power-Law", color="purple", kde=True, alpha=0.5)
plt.legend()
plt.title("Distribution Shift: Original vs. Transformed Scores")
plt.xlabel("Score")
plt.ylabel("Density")
plt.show()

# üî∑ Scatter Plot: Original vs. Shifted Scores
plt.figure(figsize=(12, 5))
plt.scatter(df["original_score"], df["exp_transformed"], alpha=0.5, label="Exponential", color="red")
plt.scatter(df["original_score"], df["logistic_transformed"], alpha=0.5, label="Logistic", color="green")
plt.scatter(df["original_score"], df["power_transformed"], alpha=0.5, label="Power-Law", color="purple")
plt.plot([0, 1000], [0, 1000], "--", color="black", alpha=0.7, label="Reference Line")
plt.xlabel("Original Score")
plt.ylabel("Transformed Score")
plt.legend()
plt.title("Score Transformation: Before vs. After")
plt.show()
üî∑ Explanation of Transformations
Transformation	Behavior	Best For
Exponential Scaling	Lower values get amplified significantly	Good for aggressive shifting
Logistic Sigmoid	Lower scores rise quickly, but higher scores approach 999 smoothly	Good for stable, controlled shifts
Power-Law	All values shift up, but keeps shape similar	Best for balanced shifting
üî∑ Which One Should You Use?
‚úÖ If you want a very aggressive shift: Use Exponential Scaling
‚úÖ If you want a smooth, natural shift: Use Logistic Sigmoid
‚úÖ If you want balanced movement without distortion: Use Power-Law

üöÄ Let me know if you need a custom version based on your real data distribution!

















import numpy as np
import pandas as pd
from scipy.special import expit  # Sigmoid function
from scipy.stats import gaussian_kde

# Generate synthetic fraud detection dataset
np.random.seed(42)
n_samples = 5000
scores = np.random.randint(0, 1000, n_samples)
probabilities = scores / 999  
labels = np.random.binomial(1, probabilities)

# Create DataFrame
df = pd.DataFrame({"score": scores, "label": labels})

# Custom Sigmoid Score Transformation
def sigmoid_transform(score, a=0.02, b=-5):
    return expit(a * score + b)

df["sigmoid_scaled"] = sigmoid_transform(df["score"])

# Tail Boosting Using Fraud Quantiles
gamma = 1.2  
df["tail_boosted"] = df["score"] ** gamma  

# Fraud Density Adjustment Using KDE
fraud_scores = df[df["label"] == 1]["score"]
kde = gaussian_kde(fraud_scores)
df["density_adjusted"] = kde(df["score"])

# Adaptive Mean Reversion (Bayesian Smoothing)
fraud_mean = df[df["label"] == 1]["score"].mean()
lambda_factor = 0.7  
df["bayesian_smoothed"] = lambda_factor * df["score"] + (1 - lambda_factor) * fraud_mean

# Dynamic Score Re-Weighting (Gradient-Based)
eta = 0.1
gradient = np.gradient(df["score"])
df["gradient_scaled"] = df["score"] + eta * gradient

# Display results
print(df.head())





import numpy as np
import pandas as pd
from scipy.optimize import minimize

# Generate synthetic fraud detection dataset
np.random.seed(42)
n_samples = 5000
scores = np.random.randint(0, 1000, n_samples)
probabilities = scores / 999  
labels = np.random.binomial(1, probabilities)

# Create DataFrame
df = pd.DataFrame({"score": scores, "label": labels})

# Compute means and variances
fraud_scores = df[df["label"] == 1]["score"]
all_scores = df["score"]

mean_fraud = fraud_scores.mean()
var_fraud = fraud_scores.var()
var_all = all_scores.var()

# 1Ô∏è‚É£ Variance Ratio Method
lambda_var = var_all / (var_all + var_fraud)

# 2Ô∏è‚É£ Maximum Likelihood Estimation (MLE)
def mle_loss(lambda_factor):
    smoothed_scores = lambda_factor * df["score"] + (1 - lambda_factor) * mean_fraud
    residuals = df["score"] - smoothed_scores
    return np.var(residuals)  # Minimize residual variance

lambda_mle = minimize(mle_loss, x0=0.5, bounds=[(0, 1)]).x[0]

# 3Ô∏è‚É£ Bayesian Smoothing with Optimal Œª
df["bayesian_smooth_var"] = lambda_var * df["score"] + (1 - lambda_var) * mean_fraud
df["bayesian_smooth_mle"] = lambda_mle * df["score"] + (1 - lambda_mle) * mean_fraud

# Display results
print(f"Optimal Lambda (Variance Method): {lambda_var:.4f}")
print(f"Optimal Lambda (MLE): {lambda_mle:.4f}")
print(df.head())




import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.optimize import minimize

# Generate synthetic fraud detection dataset
np.random.seed(42)
n_samples = 5000
scores = np.random.randint(0, 1000, n_samples)
probabilities = scores / 999  
labels = np.random.binomial(1, probabilities)

# Create DataFrame
df = pd.DataFrame({"score": scores, "label": labels})

# Compute means and variances
fraud_scores = df[df["label"] == 1]["score"]
all_scores = df["score"]

mean_fraud = fraud_scores.mean()
var_fraud = fraud_scores.var()
var_all = all_scores.var()

# 1Ô∏è‚É£ Variance Ratio Method
lambda_var = var_all / (var_all + var_fraud)

# 2Ô∏è‚É£ Maximum Likelihood Estimation (MLE)
def mle_loss(lambda_factor):
    smoothed_scores = lambda_factor * df["score"] + (1 - lambda_factor) * mean_fraud
    residuals = df["score"] - smoothed_scores
    return np.var(residuals)  # Minimize residual variance

lambda_mle = minimize(mle_loss, x0=0.5, bounds=[(0, 1)]).x[0]

# 3Ô∏è‚É£ Bayesian Smoothing with Optimal Œª
df["bayesian_smooth_var"] = lambda_var * df["score"] + (1 - lambda_var) * mean_fraud
df["bayesian_smooth_mle"] = lambda_mle * df["score"] + (1 - lambda_mle) * mean_fraud

# Display results
print(f"Optimal Lambda (Variance Method): {lambda_var:.4f}")
print(f"Optimal Lambda (MLE): {lambda_mle:.4f}")

# üî∑ Visualization 1: Histogram of Original vs. Smoothed Scores
plt.figure(figsize=(12, 5))
sns.histplot(df["score"], bins=30, color="blue", label="Original Scores", kde=True, alpha=0.5)
sns.histplot(df["bayesian_smooth_var"], bins=30, color="green", label="Smoothed (Variance Method)", kde=True, alpha=0.5)
sns.histplot(df["bayesian_smooth_mle"], bins=30, color="red", label="Smoothed (MLE Method)", kde=True, alpha=0.5)
plt.legend()
plt.title("Histogram: Original vs. Bayesian Smoothed Scores")
plt.xlabel("Score")
plt.ylabel("Density")
plt.show()

# üî∑ Visualization 2: Scatter Plot - Original vs. Smoothed Scores
plt.figure(figsize=(12, 5))
plt.scatter(df["score"], df["bayesian_smooth_var"], alpha=0.5, label="Variance Method", color="green")
plt.scatter(df["score"], df["bayesian_smooth_mle"], alpha=0.5, label="MLE Method", color="red")
plt.plot([0, 1000], [0, 1000], "--", color="black", alpha=0.7, label="Reference Line")
plt.xlabel("Original Score")
plt.ylabel("Smoothed Score")
plt.legend()
plt.title("Scatter Plot: Score Distribution Before & After Smoothing")
plt.show()

# üî∑ Visualization 3: Lambda Impact - Smoothed Scores for Different Lambda Values
lambda_values = np.linspace(0, 1, 10)
plt.figure(figsize=(12, 5))
for lam in lambda_values:
    smoothed_scores = lam * df["score"] + (1 - lam) * mean_fraud
    sns.kdeplot(smoothed_scores, label=f"Œª = {lam:.2f}")

plt.legend()
plt.title("Effect of Different Lambda Values on Score Smoothing")
plt.xlabel("Smoothed Score")
plt.ylabel("Density")
plt.show()








Here's a visualization of different Œ∑ (eta) methods to compare their effects on score re-weighting. The plots will show:

1Ô∏è‚É£ Original Scores vs. Adjusted Scores
2Ô∏è‚É£ Score Differences Across Methods

üìå Python Code for Visualization
python
Copy
Edit
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.optimize import minimize

# Generate synthetic fraud detection dataset
np.random.seed(42)
n_samples = 5000
scores = np.random.randint(0, 1000, n_samples)
probabilities = scores / 999  
labels = np.random.binomial(1, probabilities)

# Create DataFrame
df = pd.DataFrame({"score": scores, "label": labels})

# Compute score gradient
gradient = np.gradient(df["score"])

# 1Ô∏è‚É£ Mean Absolute Gradient Method
eta_mag = 1 / np.mean(np.abs(gradient))

# 2Ô∏è‚É£ Variance Ratio Method
fraud_scores = df[df["label"] == 1]["score"]
std_fraud = fraud_scores.std()
std_all = df["score"].std()
eta_var = std_fraud / std_all

# 3Ô∏è‚É£ Maximum Likelihood Estimation (MLE)
def mle_loss(eta_factor):
    adjusted_scores = df["score"] + eta_factor * gradient
    return np.var(adjusted_scores - df["score"])  # Minimize variance shift

eta_mle = minimize(mle_loss, x0=0.1, bounds=[(0, 1)]).x[0]

# Apply computed eta values
df["gradient_scaled_mag"] = df["score"] + eta_mag * gradient
df["gradient_scaled_var"] = df["score"] + eta_var * gradient
df["gradient_scaled_mle"] = df["score"] + eta_mle * gradient

# üìä Plot Original vs. Adjusted Scores
plt.figure(figsize=(12, 6))
plt.plot(df["score"], label="Original Score", color="black", alpha=0.6)
plt.plot(df["gradient_scaled_mag"], label=f"Mean Abs Gradient (Œ∑={eta_mag:.4f})", linestyle="--")
plt.plot(df["gradient_scaled_var"], label=f"Variance-Based (Œ∑={eta_var:.4f})", linestyle="-.")
plt.plot(df["gradient_scaled_mle"], label=f"MLE Optimized (Œ∑={eta_mle:.4f})", linestyle="dotted")
plt.legend()
plt.title("Comparison of Score Re-Weighting Methods")
plt.xlabel("Transaction Index")
plt.ylabel("Adjusted Score")
plt.show()

# üìä Plot Score Differences Across Methods
plt.figure(figsize=(12, 6))
plt.plot(df["gradient_scaled_mag"] - df["score"], label="Mean Abs Gradient Adjustment", linestyle="--")
plt.plot(df["gradient_scaled_var"] - df["score"], label="Variance-Based Adjustment", linestyle="-.")
plt.plot(df["gradient_scaled_mle"] - df["score"], label="MLE Optimized Adjustment", linestyle="dotted")
plt.legend()
plt.title("Score Adjustments Across Different Œ∑ Methods")
plt.xlabel("Transaction Index")
plt.ylabel("Score Adjustment")
plt.show()




