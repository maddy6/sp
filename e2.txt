import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xgb
from sklearn.model_selection import train_test_split

# 🔹 Step 1: Generate Data
np.random.seed(42)
n_samples = 5000
scores = np.random.randint(0, 1000, n_samples)
df = pd.DataFrame({"score": scores})

# 🔹 Step 2: Dynamically Compute Parameters for Uplift
mean_score = np.mean(scores)       # Dynamic central point (mean)
median_score = np.median(scores)   # Dynamic peak shift (median)
std_dev = np.std(scores)           # Standard deviation for scaling
variance = np.var(scores)          # Variance for decay control

base_uplift = 1 + (std_dev / mean_score)  # Adaptive base uplift factor
decay_factor = variance / (2 * mean_score)  # Decay factor based on variance

# 🔹 Step 3: Dynamic Target Score Computation
df["target_score"] = df["score"] * (
    base_uplift + 0.3 * np.exp(-((df["score"] - median_score) ** 2) / decay_factor)
)

# 🔹 Step 4: Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(df[["score"]], df["target_score"], test_size=0.2, random_state=42)

# 🔹 Step 5: Train XGBoost Model
model = xgb.XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.05, objective="reg:squarederror")
model.fit(X_train, y_train)

# 🔹 Step 6: Predict Adjusted Scores
df["adjusted_score"] = model.predict(df[["score"]])
df["adjusted_score"] = np.clip(df["adjusted_score"], 0, 999)  # Clip within valid range

# 🔹 Step 7: Visualization
plt.figure(figsize=(10, 6))
sns.lineplot(x=df["score"], y=df["adjusted_score"], label="Adjusted Scores (XGBoost)", color="blue")
sns.lineplot(x=df["score"], y=df["score"], label="Original Scores", color="red", linestyle="dashed")
plt.xlabel("Original Score")
plt.ylabel("Adjusted Score")
plt.title("Score Shifting Using XGBoost (Dynamic Calculation)")
plt.legend()
plt.grid(True)
plt.show()

# 🔹 Display Sample Results
print(df[["score", "adjusted_score"]].head(10))



















import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.special import expit
from sklearn.mixture import GaussianMixture

# 🔹 Step 1: Generate Synthetic Data
np.random.seed(42)
n_samples = 5000
scores = np.random.randint(0, 1000, n_samples)  # Original scores (0-999)
probabilities = scores / 999  
labels = np.random.binomial(1, probabilities)  # Fraud labels (0/1)
amounts = np.random.uniform(10, 5000, n_samples)  # Transaction amounts

# Create DataFrame
df = pd.DataFrame({"score": scores, "label": labels, "amount": amounts})

# 🔹 Step 2: Fit Gaussian Mixture Model (GMM)
gmm = GaussianMixture(n_components=3, random_state=42)  # 3 clusters (low, mid, high)
df["score_reshaped"] = df["score"].values.reshape(-1, 1)  # Reshape for GMM
gmm.fit(df[["score_reshaped"]])

# Get Gaussian components' means & standard deviations
means = gmm.means_.flatten()
stds = np.sqrt(gmm.covariances_).flatten()

# 🔹 Step 3: Define Controlled Score Adjustment Function
def gmm_adjusted_score(score):
    """Smooth score uplift based on Gaussian components."""
    z_score = (score - np.mean(means)) / np.mean(stds)  # Normalize score
    uplift_factor = 1 + 0.5 * expit(z_score)  # Smooth sigmoid-based scaling
    new_score = score * uplift_factor  # Apply uplift
    return min(new_score, 999)  # Clip at 999

df["adjusted_score"] = df["score"].apply(gmm_adjusted_score)

# 🔹 Step 4: Visualization - Score Shift
plt.figure(figsize=(10, 6))
sns.lineplot(x=df["score"], y=df["adjusted_score"], label="Adjusted Scores", color="blue")
sns.lineplot(x=df["score"], y=df["score"], label="Original Scores", color="red", linestyle="dashed")
plt.xlabel("Original Score")
plt.ylabel("Adjusted Score")
plt.title("Score Shifting Visualization (GMM-Based Adjustment)")
plt.legend()
plt.grid(True)
plt.show()

# 🔹 Display Sample Results
print(df[["score", "adjusted_score"]].head(10))

















import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.special import expit  # Sigmoid function

# 🔹 Step 1: Generate Synthetic Data
np.random.seed(42)
n_samples = 5000
scores = np.random.randint(0, 1000, n_samples)  # Original scores (0-999)
probabilities = scores / 999  
labels = np.random.binomial(1, probabilities)  # Fraud labels (0/1)
amounts = np.random.uniform(10, 5000, n_samples)  # Random transaction amounts

# Create DataFrame
df = pd.DataFrame({"score": scores, "label": labels, "amount": amounts})

# 🔹 Step 2: Compute Score Percentiles (Statistical Backing)
df["percentile"] = df["score"].rank(pct=True)  # Get percentiles (0 to 1)

# 🔹 Step 3: Define a Controlled Scaling Function
def controlled_scaling(percentile):
    """Smooth transformation that prevents extreme inflation."""
    base_alpha = 1.2 + 0.5 * (percentile**1.2)  # Gradual scaling increase
    sigmoid_adjustment = expit((percentile - 0.5) * 8)  # Smoothen transition
    return 1 + (base_alpha - 1) * sigmoid_adjustment  # Ensures smooth uplift

df["scaling_factor"] = df["percentile"].apply(controlled_scaling)  # Compute scaling

# 🔹 Step 4: Adjust Scores with Clipping
df["adjusted_score"] = df["score"] * df["scaling_factor"]
df["adjusted_score"] = np.clip(df["adjusted_score"], df["score"], 999)  # Ensure monotonic increase

# 🔹 Step 5: Visualization - Score Shift
plt.figure(figsize=(10, 6))
sns.lineplot(x=df["score"], y=df["adjusted_score"], label="Shifted Scores", color="blue")
sns.lineplot(x=df["score"], y=df["score"], label="Original Scores", color="red", linestyle="dashed")
plt.xlabel("Original Score")
plt.ylabel("Adjusted Score")
plt.title("Score Shifting Visualization")
plt.legend()
plt.grid(True)
plt.show()

# 🔹 Display Sample Results
print(df[["score", "percentile", "scaling_factor", "adjusted_score"]].head(10))

















import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 🔹 Step 1: Generate Synthetic Data
np.random.seed(42)
n_samples = 5000
scores = np.random.randint(0, 1000, n_samples)  # Original scores (0-999)
probabilities = scores / 999  
labels = np.random.binomial(1, probabilities)  # Fraud labels (0/1)
amounts = np.random.uniform(10, 5000, n_samples)  # Random transaction amounts

# Create DataFrame
df = pd.DataFrame({"score": scores, "label": labels, "amount": amounts})

# 🔹 Step 2: Compute Score Percentiles (Statistical Backing)
df["percentile"] = df["score"].rank(pct=True)  # Percentile-based scaling

# 🔹 Step 3: Define a Smooth Scaling Function (Polynomial-Based)
def smooth_scaling(percentile):
    """Polynomial-based transformation ensuring smooth upward shift."""
    return 1 + 0.6 * (percentile**0.8)  # Adjust exponent for gradual increase

df["scaling_factor"] = df["percentile"].apply(smooth_scaling)  # Compute scaling factor

# 🔹 Step 4: Adjust Scores Based on Scaling Factor
df["adjusted_score"] = df["score"] * df["scaling_factor"]
df["adjusted_score"] = np.clip(df["adjusted_score"], 0, 999)  # Ensure within [0,999]

# 🔹 Step 5: Visualization - Score Shift
plt.figure(figsize=(10, 6))
sns.lineplot(x=df["score"], y=df["adjusted_score"], label="Shifted Scores", color="blue")
sns.lineplot(x=df["score"], y=df["score"], label="Original Scores", color="red", linestyle="dashed")
plt.xlabel("Original Score")
plt.ylabel("Adjusted Score")
plt.title("Score Shifting Visualization")
plt.legend()
plt.grid(True)
plt.show()

# 🔹 Display Sample Results
print(df[["score", "percentile", "scaling_factor", "adjusted_score"]].head(10))


















import numpy as np
import pandas as pd
from scipy.interpolate import interp1d
from scipy.special import expit
import matplotlib.pyplot as plt

# 🔹 Generate synthetic fraud detection dataset
np.random.seed(42)
n_samples = 5000
scores = np.random.randint(0, 1000, n_samples)
amounts = np.random.exponential(scale=500, size=n_samples)
probabilities = scores / 999
labels = np.random.binomial(1, probabilities)

# 🔹 Create DataFrame
df = pd.DataFrame({"score": scores, "amount": amounts, "label": labels})

# 🔹 Compute Percentiles (Ensuring Uniqueness)
percentiles = np.unique(np.percentile(df["score"], np.linspace(5, 95, 20)))  # Avoids extreme tails
scaling_factors = np.linspace(1.05, 1.25, len(percentiles))  # More controlled range

# 🔹 Create smooth percentile-based scaling function
scaling_function = interp1d(percentiles, scaling_factors, kind="linear", fill_value="extrapolate")

# 🔹 Adaptive Scaling Factor with Soft Constraints
def adaptive_alpha(score):
    base_alpha = scaling_function(score)
    
    # Apply smooth sigmoid-like transformation to prevent excessive inflation
    adjusted_alpha = 1 + (base_alpha - 1) * expit((score - 500) / 150)  
    return adjusted_alpha

df["alpha"] = df["score"].apply(adaptive_alpha)

# 🔹 Step 1: Controlled Inflation
df["inflated_score"] = df["score"] * df["alpha"]

# 🔹 Step 2: Quantile-Based Normalization (Ensures shifts follow actual distribution)
sorted_scores = np.sort(df["score"].values)
quantiles = np.linspace(0, 1, len(sorted_scores))
quantile_function = interp1d(sorted_scores, quantiles, kind="linear", fill_value="extrapolate")

df["normalized_score"] = quantile_function(df["inflated_score"]) * 999  # Rescale to [0,999]

# 🔹 Step 3: Soft Adjustment with Exponential Control
df["final_score"] = 999 * expit((df["normalized_score"] / 999 - 0.5) * 1.05)

# 🔹 Step 4: Clip Values to Maintain [0, 999] Range
df["final_score"] = np.clip(df["final_score"], df["score"], 999)  # Prevent excessive drops

# 🔹 Visualization: Before vs After Score Shift
plt.figure(figsize=(10, 5))
plt.hist(df["score"], bins=30, alpha=0.5, label="Original Scores", color="blue")
plt.hist(df["final_score"], bins=30, alpha=0.5, label="Final Adjusted Scores", color="red")
plt.xlabel("Score")
plt.ylabel("Frequency")
plt.title("Score Distribution Before & After Adjustment")
plt.legend()
plt.show()

# 🔹 Display Impact
print(df[["score", "alpha", "final_score"]].head(10))















import numpy as np
import pandas as pd
from scipy.interpolate import interp1d
from scipy.special import expit
import matplotlib.pyplot as plt

# 🔹 Generate synthetic fraud detection dataset
np.random.seed(42)
n_samples = 5000
scores = np.random.randint(0, 1000, n_samples)
amounts = np.random.exponential(scale=500, size=n_samples)
probabilities = scores / 999
labels = np.random.binomial(1, probabilities)

# 🔹 Create DataFrame
df = pd.DataFrame({"score": scores, "amount": amounts, "label": labels})

# 🔹 Compute Unique Percentile Values
percentiles = np.unique(np.percentile(df["score"], np.linspace(0, 100, 20)))  # Ensure uniqueness
scaling_factors = np.linspace(1.05, 1.35, len(percentiles))  # Controlled adaptive increase

# 🔹 Create smooth percentile-based scaling function
scaling_function = interp1d(percentiles, scaling_factors, kind="linear", fill_value="extrapolate")

# 🔹 Apply Adaptive Scaling Factor
df["alpha"] = df["score"].apply(lambda x: scaling_function(x))

# 🔹 Step 1: Controlled Inflation
df["inflated_score"] = df["score"] * df["alpha"]

# 🔹 Step 2: Quantile-Based Normalization (Ensures shifts follow actual distribution)
sorted_scores = np.sort(df["score"].values)
quantiles = np.linspace(0, 1, len(sorted_scores))
quantile_function = interp1d(sorted_scores, quantiles, kind="linear", fill_value="extrapolate")

df["normalized_score"] = quantile_function(df["inflated_score"]) * 999  # Rescale to [0,999]

# 🔹 Step 3: Apply Soft Logistic Smoothing (Final Stabilization)
df["final_score"] = 999 * expit((df["normalized_score"] / 999 - 0.5) * 1.1)

# 🔹 Clip values to maintain [0, 999] range
df["final_score"] = np.clip(df["final_score"], 0, 999)

# 🔹 Visualization: Before vs After Score Shift
plt.figure(figsize=(10, 5))
plt.hist(df["score"], bins=30, alpha=0.5, label="Original Scores", color="blue")
plt.hist(df["final_score"], bins=30, alpha=0.5, label="Final Adjusted Scores", color="red")
plt.xlabel("Score")
plt.ylabel("Frequency")
plt.title("Score Distribution Before & After Adjustment")
plt.legend()
plt.show()

# 🔹 Display Impact
print(df[["score", "alpha", "final_score"]].head(10))


















import numpy as np
import pandas as pd
from scipy.interpolate import interp1d
from scipy.special import expit
import matplotlib.pyplot as plt

# 🔹 Generate synthetic fraud detection dataset
np.random.seed(42)
n_samples = 5000
scores = np.random.randint(0, 1000, n_samples)
amounts = np.random.exponential(scale=500, size=n_samples)  
probabilities = scores / 999  
labels = np.random.binomial(1, probabilities)

# 🔹 Create DataFrame
df = pd.DataFrame({"score": scores, "amount": amounts, "label": labels})

# 🔹 Compute all percentiles dynamically
percentile_values = np.percentile(df["score"], np.arange(0, 101, 5))  # Every 5% step

# 🔹 Define dynamic alpha scaling based on percentiles
alpha_values = 1 + 0.4 * np.linspace(0, 1, len(percentile_values))  # Smooth α increase

# 🔹 Create a smooth function to assign alpha values based on score percentiles
alpha_function = interp1d(percentile_values, alpha_values, kind="cubic", fill_value="extrapolate")

# 🔹 Apply adaptive α to each score
df["alpha"] = df["score"].apply(lambda x: alpha_function(x))

# 🔹 Apply Transformation: Scale + Logistic Smoothing
df["adjusted_score"] = df["score"] * df["alpha"]  # Direct Scaling
df["adjusted_score"] = 999 * expit((df["adjusted_score"] / 999 - 0.5) * 1.2)  # Smooth Shift

# 🔹 Clip values to ensure scores remain within valid range [0, 999]
df["adjusted_score"] = np.clip(df["adjusted_score"], 0, 999)

# 🔹 Visualization: Before vs After Score Shift
plt.figure(figsize=(10, 5))
plt.hist(df["score"], bins=30, alpha=0.5, label="Original Scores", color="blue")
plt.hist(df["adjusted_score"], bins=30, alpha=0.5, label="Adjusted Scores", color="red")
plt.xlabel("Score")
plt.ylabel("Frequency")
plt.title("Score Distribution Before & After Adjustment")
plt.legend()
plt.show()

# 🔹 Display Impact
print(df[["score", "alpha", "adjusted_score"]].head(10))








🔹 Step 1: Compute Adaptive Scaling Factor (α)
We will define α dynamically based on the z-score of the original scores, ensuring that the shift is justified statistically.

python
Copy
Edit
import numpy as np
import pandas as pd
from scipy.special import expit

# 🔹 Generate synthetic fraud detection dataset
np.random.seed(42)
n_samples = 5000
scores = np.random.randint(0, 1000, n_samples)
amounts = np.random.exponential(scale=500, size=n_samples)  # Sample transaction amounts
probabilities = scores / 999  
labels = np.random.binomial(1, probabilities)

# 🔹 Create DataFrame
df = pd.DataFrame({"score": scores, "amount": amounts, "label": labels})

# 🔹 Compute statistical properties
mu, sigma = df["score"].mean(), df["score"].std()
q10, q50, q90 = np.percentile(df["score"], [10, 50, 90])  # Deciles for adaptive scaling

# 🔹 Define Adaptive Scaling Factor (α) using z-score
def compute_alpha(score):
    z_score = (score - mu) / sigma  # Standardized score
    return 1 + 0.5 * expit(z_score)  # Sigmoid ensures gradual increase

df["alpha"] = df["score"].apply(compute_alpha)
🔹 Step 2: Apply Hybrid Score Transformation
We will use a hybrid transformation that:
✅ Uses multiplication-based scaling to ensure statistical justification.
✅ Uses logistic scaling (expit) to keep shifts smooth & bounded.

python
Copy
Edit
# 🔹 Step 2: Hybrid Transformation
df["adjusted_score"] = df["score"] * df["alpha"]  # Direct Scaling
df["adjusted_score"] = 999 * expit((df["adjusted_score"] / 999 - 0.5) * 1.2)  # Logistic Scaling

# 🔹 Clip values to ensure range [0, 999]
df["adjusted_score"] = np.clip(df["adjusted_score"], 0, 999)

# 🔹 Display impact
print(df[["score", "alpha", "adjusted_score"]].head(10))
✅ Why This Works Best
1️⃣ Low Scores (<500) Now Increase Gradually

The adaptive α (scaling factor) ensures they don’t drop but instead get a justified uplift.

2️⃣ High Scores (≥500) Still Get a Strong Boost But Stay Controlled

The logistic transformation prevents extreme jumps while ensuring better ranking.

3️⃣ Statistical Stability & Explainability

The approach is backed by z-score scaling and percentile-based adaptive uplift.

















import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import QuantileTransformer

# 🔹 Generate synthetic dataset (columns: score, amount, label)
np.random.seed(42)
n_samples = 5000
df = pd.DataFrame({
    "score": np.random.randint(0, 1000, n_samples),  # Scores (0-999)
    "amount": np.random.exponential(scale=500, size=n_samples),  # Transaction Amounts
    "label": np.random.binomial(1, 0.1, n_samples)  # Fraud Labels (10% fraud)
})

# 🔹 Step 1: Compute Statistical Alpha (Data-Driven)
percentile_90 = np.percentile(df["score"], 90)  # 90th percentile
percentile_10 = np.percentile(df["score"], 10)  # 10th percentile
mean_score = df["score"].mean()
std_score = df["score"].std()

# Compute α dynamically: Controls how much scores are lifted
alpha = np.log(percentile_90 / percentile_10) / np.log(mean_score / std_score)
alpha = max(1.1, min(alpha, 2.0))  # Keep α in reasonable range (1.1 to 2.0)

print(f"🔹 Computed Alpha: {alpha:.4f}")

# 🔹 Step 2: Apply Quantile Transformation
qt = QuantileTransformer(output_distribution="uniform")
df["score_uniform"] = qt.fit_transform(df[["score"]])  # Maps to [0,1]

# 🔹 Step 3: Exponential Scaling using Dynamic Alpha
df["shifted_score"] = 999 * (df["score_uniform"] ** alpha)

# 🔹 Step 4: Ensure scores stay within [0, 999]
df["shifted_score"] = np.clip(df["shifted_score"], 0, 999)

# 🔹 Visualization - Score Shift
plt.figure(figsize=(12, 6))
sns.histplot(df["score"], bins=30, label="Original Scores", color="blue", kde=True, alpha=0.5)
sns.histplot(df["shifted_score"], bins=30, label="Shifted Scores", color="red", kde=True, alpha=0.5)
plt.legend()
plt.title("Distribution Shift: Original vs. Transformed Scores")
plt.xlabel("Score")
plt.ylabel("Density")
plt.show()

# 🔹 Scatter Plot: Original vs. Shifted Scores
plt.figure(figsize=(12, 5))
plt.scatter(df["score"], df["shifted_score"], alpha=0.5, color="red", label="Shifted Score")
plt.plot([0, 1000], [0, 1000], "--", color="black", alpha=0.7, label="Reference Line")
plt.xlabel("Original Score")
plt.ylabel("Transformed Score")
plt.legend()
plt.title("Quantile-Based Score Shifting: Before vs. After")
plt.show()

# 🔹 Print sample output
print(df[["score", "shifted_score", "amount", "label"]].head())








import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import QuantileTransformer

# 🔹 Generate synthetic dataset (columns: score, amount, label)
np.random.seed(42)
n_samples = 5000
df = pd.DataFrame({
    "score": np.random.randint(0, 1000, n_samples),  # Scores (0-999)
    "amount": np.random.exponential(scale=500, size=n_samples),  # Transaction Amounts
    "label": np.random.binomial(1, 0.1, n_samples)  # Fraud Labels (10% fraud)
})

# 🔹 Step 1: Quantile Transformation to Uniform Distribution
qt = QuantileTransformer(output_distribution="uniform")
df["score_uniform"] = qt.fit_transform(df[["score"]])  # Maps to [0,1]

# 🔹 Step 2: Exponential Scaling to Shift Scores Upwards
alpha = 1.5  # Controls how aggressively scores increase
df["shifted_score"] = 999 * (df["score_uniform"] ** alpha)

# 🔹 Step 3: Ensure scores stay within [0, 999]
df["shifted_score"] = np.clip(df["shifted_score"], 0, 999)

# 🔹 Visualization - Score Shift
plt.figure(figsize=(12, 6))
sns.histplot(df["score"], bins=30, label="Original Scores", color="blue", kde=True, alpha=0.5)
sns.histplot(df["shifted_score"], bins=30, label="Shifted Scores", color="red", kde=True, alpha=0.5)
plt.legend()
plt.title("Distribution Shift: Original vs. Transformed Scores")
plt.xlabel("Score")
plt.ylabel("Density")
plt.show()

# 🔹 Scatter Plot: Original vs. Shifted Scores
plt.figure(figsize=(12, 5))
plt.scatter(df["score"], df["shifted_score"], alpha=0.5, color="red", label="Shifted Score")
plt.plot([0, 1000], [0, 1000], "--", color="black", alpha=0.7, label="Reference Line")
plt.xlabel("Original Score")
plt.ylabel("Transformed Score")
plt.legend()
plt.title("Quantile-Based Score Shifting: Before vs. After")
plt.show()

# 🔹 Print sample output
print(df[["score", "shifted_score", "amount", "label"]].head())
















If your goal is to shift all scores to higher bins, you need a transformation that pushes scores upward while preserving relative ranking and variance balance.

🚀 Key Approach: Non-Linear Score Transformation
We will apply a Monotonic Increasing Transformation to ensure:

All scores shift to higher bins

Relative ranking is preserved (higher original scores still remain higher)

Transformation is independent of distribution

No compression at the higher end

🔷 Advanced Transformation Methods
1️⃣ Exponential Scaling (𝑓(𝑥) = A * exp(B * x))
This method amplifies small differences in lower scores but ensures all values shift upwards.

We set A and B such that the max score remains close to 999 while lower scores rise significantly.

2️⃣ Logistic Sigmoid Transformation (𝑓(𝑥) = 999 / (1 + exp(-𝛼(𝑥 - 𝛽))))
This method compresses lower values and stretches higher values toward 999.

𝛼 controls steepness, 𝛽 centers the transformation.

3️⃣ Power-Law Transformation (𝑓(𝑥) = 999 * (x / 999)^γ)
This ensures all scores increase while preserving relative rankings.

γ > 1 pushes all values upwards.

🔷 Python Implementation
python
Copy
Edit
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Generate synthetic scores (range 0-999)
np.random.seed(42)
n_samples = 5000
scores = np.random.randint(0, 1000, n_samples)

# Exponential Transformation
A, B = 500, 0.002  # Tuned parameters
exp_transformed = A * np.exp(B * scores)

# Logistic Sigmoid Transformation
alpha, beta = 0.02, 500  # Tuned for smooth shift
logistic_transformed = 999 / (1 + np.exp(-alpha * (scores - beta)))

# Power-Law Transformation
gamma = 1.3  # Higher gamma shifts scores upward
power_transformed = 999 * (scores / 999) ** gamma

# Store in DataFrame
df = pd.DataFrame({"original_score": scores, 
                   "exp_transformed": exp_transformed, 
                   "logistic_transformed": logistic_transformed, 
                   "power_transformed": power_transformed})

# Ensure scores remain within [0, 999]
df = df.clip(0, 999)

# 🔷 Visualization - Shifted Scores
plt.figure(figsize=(12, 6))
sns.histplot(df["original_score"], bins=30, label="Original Score", color="blue", kde=True, alpha=0.5)
sns.histplot(df["exp_transformed"], bins=30, label="Exponential", color="red", kde=True, alpha=0.5)
sns.histplot(df["logistic_transformed"], bins=30, label="Logistic", color="green", kde=True, alpha=0.5)
sns.histplot(df["power_transformed"], bins=30, label="Power-Law", color="purple", kde=True, alpha=0.5)
plt.legend()
plt.title("Distribution Shift: Original vs. Transformed Scores")
plt.xlabel("Score")
plt.ylabel("Density")
plt.show()

# 🔷 Scatter Plot: Original vs. Shifted Scores
plt.figure(figsize=(12, 5))
plt.scatter(df["original_score"], df["exp_transformed"], alpha=0.5, label="Exponential", color="red")
plt.scatter(df["original_score"], df["logistic_transformed"], alpha=0.5, label="Logistic", color="green")
plt.scatter(df["original_score"], df["power_transformed"], alpha=0.5, label="Power-Law", color="purple")
plt.plot([0, 1000], [0, 1000], "--", color="black", alpha=0.7, label="Reference Line")
plt.xlabel("Original Score")
plt.ylabel("Transformed Score")
plt.legend()
plt.title("Score Transformation: Before vs. After")
plt.show()
🔷 Explanation of Transformations
Transformation	Behavior	Best For
Exponential Scaling	Lower values get amplified significantly	Good for aggressive shifting
Logistic Sigmoid	Lower scores rise quickly, but higher scores approach 999 smoothly	Good for stable, controlled shifts
Power-Law	All values shift up, but keeps shape similar	Best for balanced shifting
🔷 Which One Should You Use?
✅ If you want a very aggressive shift: Use Exponential Scaling
✅ If you want a smooth, natural shift: Use Logistic Sigmoid
✅ If you want balanced movement without distortion: Use Power-Law

🚀 Let me know if you need a custom version based on your real data distribution!

















import numpy as np
import pandas as pd
from scipy.special import expit  # Sigmoid function
from scipy.stats import gaussian_kde

# Generate synthetic fraud detection dataset
np.random.seed(42)
n_samples = 5000
scores = np.random.randint(0, 1000, n_samples)
probabilities = scores / 999  
labels = np.random.binomial(1, probabilities)

# Create DataFrame
df = pd.DataFrame({"score": scores, "label": labels})

# Custom Sigmoid Score Transformation
def sigmoid_transform(score, a=0.02, b=-5):
    return expit(a * score + b)

df["sigmoid_scaled"] = sigmoid_transform(df["score"])

# Tail Boosting Using Fraud Quantiles
gamma = 1.2  
df["tail_boosted"] = df["score"] ** gamma  

# Fraud Density Adjustment Using KDE
fraud_scores = df[df["label"] == 1]["score"]
kde = gaussian_kde(fraud_scores)
df["density_adjusted"] = kde(df["score"])

# Adaptive Mean Reversion (Bayesian Smoothing)
fraud_mean = df[df["label"] == 1]["score"].mean()
lambda_factor = 0.7  
df["bayesian_smoothed"] = lambda_factor * df["score"] + (1 - lambda_factor) * fraud_mean

# Dynamic Score Re-Weighting (Gradient-Based)
eta = 0.1
gradient = np.gradient(df["score"])
df["gradient_scaled"] = df["score"] + eta * gradient

# Display results
print(df.head())





import numpy as np
import pandas as pd
from scipy.optimize import minimize

# Generate synthetic fraud detection dataset
np.random.seed(42)
n_samples = 5000
scores = np.random.randint(0, 1000, n_samples)
probabilities = scores / 999  
labels = np.random.binomial(1, probabilities)

# Create DataFrame
df = pd.DataFrame({"score": scores, "label": labels})

# Compute means and variances
fraud_scores = df[df["label"] == 1]["score"]
all_scores = df["score"]

mean_fraud = fraud_scores.mean()
var_fraud = fraud_scores.var()
var_all = all_scores.var()

# 1️⃣ Variance Ratio Method
lambda_var = var_all / (var_all + var_fraud)

# 2️⃣ Maximum Likelihood Estimation (MLE)
def mle_loss(lambda_factor):
    smoothed_scores = lambda_factor * df["score"] + (1 - lambda_factor) * mean_fraud
    residuals = df["score"] - smoothed_scores
    return np.var(residuals)  # Minimize residual variance

lambda_mle = minimize(mle_loss, x0=0.5, bounds=[(0, 1)]).x[0]

# 3️⃣ Bayesian Smoothing with Optimal λ
df["bayesian_smooth_var"] = lambda_var * df["score"] + (1 - lambda_var) * mean_fraud
df["bayesian_smooth_mle"] = lambda_mle * df["score"] + (1 - lambda_mle) * mean_fraud

# Display results
print(f"Optimal Lambda (Variance Method): {lambda_var:.4f}")
print(f"Optimal Lambda (MLE): {lambda_mle:.4f}")
print(df.head())




import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.optimize import minimize

# Generate synthetic fraud detection dataset
np.random.seed(42)
n_samples = 5000
scores = np.random.randint(0, 1000, n_samples)
probabilities = scores / 999  
labels = np.random.binomial(1, probabilities)

# Create DataFrame
df = pd.DataFrame({"score": scores, "label": labels})

# Compute means and variances
fraud_scores = df[df["label"] == 1]["score"]
all_scores = df["score"]

mean_fraud = fraud_scores.mean()
var_fraud = fraud_scores.var()
var_all = all_scores.var()

# 1️⃣ Variance Ratio Method
lambda_var = var_all / (var_all + var_fraud)

# 2️⃣ Maximum Likelihood Estimation (MLE)
def mle_loss(lambda_factor):
    smoothed_scores = lambda_factor * df["score"] + (1 - lambda_factor) * mean_fraud
    residuals = df["score"] - smoothed_scores
    return np.var(residuals)  # Minimize residual variance

lambda_mle = minimize(mle_loss, x0=0.5, bounds=[(0, 1)]).x[0]

# 3️⃣ Bayesian Smoothing with Optimal λ
df["bayesian_smooth_var"] = lambda_var * df["score"] + (1 - lambda_var) * mean_fraud
df["bayesian_smooth_mle"] = lambda_mle * df["score"] + (1 - lambda_mle) * mean_fraud

# Display results
print(f"Optimal Lambda (Variance Method): {lambda_var:.4f}")
print(f"Optimal Lambda (MLE): {lambda_mle:.4f}")

# 🔷 Visualization 1: Histogram of Original vs. Smoothed Scores
plt.figure(figsize=(12, 5))
sns.histplot(df["score"], bins=30, color="blue", label="Original Scores", kde=True, alpha=0.5)
sns.histplot(df["bayesian_smooth_var"], bins=30, color="green", label="Smoothed (Variance Method)", kde=True, alpha=0.5)
sns.histplot(df["bayesian_smooth_mle"], bins=30, color="red", label="Smoothed (MLE Method)", kde=True, alpha=0.5)
plt.legend()
plt.title("Histogram: Original vs. Bayesian Smoothed Scores")
plt.xlabel("Score")
plt.ylabel("Density")
plt.show()

# 🔷 Visualization 2: Scatter Plot - Original vs. Smoothed Scores
plt.figure(figsize=(12, 5))
plt.scatter(df["score"], df["bayesian_smooth_var"], alpha=0.5, label="Variance Method", color="green")
plt.scatter(df["score"], df["bayesian_smooth_mle"], alpha=0.5, label="MLE Method", color="red")
plt.plot([0, 1000], [0, 1000], "--", color="black", alpha=0.7, label="Reference Line")
plt.xlabel("Original Score")
plt.ylabel("Smoothed Score")
plt.legend()
plt.title("Scatter Plot: Score Distribution Before & After Smoothing")
plt.show()

# 🔷 Visualization 3: Lambda Impact - Smoothed Scores for Different Lambda Values
lambda_values = np.linspace(0, 1, 10)
plt.figure(figsize=(12, 5))
for lam in lambda_values:
    smoothed_scores = lam * df["score"] + (1 - lam) * mean_fraud
    sns.kdeplot(smoothed_scores, label=f"λ = {lam:.2f}")

plt.legend()
plt.title("Effect of Different Lambda Values on Score Smoothing")
plt.xlabel("Smoothed Score")
plt.ylabel("Density")
plt.show()








Here's a visualization of different η (eta) methods to compare their effects on score re-weighting. The plots will show:

1️⃣ Original Scores vs. Adjusted Scores
2️⃣ Score Differences Across Methods

📌 Python Code for Visualization
python
Copy
Edit
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.optimize import minimize

# Generate synthetic fraud detection dataset
np.random.seed(42)
n_samples = 5000
scores = np.random.randint(0, 1000, n_samples)
probabilities = scores / 999  
labels = np.random.binomial(1, probabilities)

# Create DataFrame
df = pd.DataFrame({"score": scores, "label": labels})

# Compute score gradient
gradient = np.gradient(df["score"])

# 1️⃣ Mean Absolute Gradient Method
eta_mag = 1 / np.mean(np.abs(gradient))

# 2️⃣ Variance Ratio Method
fraud_scores = df[df["label"] == 1]["score"]
std_fraud = fraud_scores.std()
std_all = df["score"].std()
eta_var = std_fraud / std_all

# 3️⃣ Maximum Likelihood Estimation (MLE)
def mle_loss(eta_factor):
    adjusted_scores = df["score"] + eta_factor * gradient
    return np.var(adjusted_scores - df["score"])  # Minimize variance shift

eta_mle = minimize(mle_loss, x0=0.1, bounds=[(0, 1)]).x[0]

# Apply computed eta values
df["gradient_scaled_mag"] = df["score"] + eta_mag * gradient
df["gradient_scaled_var"] = df["score"] + eta_var * gradient
df["gradient_scaled_mle"] = df["score"] + eta_mle * gradient

# 📊 Plot Original vs. Adjusted Scores
plt.figure(figsize=(12, 6))
plt.plot(df["score"], label="Original Score", color="black", alpha=0.6)
plt.plot(df["gradient_scaled_mag"], label=f"Mean Abs Gradient (η={eta_mag:.4f})", linestyle="--")
plt.plot(df["gradient_scaled_var"], label=f"Variance-Based (η={eta_var:.4f})", linestyle="-.")
plt.plot(df["gradient_scaled_mle"], label=f"MLE Optimized (η={eta_mle:.4f})", linestyle="dotted")
plt.legend()
plt.title("Comparison of Score Re-Weighting Methods")
plt.xlabel("Transaction Index")
plt.ylabel("Adjusted Score")
plt.show()

# 📊 Plot Score Differences Across Methods
plt.figure(figsize=(12, 6))
plt.plot(df["gradient_scaled_mag"] - df["score"], label="Mean Abs Gradient Adjustment", linestyle="--")
plt.plot(df["gradient_scaled_var"] - df["score"], label="Variance-Based Adjustment", linestyle="-.")
plt.plot(df["gradient_scaled_mle"] - df["score"], label="MLE Optimized Adjustment", linestyle="dotted")
plt.legend()
plt.title("Score Adjustments Across Different η Methods")
plt.xlabel("Transaction Index")
plt.ylabel("Score Adjustment")
plt.show()




