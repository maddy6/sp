1Ô∏è‚É£ Starting point

You already have:

results_df = pd.DataFrame(results,
                          columns=['Group','Test_RMSE','Test_MAPE','Val_RMSE','Val_MAPE'])


We‚Äôll add a Stability Index and (if you have it) a Group Size or Weight column to show business impact.

# Add Stability Index (difference between Val & Test MAPE)
results_df['Stability_Index'] = (results_df['Val_MAPE'] - results_df['Test_MAPE']).abs()

# (Optional) Add Group Size column to show business impact
# results_df['Group_Size'] = [volume per group]  # supply if you have

2Ô∏è‚É£ Combined Dashboard Code
import matplotlib.pyplot as plt
import seaborn as sns

# sort groups by Val_MAPE for consistent ordering
results_df = results_df.sort_values('Val_MAPE')

fig, axes = plt.subplots(2, 2, figsize=(15, 10))
axes = axes.flatten()

# 1Ô∏è‚É£ Bar chart ‚Äì Validation MAPE per group
axes[0].barh(results_df['Group'], results_df['Val_MAPE'], color='teal')
axes[0].set_title('Validation MAPE by Group (Forecast Accuracy)')
axes[0].set_xlabel('Val MAPE (%)')

# 2Ô∏è‚É£ Scatter plot ‚Äì Test vs Validation MAPE (stability)
axes[1].scatter(results_df['Test_MAPE'], results_df['Val_MAPE'], c='purple')
for i, txt in enumerate(results_df['Group']):
    axes[1].annotate(txt,
        (results_df['Test_MAPE'].iloc[i], results_df['Val_MAPE'].iloc[i]))
axes[1].set_xlabel('Test MAPE')
axes[1].set_ylabel('Val MAPE')
axes[1].set_title('Stability: Test vs Validation MAPE')

# 3Ô∏è‚É£ Heatmap ‚Äì all metrics
sns.heatmap(results_df.set_index('Group')[['Test_MAPE','Val_MAPE','Stability_Index']],
            annot=True, fmt=".2f", cmap='RdYlGn_r', ax=axes[2])
axes[2].set_title('Heatmap of Forecast Metrics')

# 4Ô∏è‚É£ Stability Index Bar chart
axes[3].bar(results_df['Group'], results_df['Stability_Index'], color='orange')
axes[3].set_title('Stability Index (Lower = More Stable)')
axes[3].set_ylabel('Abs(Val MAPE - Test MAPE)')
axes[3].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()


This produces 4 views at once:
‚Äì Val MAPE bar chart
‚Äì Test vs Val scatter
‚Äì Heatmap of all metrics
‚Äì Stability Index bar chart

3Ô∏è‚É£ Reverse-engineering each view
View 1: Bar Chart ‚Äì Validation MAPE by Group

What we plot: Each group‚Äôs Val MAPE (Y axis), group name (X axis).

Why: This instantly shows CEO which business groups the model is most accurate on.

How: plt.barh() with Val_MAPE values.

Takeaway: Lower bars = better forecast accuracy.

View 2: Scatter Plot ‚Äì Test vs Validation MAPE

What we plot: X = Test MAPE, Y = Val MAPE. Each point = one group.

Why: Checks stability between test and validation.

How: plt.scatter() + annotate each point with group name.

Interpretation:

Points near the diagonal line (Val ‚âà Test) ‚Üí stable forecasts.

Points far above diagonal ‚Üí model degrades on validation.

Points below diagonal ‚Üí model improves on validation.

CEO-level message: Which segments hold up out-of-sample.

View 3: Heatmap ‚Äì Metrics Table with Colors

What we plot: Groups as rows, metrics as columns (Test_MAPE, Val_MAPE, Stability Index).

Why: Color-coded performance shows good (green) vs bad (red) segments at a glance.

How: sns.heatmap() of the metrics dataframe.

Interpretation:

Darker green = better.

Redder = worse.

CEO-level message: Immediate ‚Äúgreen/yellow/red‚Äù view of performance.

View 4: Bar Chart ‚Äì Stability Index

What we plot: |Val MAPE ‚Äì Test MAPE| for each group.

Why: Measures forecast stability. Lower = more stable.

How: plt.bar() with Stability_Index.

Interpretation:

Small bars = model behaves consistently.

Tall bars = unstable, group may be volatile or model weak.

CEO-level message: Which business segments are predictable vs volatile.

4Ô∏è‚É£ Optional Extra Views
A. Business Impact Chart (if you have group size):
if 'Group_Size' in results_df.columns:
    results_df['Impact'] = results_df['Group_Size'] * results_df['Val_MAPE']
    plt.figure(figsize=(10,5))
    plt.barh(results_df['Group'], results_df['Impact'], color='red')
    plt.title('Business Impact of Forecast Error')
    plt.xlabel('Impact (Group Size √ó Val MAPE)')
    plt.show()


This translates error into dollars or customer counts, which CEOs love.

B. Forecast Fan Chart (for one group)

Use your actual vs forecast data to show uncertainty bands. (We can do this once you have the forecast + confidence intervals.)

5Ô∏è‚É£ CEO-level narrative (tie it all together)

When you present:

Start with accuracy overview (bar chart).

Show stability vs test (scatter).

Show heatmap (one slide).

Show stability index (one slide).

Optional: business impact (one slide).

Frame as:

‚ÄúWe tested our forecasting system across 15 business groups.
‚Äì Achieved <X% error on 80% of groups.
‚Äì Identified 3 volatile segments for deeper investigation.
‚Äì Quantified potential revenue risk due to forecast error.
‚Äì Next step: integrate marketing & macro signals for further accuracy.‚Äù

TL;DR: What CEO sees
View	CEO takeaway
Val MAPE bar	Who‚Äôs accurate vs not
Test vs Val scatter	Stability of forecast
Heatmap	At-a-glance green/red performance
Stability index	Predictability vs volatility
Impact chart	$$$ / customer effect of error
Fan chart	We know our uncertainty, not just point forecasts













1Ô∏è‚É£ Bar Chart ‚Äì Validation MAPE per Group
import matplotlib.pyplot as plt
import seaborn as sns

# sort by Val_MAPE
results_df = results_df.sort_values('Val_MAPE')

plt.figure(figsize=(12,7))
plt.barh(results_df['Group'], results_df['Val_MAPE'], color='teal')
plt.title('Validation MAPE by Group (Forecast Accuracy)', fontsize=16)
plt.xlabel('Val MAPE')

# Add explanation box
plt.figtext(0.1, -0.1,
            'üìä Interpretation:\n'
            'Each bar shows the Validation MAPE (error %) per group.\n'
            'Lower bars = more accurate forecasts.\n'
            'Compare across groups to see which areas need improvement.',
            ha='left', fontsize=12, bbox=dict(facecolor='white', alpha=0.7))

plt.tight_layout()
plt.show()

2Ô∏è‚É£ Scatter Plot ‚Äì Test vs Validation MAPE
plt.figure(figsize=(10,8))
plt.scatter(results_df['Test_MAPE'], results_df['Val_MAPE'], c='purple', s=100)
for i, txt in enumerate(results_df['Group']):
    plt.annotate(txt,
                 (results_df['Test_MAPE'].iloc[i], results_df['Val_MAPE'].iloc[i]))

plt.xlabel('Test MAPE')
plt.ylabel('Val MAPE')
plt.title('Stability: Test vs Validation MAPE', fontsize=16)

# diagonal line for perfect stability
min_val = min(results_df['Test_MAPE'].min(), results_df['Val_MAPE'].min())
max_val = max(results_df['Test_MAPE'].max(), results_df['Val_MAPE'].max())
plt.plot([min_val, max_val],[min_val, max_val], 'r--')

# explanation box
plt.figtext(0.1, -0.1,
            'üìä Interpretation:\n'
            'Points near the dashed line = stable performance between test and validation.\n'
            'Above line = model degraded on validation.\n'
            'Below line = model improved on validation.',
            ha='left', fontsize=12, bbox=dict(facecolor='white', alpha=0.7))

plt.tight_layout()
plt.show()

3Ô∏è‚É£ Heatmap ‚Äì All Metrics
plt.figure(figsize=(12,7))
sns.heatmap(results_df.set_index('Group')[['Test_MAPE','Val_MAPE','Stability_Index']],
            annot=True, fmt=".2f", cmap='RdYlGn_r')
plt.title('Heatmap of Forecast Metrics', fontsize=16)

plt.figtext(0.1, -0.15,
            'üìä Interpretation:\n'
            'Green cells = better performance, Red = worse.\n'
            'Compare metrics across groups instantly.\n'
            'Use this to identify priority groups for improvement.',
            ha='left', fontsize=12, bbox=dict(facecolor='white', alpha=0.7))

plt.tight_layout()
plt.show()

4Ô∏è‚É£ Bar Chart ‚Äì Stability Index
plt.figure(figsize=(12,7))
plt.bar(results_df['Group'], results_df['Stability_Index'], color='orange')
plt.title('Stability Index (Lower = More Stable)', fontsize=16)
plt.ylabel('Abs(Val MAPE - Test MAPE)')
plt.xticks(rotation=45)

plt.figtext(0.1, -0.15,
            'üìä Interpretation:\n'
            'Lower bar = model behaves consistently between test and validation.\n'
            'Higher bar = unstable predictions (model or data volatility).\n'
            'Helps prioritize groups needing stability improvements.',
            ha='left', fontsize=12, bbox=dict(facecolor='white', alpha=0.7))

plt.tight_layout()
plt.show()

üîé How it works (reverse-engineer):

plt.figure(figsize=(w,h)) ‚Üí bigger chart.

plt.figtext(x, y, text, bbox=‚Ä¶) ‚Üí places a text box anywhere on the figure. We used y negative (below plot) to make it a footer.

bbox=dict(facecolor='white', alpha=0.7) ‚Üí makes a semi-transparent white box to highlight explanation.

Why separate: CEO gets one clean slide per chart, each with its own interpretation.


















1Ô∏è‚É£ Business Impact Bubble Chart

üîπ Why CEOs love it: Shows where error actually costs the most.
üîπ Idea: X = Validation MAPE, Y = Group Size (or Revenue), bubble size = Test RMSE.

plt.figure(figsize=(10,8))
plt.scatter(results_df['Val_MAPE'], results_df['Group_Size'],
            s=results_df['Test_RMSE']*500, alpha=0.6, c='skyblue', edgecolors='black')

for i, txt in enumerate(results_df['Group']):
    plt.annotate(txt, (results_df['Val_MAPE'].iloc[i], results_df['Group_Size'].iloc[i]))

plt.xlabel('Validation MAPE (%)')
plt.ylabel('Group Size / Revenue')
plt.title('Forecast Error vs Business Impact')

plt.figtext(0.1, -0.1,
            'Big bubbles = high RMSE. Top right quadrant = high error + big group = highest risk.\n'
            'Bottom left quadrant = low error + small group = safest.',
            ha='left', fontsize=11, bbox=dict(facecolor='white', alpha=0.7))
plt.tight_layout()
plt.show()

2Ô∏è‚É£ Ranking Table with Icons (Red/Green Arrows)

üîπ Why CEOs love it: Easy to see winners vs laggards.
üîπ Idea: Create a DataFrame sorted by Val_MAPE and add ‚ñ≤/‚ñº arrows for improvement vs test.

results_df['Improvement'] = results_df['Test_MAPE'] - results_df['Val_MAPE']
results_df['Status'] = results_df['Improvement'].apply(lambda x: '‚ñ≤ Improved' if x>0 else '‚ñº Degraded')
styled = results_df[['Group','Test_MAPE','Val_MAPE','Improvement','Status']]\
            .style.background_gradient(cmap='RdYlGn_r', subset=['Val_MAPE'])\
            .format({'Test_MAPE':'{:.2f}','Val_MAPE':'{:.2f}','Improvement':'{:.2f}'})
styled


This renders an HTML table with red/green backgrounds ‚Äî perfect for dashboards.

3Ô∏è‚É£ Rolling Error Trend per Group (Time Series View)

üîπ Why CEOs love it: Shows how error evolves over time, not just one number.
üîπ Idea: Plot actual vs forecast or rolling MAPE per group month by month.

# Suppose you have df with columns: ['Date','Group','Actual','Forecast']
df['AbsPercError'] = (abs(df['Actual'] - df['Forecast']) / df['Actual'])*100
sns.lineplot(data=df, x='Date', y='AbsPercError', hue='Group')
plt.title('Rolling Forecast Error Over Time by Group')
plt.ylabel('Absolute % Error')
plt.show()

4Ô∏è‚É£ Stability Quadrant Chart (Predictability vs Volatility)

üîπ Why CEOs love it: It‚Äôs a 2√ó2 matrix of ‚ÄúStable & Accurate‚Äù vs ‚ÄúUnstable & High Error‚Äù.
üîπ Idea:

X axis = Stability Index (low = stable).

Y axis = Val MAPE (low = accurate).

Quadrants automatically highlight risk vs safety.

plt.figure(figsize=(10,8))
plt.scatter(results_df['Stability_Index'], results_df['Val_MAPE'], c='orange', s=100)
plt.axhline(results_df['Val_MAPE'].median(), color='red', linestyle='--')
plt.axvline(results_df['Stability_Index'].median(), color='red', linestyle='--')
for i, txt in enumerate(results_df['Group']):
    plt.annotate(txt, (results_df['Stability_Index'].iloc[i], results_df['Val_MAPE'].iloc[i]))
plt.xlabel('Stability Index (Low=Stable)')
plt.ylabel('Validation MAPE (Low=Accurate)')
plt.title('Accuracy vs Stability Quadrant')
plt.figtext(0.1, -0.1,
            'Bottom-left quadrant = best groups (stable & accurate).\n'
            'Top-right quadrant = worst groups (unstable & inaccurate).',
            ha='left', fontsize=11, bbox=dict(facecolor='white', alpha=0.7))
plt.tight_layout()
plt.show()

5Ô∏è‚É£ Waterfall Chart ‚Äì Error Contribution to Total Risk

üîπ Why CEOs love it: Shows which groups contribute most to total forecast error.
üîπ Idea: Combine Val MAPE √ó Group Size to compute ‚Äúerror impact‚Äù and plot as waterfall.

results_df['Impact'] = results_df['Group_Size'] * results_df['Val_MAPE']
impact_sorted = results_df.sort_values('Impact', ascending=False)

plt.figure(figsize=(12,6))
plt.bar(impact_sorted['Group'], impact_sorted['Impact'], color='crimson')
plt.xticks(rotation=45)
plt.title('Total Forecast Error Impact by Group')
plt.ylabel('Impact (Group Size √ó Val MAPE)')
plt.figtext(0.1, -0.2,
            'Left bars = biggest contributors to total error.\n'
            'Helps prioritize which groups to focus on.',
            ha='left', fontsize=11, bbox=dict(facecolor='white', alpha=0.7))
plt.tight_layout()
plt.show()

6Ô∏è‚É£ Uncertainty Fan Chart (for a single group)

üîπ Why CEOs love it: Shows forecast + confidence intervals ‚Üí conveys risk transparently.
üîπ Idea: Use ARIMA‚Äôs .get_forecast() to retrieve conf_int.

forecast_obj = model.get_forecast(steps=len(val_series))
forecast_mean = forecast_obj.predicted_mean
conf_int = forecast_obj.conf_int()

plt.figure(figsize=(12,6))
plt.plot(train_series.index, train_series, label='Train')
plt.plot(test_series.index, test_series, label='Test Actual')
plt.plot(val_series.index, val_series, label='Validation Actual')
plt.plot(forecast_mean.index, forecast_mean, label='Forecast', color='black')
plt.fill_between(conf_int.index, conf_int.iloc[:,0], conf_int.iloc[:,1],
                 color='gray', alpha=0.3, label='95% Confidence Interval')
plt.legend()
plt.title('Forecast with Uncertainty Band')
plt.figtext(0.1, -0.1,
            'The shaded area shows 95% confidence interval.\n'
            'Narrow bands = high certainty, Wide bands = high uncertainty.',
            ha='left', fontsize=11, bbox=dict(facecolor='white', alpha=0.7))
plt.tight_layout()
plt.show()

7Ô∏è‚É£ Pareto (80/20) Chart of Error

üîπ Why CEOs love it: Shows that a few groups cause most of the forecast error (the ‚Äú80/20 rule‚Äù).

impact_sorted['CumulativeImpact'] = impact_sorted['Impact'].cumsum()/impact_sorted['Impact'].sum()*100

fig, ax1 = plt.subplots(figsize=(12,6))
ax1.bar(impact_sorted['Group'], impact_sorted['Impact'], color='steelblue')
ax2 = ax1.twinx()
ax2.plot(impact_sorted['Group'], impact_sorted['CumulativeImpact'], color='red', marker='o')
ax1.set_ylabel('Impact')
ax2.set_ylabel('Cumulative %')
ax1.set_title('Pareto Analysis of Forecast Error Impact')
ax1.tick_params(axis='x', rotation=45)

plt.figtext(0.1, -0.1,
            'Blue bars = impact per group. Red line = cumulative contribution.\n'
            'Use this to see which top few groups drive most error.',
            ha='left', fontsize=11, bbox=dict(facecolor='white', alpha=0.7))
plt.tight_layout()
plt.show()

üí° How to ‚ÄúTell the Story‚Äù to CEO:
Chart Idea	CEO Takeaway
Bubble Chart	‚ÄúWhere is error costing us the most money.‚Äù
Ranking Table	‚ÄúGreen arrows = improving segments.‚Äù
Rolling Error Trend	‚ÄúWe‚Äôre trending down error over time.‚Äù
Stability Quadrant	‚ÄúSafe vs risky groups at a glance.‚Äù
Waterfall Chart	‚ÄúFocus efforts where risk is concentrated.‚Äù
Fan Chart	‚ÄúWe know our uncertainty and can plan around it.‚Äù
Pareto Chart	‚Äú80% of error comes from 20% of groups.‚Äù
üéØ Executive framing tips:

Translate % error to dollars, customers, or risk ‚Üí CEOs think in impact not error.

Color code green/yellow/red consistently across all visuals.

Show trends not just snapshots ‚Üí trending down shows progress.

Show uncertainty ‚Üí indicates maturity of forecasting process.





