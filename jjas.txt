1Ô∏è‚É£ Starting point

You already have:

results_df = pd.DataFrame(results,
                          columns=['Group','Test_RMSE','Test_MAPE','Val_RMSE','Val_MAPE'])


We‚Äôll add a Stability Index and (if you have it) a Group Size or Weight column to show business impact.

# Add Stability Index (difference between Val & Test MAPE)
results_df['Stability_Index'] = (results_df['Val_MAPE'] - results_df['Test_MAPE']).abs()

# (Optional) Add Group Size column to show business impact
# results_df['Group_Size'] = [volume per group]  # supply if you have

2Ô∏è‚É£ Combined Dashboard Code
import matplotlib.pyplot as plt
import seaborn as sns

# sort groups by Val_MAPE for consistent ordering
results_df = results_df.sort_values('Val_MAPE')

fig, axes = plt.subplots(2, 2, figsize=(15, 10))
axes = axes.flatten()

# 1Ô∏è‚É£ Bar chart ‚Äì Validation MAPE per group
axes[0].barh(results_df['Group'], results_df['Val_MAPE'], color='teal')
axes[0].set_title('Validation MAPE by Group (Forecast Accuracy)')
axes[0].set_xlabel('Val MAPE (%)')

# 2Ô∏è‚É£ Scatter plot ‚Äì Test vs Validation MAPE (stability)
axes[1].scatter(results_df['Test_MAPE'], results_df['Val_MAPE'], c='purple')
for i, txt in enumerate(results_df['Group']):
    axes[1].annotate(txt,
        (results_df['Test_MAPE'].iloc[i], results_df['Val_MAPE'].iloc[i]))
axes[1].set_xlabel('Test MAPE')
axes[1].set_ylabel('Val MAPE')
axes[1].set_title('Stability: Test vs Validation MAPE')

# 3Ô∏è‚É£ Heatmap ‚Äì all metrics
sns.heatmap(results_df.set_index('Group')[['Test_MAPE','Val_MAPE','Stability_Index']],
            annot=True, fmt=".2f", cmap='RdYlGn_r', ax=axes[2])
axes[2].set_title('Heatmap of Forecast Metrics')

# 4Ô∏è‚É£ Stability Index Bar chart
axes[3].bar(results_df['Group'], results_df['Stability_Index'], color='orange')
axes[3].set_title('Stability Index (Lower = More Stable)')
axes[3].set_ylabel('Abs(Val MAPE - Test MAPE)')
axes[3].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()


This produces 4 views at once:
‚Äì Val MAPE bar chart
‚Äì Test vs Val scatter
‚Äì Heatmap of all metrics
‚Äì Stability Index bar chart

3Ô∏è‚É£ Reverse-engineering each view
View 1: Bar Chart ‚Äì Validation MAPE by Group

What we plot: Each group‚Äôs Val MAPE (Y axis), group name (X axis).

Why: This instantly shows CEO which business groups the model is most accurate on.

How: plt.barh() with Val_MAPE values.

Takeaway: Lower bars = better forecast accuracy.

View 2: Scatter Plot ‚Äì Test vs Validation MAPE

What we plot: X = Test MAPE, Y = Val MAPE. Each point = one group.

Why: Checks stability between test and validation.

How: plt.scatter() + annotate each point with group name.

Interpretation:

Points near the diagonal line (Val ‚âà Test) ‚Üí stable forecasts.

Points far above diagonal ‚Üí model degrades on validation.

Points below diagonal ‚Üí model improves on validation.

CEO-level message: Which segments hold up out-of-sample.

View 3: Heatmap ‚Äì Metrics Table with Colors

What we plot: Groups as rows, metrics as columns (Test_MAPE, Val_MAPE, Stability Index).

Why: Color-coded performance shows good (green) vs bad (red) segments at a glance.

How: sns.heatmap() of the metrics dataframe.

Interpretation:

Darker green = better.

Redder = worse.

CEO-level message: Immediate ‚Äúgreen/yellow/red‚Äù view of performance.

View 4: Bar Chart ‚Äì Stability Index

What we plot: |Val MAPE ‚Äì Test MAPE| for each group.

Why: Measures forecast stability. Lower = more stable.

How: plt.bar() with Stability_Index.

Interpretation:

Small bars = model behaves consistently.

Tall bars = unstable, group may be volatile or model weak.

CEO-level message: Which business segments are predictable vs volatile.

4Ô∏è‚É£ Optional Extra Views
A. Business Impact Chart (if you have group size):
if 'Group_Size' in results_df.columns:
    results_df['Impact'] = results_df['Group_Size'] * results_df['Val_MAPE']
    plt.figure(figsize=(10,5))
    plt.barh(results_df['Group'], results_df['Impact'], color='red')
    plt.title('Business Impact of Forecast Error')
    plt.xlabel('Impact (Group Size √ó Val MAPE)')
    plt.show()


This translates error into dollars or customer counts, which CEOs love.

B. Forecast Fan Chart (for one group)

Use your actual vs forecast data to show uncertainty bands. (We can do this once you have the forecast + confidence intervals.)

5Ô∏è‚É£ CEO-level narrative (tie it all together)

When you present:

Start with accuracy overview (bar chart).

Show stability vs test (scatter).

Show heatmap (one slide).

Show stability index (one slide).

Optional: business impact (one slide).

Frame as:

‚ÄúWe tested our forecasting system across 15 business groups.
‚Äì Achieved <X% error on 80% of groups.
‚Äì Identified 3 volatile segments for deeper investigation.
‚Äì Quantified potential revenue risk due to forecast error.
‚Äì Next step: integrate marketing & macro signals for further accuracy.‚Äù

TL;DR: What CEO sees
View	CEO takeaway
Val MAPE bar	Who‚Äôs accurate vs not
Test vs Val scatter	Stability of forecast
Heatmap	At-a-glance green/red performance
Stability index	Predictability vs volatility
Impact chart	$$$ / customer effect of error
Fan chart	We know our uncertainty, not just point forecasts













1Ô∏è‚É£ Bar Chart ‚Äì Validation MAPE per Group
import matplotlib.pyplot as plt
import seaborn as sns

# sort by Val_MAPE
results_df = results_df.sort_values('Val_MAPE')

plt.figure(figsize=(12,7))
plt.barh(results_df['Group'], results_df['Val_MAPE'], color='teal')
plt.title('Validation MAPE by Group (Forecast Accuracy)', fontsize=16)
plt.xlabel('Val MAPE')

# Add explanation box
plt.figtext(0.1, -0.1,
            'üìä Interpretation:\n'
            'Each bar shows the Validation MAPE (error %) per group.\n'
            'Lower bars = more accurate forecasts.\n'
            'Compare across groups to see which areas need improvement.',
            ha='left', fontsize=12, bbox=dict(facecolor='white', alpha=0.7))

plt.tight_layout()
plt.show()

2Ô∏è‚É£ Scatter Plot ‚Äì Test vs Validation MAPE
plt.figure(figsize=(10,8))
plt.scatter(results_df['Test_MAPE'], results_df['Val_MAPE'], c='purple', s=100)
for i, txt in enumerate(results_df['Group']):
    plt.annotate(txt,
                 (results_df['Test_MAPE'].iloc[i], results_df['Val_MAPE'].iloc[i]))

plt.xlabel('Test MAPE')
plt.ylabel('Val MAPE')
plt.title('Stability: Test vs Validation MAPE', fontsize=16)

# diagonal line for perfect stability
min_val = min(results_df['Test_MAPE'].min(), results_df['Val_MAPE'].min())
max_val = max(results_df['Test_MAPE'].max(), results_df['Val_MAPE'].max())
plt.plot([min_val, max_val],[min_val, max_val], 'r--')

# explanation box
plt.figtext(0.1, -0.1,
            'üìä Interpretation:\n'
            'Points near the dashed line = stable performance between test and validation.\n'
            'Above line = model degraded on validation.\n'
            'Below line = model improved on validation.',
            ha='left', fontsize=12, bbox=dict(facecolor='white', alpha=0.7))

plt.tight_layout()
plt.show()

3Ô∏è‚É£ Heatmap ‚Äì All Metrics
plt.figure(figsize=(12,7))
sns.heatmap(results_df.set_index('Group')[['Test_MAPE','Val_MAPE','Stability_Index']],
            annot=True, fmt=".2f", cmap='RdYlGn_r')
plt.title('Heatmap of Forecast Metrics', fontsize=16)

plt.figtext(0.1, -0.15,
            'üìä Interpretation:\n'
            'Green cells = better performance, Red = worse.\n'
            'Compare metrics across groups instantly.\n'
            'Use this to identify priority groups for improvement.',
            ha='left', fontsize=12, bbox=dict(facecolor='white', alpha=0.7))

plt.tight_layout()
plt.show()

4Ô∏è‚É£ Bar Chart ‚Äì Stability Index
plt.figure(figsize=(12,7))
plt.bar(results_df['Group'], results_df['Stability_Index'], color='orange')
plt.title('Stability Index (Lower = More Stable)', fontsize=16)
plt.ylabel('Abs(Val MAPE - Test MAPE)')
plt.xticks(rotation=45)

plt.figtext(0.1, -0.15,
            'üìä Interpretation:\n'
            'Lower bar = model behaves consistently between test and validation.\n'
            'Higher bar = unstable predictions (model or data volatility).\n'
            'Helps prioritize groups needing stability improvements.',
            ha='left', fontsize=12, bbox=dict(facecolor='white', alpha=0.7))

plt.tight_layout()
plt.show()

üîé How it works (reverse-engineer):

plt.figure(figsize=(w,h)) ‚Üí bigger chart.

plt.figtext(x, y, text, bbox=‚Ä¶) ‚Üí places a text box anywhere on the figure. We used y negative (below plot) to make it a footer.

bbox=dict(facecolor='white', alpha=0.7) ‚Üí makes a semi-transparent white box to highlight explanation.

Why separate: CEO gets one clean slide per chart, each with its own interpretation.



