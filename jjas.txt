1Ô∏è‚É£ Starting point

You already have:

results_df = pd.DataFrame(results,
                          columns=['Group','Test_RMSE','Test_MAPE','Val_RMSE','Val_MAPE'])


We‚Äôll add a Stability Index and (if you have it) a Group Size or Weight column to show business impact.

# Add Stability Index (difference between Val & Test MAPE)
results_df['Stability_Index'] = (results_df['Val_MAPE'] - results_df['Test_MAPE']).abs()

# (Optional) Add Group Size column to show business impact
# results_df['Group_Size'] = [volume per group]  # supply if you have

2Ô∏è‚É£ Combined Dashboard Code
import matplotlib.pyplot as plt
import seaborn as sns

# sort groups by Val_MAPE for consistent ordering
results_df = results_df.sort_values('Val_MAPE')

fig, axes = plt.subplots(2, 2, figsize=(15, 10))
axes = axes.flatten()

# 1Ô∏è‚É£ Bar chart ‚Äì Validation MAPE per group
axes[0].barh(results_df['Group'], results_df['Val_MAPE'], color='teal')
axes[0].set_title('Validation MAPE by Group (Forecast Accuracy)')
axes[0].set_xlabel('Val MAPE (%)')

# 2Ô∏è‚É£ Scatter plot ‚Äì Test vs Validation MAPE (stability)
axes[1].scatter(results_df['Test_MAPE'], results_df['Val_MAPE'], c='purple')
for i, txt in enumerate(results_df['Group']):
    axes[1].annotate(txt,
        (results_df['Test_MAPE'].iloc[i], results_df['Val_MAPE'].iloc[i]))
axes[1].set_xlabel('Test MAPE')
axes[1].set_ylabel('Val MAPE')
axes[1].set_title('Stability: Test vs Validation MAPE')

# 3Ô∏è‚É£ Heatmap ‚Äì all metrics
sns.heatmap(results_df.set_index('Group')[['Test_MAPE','Val_MAPE','Stability_Index']],
            annot=True, fmt=".2f", cmap='RdYlGn_r', ax=axes[2])
axes[2].set_title('Heatmap of Forecast Metrics')

# 4Ô∏è‚É£ Stability Index Bar chart
axes[3].bar(results_df['Group'], results_df['Stability_Index'], color='orange')
axes[3].set_title('Stability Index (Lower = More Stable)')
axes[3].set_ylabel('Abs(Val MAPE - Test MAPE)')
axes[3].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()


This produces 4 views at once:
‚Äì Val MAPE bar chart
‚Äì Test vs Val scatter
‚Äì Heatmap of all metrics
‚Äì Stability Index bar chart

3Ô∏è‚É£ Reverse-engineering each view
View 1: Bar Chart ‚Äì Validation MAPE by Group

What we plot: Each group‚Äôs Val MAPE (Y axis), group name (X axis).

Why: This instantly shows CEO which business groups the model is most accurate on.

How: plt.barh() with Val_MAPE values.

Takeaway: Lower bars = better forecast accuracy.

View 2: Scatter Plot ‚Äì Test vs Validation MAPE

What we plot: X = Test MAPE, Y = Val MAPE. Each point = one group.

Why: Checks stability between test and validation.

How: plt.scatter() + annotate each point with group name.

Interpretation:

Points near the diagonal line (Val ‚âà Test) ‚Üí stable forecasts.

Points far above diagonal ‚Üí model degrades on validation.

Points below diagonal ‚Üí model improves on validation.

CEO-level message: Which segments hold up out-of-sample.

View 3: Heatmap ‚Äì Metrics Table with Colors

What we plot: Groups as rows, metrics as columns (Test_MAPE, Val_MAPE, Stability Index).

Why: Color-coded performance shows good (green) vs bad (red) segments at a glance.

How: sns.heatmap() of the metrics dataframe.

Interpretation:

Darker green = better.

Redder = worse.

CEO-level message: Immediate ‚Äúgreen/yellow/red‚Äù view of performance.

View 4: Bar Chart ‚Äì Stability Index

What we plot: |Val MAPE ‚Äì Test MAPE| for each group.

Why: Measures forecast stability. Lower = more stable.

How: plt.bar() with Stability_Index.

Interpretation:

Small bars = model behaves consistently.

Tall bars = unstable, group may be volatile or model weak.

CEO-level message: Which business segments are predictable vs volatile.

4Ô∏è‚É£ Optional Extra Views
A. Business Impact Chart (if you have group size):
if 'Group_Size' in results_df.columns:
    results_df['Impact'] = results_df['Group_Size'] * results_df['Val_MAPE']
    plt.figure(figsize=(10,5))
    plt.barh(results_df['Group'], results_df['Impact'], color='red')
    plt.title('Business Impact of Forecast Error')
    plt.xlabel('Impact (Group Size √ó Val MAPE)')
    plt.show()


This translates error into dollars or customer counts, which CEOs love.

B. Forecast Fan Chart (for one group)

Use your actual vs forecast data to show uncertainty bands. (We can do this once you have the forecast + confidence intervals.)

5Ô∏è‚É£ CEO-level narrative (tie it all together)

When you present:

Start with accuracy overview (bar chart).

Show stability vs test (scatter).

Show heatmap (one slide).

Show stability index (one slide).

Optional: business impact (one slide).

Frame as:

‚ÄúWe tested our forecasting system across 15 business groups.
‚Äì Achieved <X% error on 80% of groups.
‚Äì Identified 3 volatile segments for deeper investigation.
‚Äì Quantified potential revenue risk due to forecast error.
‚Äì Next step: integrate marketing & macro signals for further accuracy.‚Äù

TL;DR: What CEO sees
View	CEO takeaway
Val MAPE bar	Who‚Äôs accurate vs not
Test vs Val scatter	Stability of forecast
Heatmap	At-a-glance green/red performance
Stability index	Predictability vs volatility
Impact chart	$$$ / customer effect of error
Fan chart	We know our uncertainty, not just point forecasts













1Ô∏è‚É£ Bar Chart ‚Äì Validation MAPE per Group
import matplotlib.pyplot as plt
import seaborn as sns

# sort by Val_MAPE
results_df = results_df.sort_values('Val_MAPE')

plt.figure(figsize=(12,7))
plt.barh(results_df['Group'], results_df['Val_MAPE'], color='teal')
plt.title('Validation MAPE by Group (Forecast Accuracy)', fontsize=16)
plt.xlabel('Val MAPE')

# Add explanation box
plt.figtext(0.1, -0.1,
            'üìä Interpretation:\n'
            'Each bar shows the Validation MAPE (error %) per group.\n'
            'Lower bars = more accurate forecasts.\n'
            'Compare across groups to see which areas need improvement.',
            ha='left', fontsize=12, bbox=dict(facecolor='white', alpha=0.7))

plt.tight_layout()
plt.show()

2Ô∏è‚É£ Scatter Plot ‚Äì Test vs Validation MAPE
plt.figure(figsize=(10,8))
plt.scatter(results_df['Test_MAPE'], results_df['Val_MAPE'], c='purple', s=100)
for i, txt in enumerate(results_df['Group']):
    plt.annotate(txt,
                 (results_df['Test_MAPE'].iloc[i], results_df['Val_MAPE'].iloc[i]))

plt.xlabel('Test MAPE')
plt.ylabel('Val MAPE')
plt.title('Stability: Test vs Validation MAPE', fontsize=16)

# diagonal line for perfect stability
min_val = min(results_df['Test_MAPE'].min(), results_df['Val_MAPE'].min())
max_val = max(results_df['Test_MAPE'].max(), results_df['Val_MAPE'].max())
plt.plot([min_val, max_val],[min_val, max_val], 'r--')

# explanation box
plt.figtext(0.1, -0.1,
            'üìä Interpretation:\n'
            'Points near the dashed line = stable performance between test and validation.\n'
            'Above line = model degraded on validation.\n'
            'Below line = model improved on validation.',
            ha='left', fontsize=12, bbox=dict(facecolor='white', alpha=0.7))

plt.tight_layout()
plt.show()

3Ô∏è‚É£ Heatmap ‚Äì All Metrics
plt.figure(figsize=(12,7))
sns.heatmap(results_df.set_index('Group')[['Test_MAPE','Val_MAPE','Stability_Index']],
            annot=True, fmt=".2f", cmap='RdYlGn_r')
plt.title('Heatmap of Forecast Metrics', fontsize=16)

plt.figtext(0.1, -0.15,
            'üìä Interpretation:\n'
            'Green cells = better performance, Red = worse.\n'
            'Compare metrics across groups instantly.\n'
            'Use this to identify priority groups for improvement.',
            ha='left', fontsize=12, bbox=dict(facecolor='white', alpha=0.7))

plt.tight_layout()
plt.show()

4Ô∏è‚É£ Bar Chart ‚Äì Stability Index
plt.figure(figsize=(12,7))
plt.bar(results_df['Group'], results_df['Stability_Index'], color='orange')
plt.title('Stability Index (Lower = More Stable)', fontsize=16)
plt.ylabel('Abs(Val MAPE - Test MAPE)')
plt.xticks(rotation=45)

plt.figtext(0.1, -0.15,
            'üìä Interpretation:\n'
            'Lower bar = model behaves consistently between test and validation.\n'
            'Higher bar = unstable predictions (model or data volatility).\n'
            'Helps prioritize groups needing stability improvements.',
            ha='left', fontsize=12, bbox=dict(facecolor='white', alpha=0.7))

plt.tight_layout()
plt.show()

üîé How it works (reverse-engineer):

plt.figure(figsize=(w,h)) ‚Üí bigger chart.

plt.figtext(x, y, text, bbox=‚Ä¶) ‚Üí places a text box anywhere on the figure. We used y negative (below plot) to make it a footer.

bbox=dict(facecolor='white', alpha=0.7) ‚Üí makes a semi-transparent white box to highlight explanation.

Why separate: CEO gets one clean slide per chart, each with its own interpretation.


















1Ô∏è‚É£ Business Impact Bubble Chart

üîπ Why CEOs love it: Shows where error actually costs the most.
üîπ Idea: X = Validation MAPE, Y = Group Size (or Revenue), bubble size = Test RMSE.

plt.figure(figsize=(10,8))
plt.scatter(results_df['Val_MAPE'], results_df['Group_Size'],
            s=results_df['Test_RMSE']*500, alpha=0.6, c='skyblue', edgecolors='black')

for i, txt in enumerate(results_df['Group']):
    plt.annotate(txt, (results_df['Val_MAPE'].iloc[i], results_df['Group_Size'].iloc[i]))

plt.xlabel('Validation MAPE (%)')
plt.ylabel('Group Size / Revenue')
plt.title('Forecast Error vs Business Impact')

plt.figtext(0.1, -0.1,
            'Big bubbles = high RMSE. Top right quadrant = high error + big group = highest risk.\n'
            'Bottom left quadrant = low error + small group = safest.',
            ha='left', fontsize=11, bbox=dict(facecolor='white', alpha=0.7))
plt.tight_layout()
plt.show()

2Ô∏è‚É£ Ranking Table with Icons (Red/Green Arrows)

üîπ Why CEOs love it: Easy to see winners vs laggards.
üîπ Idea: Create a DataFrame sorted by Val_MAPE and add ‚ñ≤/‚ñº arrows for improvement vs test.

results_df['Improvement'] = results_df['Test_MAPE'] - results_df['Val_MAPE']
results_df['Status'] = results_df['Improvement'].apply(lambda x: '‚ñ≤ Improved' if x>0 else '‚ñº Degraded')
styled = results_df[['Group','Test_MAPE','Val_MAPE','Improvement','Status']]\
            .style.background_gradient(cmap='RdYlGn_r', subset=['Val_MAPE'])\
            .format({'Test_MAPE':'{:.2f}','Val_MAPE':'{:.2f}','Improvement':'{:.2f}'})
styled


This renders an HTML table with red/green backgrounds ‚Äî perfect for dashboards.

3Ô∏è‚É£ Rolling Error Trend per Group (Time Series View)

üîπ Why CEOs love it: Shows how error evolves over time, not just one number.
üîπ Idea: Plot actual vs forecast or rolling MAPE per group month by month.

# Suppose you have df with columns: ['Date','Group','Actual','Forecast']
df['AbsPercError'] = (abs(df['Actual'] - df['Forecast']) / df['Actual'])*100
sns.lineplot(data=df, x='Date', y='AbsPercError', hue='Group')
plt.title('Rolling Forecast Error Over Time by Group')
plt.ylabel('Absolute % Error')
plt.show()

4Ô∏è‚É£ Stability Quadrant Chart (Predictability vs Volatility)

üîπ Why CEOs love it: It‚Äôs a 2√ó2 matrix of ‚ÄúStable & Accurate‚Äù vs ‚ÄúUnstable & High Error‚Äù.
üîπ Idea:

X axis = Stability Index (low = stable).

Y axis = Val MAPE (low = accurate).

Quadrants automatically highlight risk vs safety.

plt.figure(figsize=(10,8))
plt.scatter(results_df['Stability_Index'], results_df['Val_MAPE'], c='orange', s=100)
plt.axhline(results_df['Val_MAPE'].median(), color='red', linestyle='--')
plt.axvline(results_df['Stability_Index'].median(), color='red', linestyle='--')
for i, txt in enumerate(results_df['Group']):
    plt.annotate(txt, (results_df['Stability_Index'].iloc[i], results_df['Val_MAPE'].iloc[i]))
plt.xlabel('Stability Index (Low=Stable)')
plt.ylabel('Validation MAPE (Low=Accurate)')
plt.title('Accuracy vs Stability Quadrant')
plt.figtext(0.1, -0.1,
            'Bottom-left quadrant = best groups (stable & accurate).\n'
            'Top-right quadrant = worst groups (unstable & inaccurate).',
            ha='left', fontsize=11, bbox=dict(facecolor='white', alpha=0.7))
plt.tight_layout()
plt.show()

5Ô∏è‚É£ Waterfall Chart ‚Äì Error Contribution to Total Risk

üîπ Why CEOs love it: Shows which groups contribute most to total forecast error.
üîπ Idea: Combine Val MAPE √ó Group Size to compute ‚Äúerror impact‚Äù and plot as waterfall.

results_df['Impact'] = results_df['Group_Size'] * results_df['Val_MAPE']
impact_sorted = results_df.sort_values('Impact', ascending=False)

plt.figure(figsize=(12,6))
plt.bar(impact_sorted['Group'], impact_sorted['Impact'], color='crimson')
plt.xticks(rotation=45)
plt.title('Total Forecast Error Impact by Group')
plt.ylabel('Impact (Group Size √ó Val MAPE)')
plt.figtext(0.1, -0.2,
            'Left bars = biggest contributors to total error.\n'
            'Helps prioritize which groups to focus on.',
            ha='left', fontsize=11, bbox=dict(facecolor='white', alpha=0.7))
plt.tight_layout()
plt.show()

6Ô∏è‚É£ Uncertainty Fan Chart (for a single group)

üîπ Why CEOs love it: Shows forecast + confidence intervals ‚Üí conveys risk transparently.
üîπ Idea: Use ARIMA‚Äôs .get_forecast() to retrieve conf_int.

forecast_obj = model.get_forecast(steps=len(val_series))
forecast_mean = forecast_obj.predicted_mean
conf_int = forecast_obj.conf_int()

plt.figure(figsize=(12,6))
plt.plot(train_series.index, train_series, label='Train')
plt.plot(test_series.index, test_series, label='Test Actual')
plt.plot(val_series.index, val_series, label='Validation Actual')
plt.plot(forecast_mean.index, forecast_mean, label='Forecast', color='black')
plt.fill_between(conf_int.index, conf_int.iloc[:,0], conf_int.iloc[:,1],
                 color='gray', alpha=0.3, label='95% Confidence Interval')
plt.legend()
plt.title('Forecast with Uncertainty Band')
plt.figtext(0.1, -0.1,
            'The shaded area shows 95% confidence interval.\n'
            'Narrow bands = high certainty, Wide bands = high uncertainty.',
            ha='left', fontsize=11, bbox=dict(facecolor='white', alpha=0.7))
plt.tight_layout()
plt.show()

7Ô∏è‚É£ Pareto (80/20) Chart of Error

üîπ Why CEOs love it: Shows that a few groups cause most of the forecast error (the ‚Äú80/20 rule‚Äù).

impact_sorted['CumulativeImpact'] = impact_sorted['Impact'].cumsum()/impact_sorted['Impact'].sum()*100

fig, ax1 = plt.subplots(figsize=(12,6))
ax1.bar(impact_sorted['Group'], impact_sorted['Impact'], color='steelblue')
ax2 = ax1.twinx()
ax2.plot(impact_sorted['Group'], impact_sorted['CumulativeImpact'], color='red', marker='o')
ax1.set_ylabel('Impact')
ax2.set_ylabel('Cumulative %')
ax1.set_title('Pareto Analysis of Forecast Error Impact')
ax1.tick_params(axis='x', rotation=45)

plt.figtext(0.1, -0.1,
            'Blue bars = impact per group. Red line = cumulative contribution.\n'
            'Use this to see which top few groups drive most error.',
            ha='left', fontsize=11, bbox=dict(facecolor='white', alpha=0.7))
plt.tight_layout()
plt.show()

üí° How to ‚ÄúTell the Story‚Äù to CEO:
Chart Idea	CEO Takeaway
Bubble Chart	‚ÄúWhere is error costing us the most money.‚Äù
Ranking Table	‚ÄúGreen arrows = improving segments.‚Äù
Rolling Error Trend	‚ÄúWe‚Äôre trending down error over time.‚Äù
Stability Quadrant	‚ÄúSafe vs risky groups at a glance.‚Äù
Waterfall Chart	‚ÄúFocus efforts where risk is concentrated.‚Äù
Fan Chart	‚ÄúWe know our uncertainty and can plan around it.‚Äù
Pareto Chart	‚Äú80% of error comes from 20% of groups.‚Äù
üéØ Executive framing tips:

Translate % error to dollars, customers, or risk ‚Üí CEOs think in impact not error.

Color code green/yellow/red consistently across all visuals.

Show trends not just snapshots ‚Üí trending down shows progress.

Show uncertainty ‚Üí indicates maturity of forecasting process.











from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
plot_acf(model.resid, lags=24)
plt.show()






1Ô∏è‚É£ What ARIMA / SARIMAX actually is

At its core ARIMA is a way of saying:

‚ÄúThis month‚Äôs value can be explained by (a) some previous months‚Äô values,
(b) some past forecast errors,
and (c) seasonal adjustments.‚Äù

AR = AutoRegressive = use previous values (lags).

I = Integrated = differences to remove trend/seasonality.

MA = Moving Average = use previous errors (shocks).

SARIMA = Seasonal ARIMA ‚Äî same but explicitly models seasonal patterns (like 12 months cycle).
SARIMAX = same but can add extra regressors (the ‚ÄúX‚Äù for exogenous).

2Ô∏è‚É£ How to read the model notation

Your model says:

SARIMAX(2,0,1)x(0,1,0,12)


Break it:

Non-seasonal part (p,d,q)=(2,0,1)

p=2 ‚Üí two AR lags (use y[t‚àí1], y[t‚àí2])

d=0 ‚Üí no regular differencing (series stationary except for seasonal)

q=1 ‚Üí one MA lag (use past forecast error e[t‚àí1])

Seasonal part (P,D,Q,m)=(0,1,0,12)

D=1 ‚Üí you did seasonal differencing at lag 12 ‚Üí (y[t]‚àíy[t‚àí12]) to remove yearly pattern

P=0,Q=0 ‚Üí no seasonal AR/MA terms

m=12 ‚Üí monthly seasonality

So you seasonally differenced once at lag 12 and then fit an AR(2)+MA(1) model on the transformed series.

3Ô∏è‚É£ Why differencing matters

Imagine a wavy series with upward trend.

Differencing once (y[t]‚àíy[t‚àí1]) removes trend.

Seasonal differencing (y[t]‚àíy[t‚àí12]) removes yearly cycle.
Then AR and MA terms model the ‚Äústationary core‚Äù left over.

4Ô∏è‚É£ Understanding the coefficients one by one

Your table:

Parameter	Coef	p-value	Meaning
ar.L1	+1.9427	<0.001	Very strong link to previous month
ar.L2	-0.9760	<0.001	Second month offset/correction
ma.L1	-0.9811	0.532	Past error influence, but not significant
sigma2	0.0001	residual variance	

Let‚Äôs interpret:

AR(2) means the next value is built from the last two values. Large positive at lag1 and large negative at lag2 typically means oscillating behavior (think up-down-up-down but decaying slowly).

MA(1) is how last month‚Äôs random error affects this month. Here it‚Äôs not significant (p>0.05) ‚Üí maybe not needed.

sigma2 is just the variance of residual noise.

Visualizing AR(2) vs AR(1)

AR(1) alone: y[t] ‚âà œÜ‚ÇÅy[t‚àí1] + error.

AR(2): y[t] ‚âà œÜ‚ÇÅy[t‚àí1] + œÜ‚ÇÇy[t‚àí2] + error.
If œÜ‚ÇÇ negative, it creates a bounce/overshoot effect ‚Äî like a spring oscillating.

5Ô∏è‚É£ The ‚ÄúInformation criteria‚Äù (AIC/BIC)

AIC (Akaike) and BIC (Bayesian) measure model quality: smaller = better (balances fit vs complexity).

auto_arima tries different combinations to minimize AIC.

You don‚Äôt care about absolute number; you care about ‚Äúwhich model has lower AIC than others.‚Äù

6Ô∏è‚É£ Putting it all together

Your pipeline:

Raw monthly series

Seasonal difference at lag 12

Fit AR(2)+MA(1)

Forecast next months

What this model is telling you:

After removing yearly seasonality, the attrition metric is highly persistent ‚Äî what happened last month and the month before heavily influences this month.

It tends to oscillate a bit because lag2 coefficient is negative.

The MA(1) is not adding much explanatory power.

7Ô∏è‚É£ Do you still need ACF/PACF?

For selection: No, because auto_arima already tried many combos.

For diagnostics & explanation: Yes, especially on residuals.

ACF plot of residuals should look like random noise.

If not, your model missed something.

8Ô∏è‚É£ How to diagnose residuals
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
plot_acf(model.resid, lags=24)
plt.show()


If all bars stay within the shaded area (95% conf), residuals are fine.

9Ô∏è‚É£ Forecast and uncertainty

model.get_forecast(steps=12) ‚Üí predicted_mean + conf_int

Plot fan chart to show CEO uncertainty zone.

üîü Checklist for you to internalize
Step	What you‚Äôre checking	Why
ADF / differencing	Stationarity	Make ARIMA valid
Seasonal differencing	Remove cycles	Make ARIMA simpler
auto_arima	Picks (p,d,q)(P,D,Q)	Automates model
Residual ACF/PACF	Noise?	Validate model
Forecast with CI	Show risk band	Impress CEO
üîÅ In plain story form for your CEO:

We started with raw monthly attrition. It had yearly seasonality.

We removed yearly seasonality automatically (seasonal difference).

We built a dynamic model where each month depends on the last two months. This captures persistence and oscillations.

We tested accuracy on 2023‚Äì2024 (test) and 2025 (val) and saw good RMSE and MAPE.

We validated residuals (they look like noise).

We forecast 12 months ahead with confidence intervals (fan chart).

Action: our top-3 groups have high forecast error ‚Äî focus on them.

üìù Key takeaways (beginner-friendly but technical):

ARIMA = AR (memory of past values) + I (difference for stationarity) + MA (memory of past shocks).

Seasonal ARIMA = same but accounts for repeating patterns.

Auto_ARIMA picks model automatically; you still check residuals.

Big AR coefficients = strong dependence; negative AR2 = oscillation.

MA coefficients = effect of past forecast errors; if not significant, drop them.

AIC/BIC low = better model but don‚Äôt overfit.

Always plot residual ACF and fan chart to communicate.

